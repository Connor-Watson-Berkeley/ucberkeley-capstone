{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02e5483f-7d3b-4f3b-a187-17beb4f20bb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load & Validate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "accefd6c-8978-47b6-a4e6-cf6fee454b76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Quick validation\n",
    "SELECT \n",
    "  'Total Rows' as metric, COUNT(*) as value FROM commodity.silver.unified_data\n",
    "UNION ALL\n",
    "SELECT 'Unique (date, commodity, region)', COUNT(DISTINCT date, commodity, region) FROM commodity.silver.unified_data\n",
    "UNION ALL\n",
    "SELECT 'Duplicates', COUNT(*) - COUNT(DISTINCT date, commodity, region) FROM commodity.silver.unified_data\n",
    "UNION ALL\n",
    "SELECT 'Null Close Prices', SUM(CASE WHEN close IS NULL THEN 1 ELSE 0 END) FROM commodity.silver.unified_data\n",
    "UNION ALL\n",
    "SELECT 'Null VIX', SUM(CASE WHEN vix IS NULL THEN 1 ELSE 0 END) FROM commodity.silver.unified_data;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32a7bc08-a04e-4170-9dd1-9a372b3afffe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Check for nulls across ALL columns\n",
    "SELECT \n",
    "  'date' as column_name, SUM(CASE WHEN date IS NULL THEN 1 ELSE 0 END) as null_count FROM commodity.silver.unified_data\n",
    "UNION ALL\n",
    "SELECT 'commodity', SUM(CASE WHEN commodity IS NULL THEN 1 ELSE 0 END) FROM commodity.silver.unified_data\n",
    "UNION ALL\n",
    "SELECT 'close', SUM(CASE WHEN close IS NULL THEN 1 ELSE 0 END) FROM commodity.silver.unified_data\n",
    "UNION ALL\n",
    "SELECT 'high', SUM(CASE WHEN high IS NULL THEN 1 ELSE 0 END) FROM commodity.silver.unified_data\n",
    "UNION ALL\n",
    "SELECT 'low', SUM(CASE WHEN low IS NULL THEN 1 ELSE 0 END) FROM commodity.silver.unified_data\n",
    "UNION ALL\n",
    "SELECT 'open', SUM(CASE WHEN open IS NULL THEN 1 ELSE 0 END) FROM commodity.silver.unified_data\n",
    "UNION ALL\n",
    "SELECT 'volume', SUM(CASE WHEN volume IS NULL THEN 1 ELSE 0 END) FROM commodity.silver.unified_data\n",
    "UNION ALL\n",
    "SELECT 'vix', SUM(CASE WHEN vix IS NULL THEN 1 ELSE 0 END) FROM commodity.silver.unified_data\n",
    "UNION ALL\n",
    "SELECT 'region', SUM(CASE WHEN region IS NULL THEN 1 ELSE 0 END) FROM commodity.silver.unified_data\n",
    "UNION ALL\n",
    "SELECT 'temp_c', SUM(CASE WHEN temp_c IS NULL THEN 1 ELSE 0 END) FROM commodity.silver.unified_data\n",
    "UNION ALL\n",
    "SELECT 'humidity_pct', SUM(CASE WHEN humidity_pct IS NULL THEN 1 ELSE 0 END) FROM commodity.silver.unified_data\n",
    "UNION ALL\n",
    "SELECT 'precipitation_mm', SUM(CASE WHEN precipitation_mm IS NULL THEN 1 ELSE 0 END) FROM commodity.silver.unified_data\n",
    "UNION ALL\n",
    "SELECT 'vnd_usd', SUM(CASE WHEN vnd_usd IS NULL THEN 1 ELSE 0 END) FROM commodity.silver.unified_data\n",
    "UNION ALL\n",
    "SELECT 'cop_usd', SUM(CASE WHEN cop_usd IS NULL THEN 1 ELSE 0 END) FROM commodity.silver.unified_data\n",
    "UNION ALL\n",
    "SELECT 'idr_usd', SUM(CASE WHEN idr_usd IS NULL THEN 1 ELSE 0 END) FROM commodity.silver.unified_data\n",
    "UNION ALL\n",
    "SELECT 'uah_usd', SUM(CASE WHEN uah_usd IS NULL THEN 1 ELSE 0 END) FROM commodity.silver.unified_data\n",
    "UNION ALL\n",
    "SELECT 'irr_usd', SUM(CASE WHEN irr_usd IS NULL THEN 1 ELSE 0 END) FROM commodity.silver.unified_data\n",
    "UNION ALL\n",
    "SELECT 'byn_usd', SUM(CASE WHEN byn_usd IS NULL THEN 1 ELSE 0 END) FROM commodity.silver.unified_data\n",
    "ORDER BY column_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5289c57-f253-402b-8ef3-47deb7cd2587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save result to dataframe\n",
    "df_spark = spark.table(\"commodity.silver.unified_data\")\n",
    "df_spark.cache()\n",
    "df_spark.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c6b49c2-530f-43b0-bc7b-3438eabb2d21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM commodity.silver.unified_data\n",
    "WHERE region IN ('Bugisu_Uganda','Chiapas_Mexico')\n",
    "    AND commodity IN ('Coffee')\n",
    "ORDER BY date DESC\n",
    "LIMIT 15\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee41e254-c8da-4005-9e0a-1d0ccfbb9338",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Low-accuracy Forecast for Integration Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d526d3a-12f0-48b4-8ace-b4ef71171668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple Coffee Forecast - Databricks PySpark Notebook\n",
    "=====================================================\n",
    "\n",
    "Purpose: Establish data contract with Risk Agent by producing backdated forecast \n",
    "distributions for backtesting.\n",
    "\n",
    "Focus: Simple implementation first, iterate on accuracy later.\n",
    "\n",
    "Output Contract:\n",
    "- Point forecasts (14-day ahead with confidence intervals)\n",
    "- Distribution paths (2,000 Monte Carlo samples)\n",
    "- Backdated with data_cutoff_date for proper backtesting\n",
    "- Partitioned by model_type_version (e.g., sarimax_v0)\n",
    "- Saved to Delta tables in commodity.default schema\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 1. SETUP & IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "MODEL_VERSION = \"sarimax_v0\"\n",
    "N_PATHS = 2000\n",
    "FORECAST_HORIZON = 14\n",
    "BACKTESTING_START_DATE = '2018-01-01'\n",
    "FORECAST_FREQUENCY = '1D'  # Daily forecasts for complete coverage\n",
    "\n",
    "# Delta table configuration\n",
    "CATALOG = \"commodity\"\n",
    "SCHEMA = \"silver\"  \n",
    "FORECAST_TABLE = f\"{CATALOG}.{SCHEMA}.coffee_point_forecasts\"\n",
    "DISTRIBUTION_TABLE = f\"{CATALOG}.{SCHEMA}.coffee_distributions\"\n",
    "\n",
    "print(f\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════╗\n",
    "║           COFFEE FORECAST CONFIGURATION                      ║\n",
    "╚══════════════════════════════════════════════════════════════╝\n",
    "  Model Version:     {MODEL_VERSION}\n",
    "  Forecast Horizon:  {FORECAST_HORIZON} days\n",
    "  Frequency:         {FORECAST_FREQUENCY} (daily)\n",
    "  Start Date:        {BACKTESTING_START_DATE}\n",
    "  Sample Paths:      {N_PATHS:,}\n",
    "  \n",
    "  Output Tables:\n",
    "    • {FORECAST_TABLE}\n",
    "    • {DISTRIBUTION_TABLE}\n",
    "  \n",
    "  Estimated: ~2,500 forecasts, ~60-90 min runtime\n",
    "╚══════════════════════════════════════════════════════════════╝\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. LOAD UNIFIED DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading unified_data from Databricks...\")\n",
    "print(\"(Make sure you've run the unified_data temp view creation SQL first!)\")\n",
    "\n",
    "# Load from the temp view you created\n",
    "df_spark = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        date,\n",
    "        is_trading_day,\n",
    "        commodity,\n",
    "        close,\n",
    "        high,\n",
    "        low,\n",
    "        open,\n",
    "        volume,\n",
    "        vix,\n",
    "        region,\n",
    "        temp_c,\n",
    "        humidity_pct,\n",
    "        precipitation_mm\n",
    "    FROM commodity.silver.unified_data\n",
    "    WHERE commodity = 'Coffee'\n",
    "    AND is_trading_day = 1  -- Only trading days for modeling\n",
    "    ORDER BY date, region\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Loaded {df_spark.count()} rows\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. PREPARE DATA FOR MODELING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Preparing data for time series modeling...\")\n",
    "\n",
    "# Convert to pandas for SARIMAX (simpler for MVP)\n",
    "# For a simple model, we'll aggregate weather across regions (mean)\n",
    "df_agg = df_spark.groupBy(\"date\", \"commodity\").agg(\n",
    "    first(\"close\").alias(\"close\"),\n",
    "    first(\"vix\").alias(\"vix\"),\n",
    "    avg(\"temp_c\").alias(\"avg_temp\"),\n",
    "    avg(\"humidity_pct\").alias(\"avg_humidity\"),\n",
    "    avg(\"precipitation_mm\").alias(\"avg_precip\")\n",
    ").orderBy(\"date\")\n",
    "\n",
    "df_pd = df_agg.toPandas()\n",
    "df_pd['date'] = pd.to_datetime(df_pd['date'])\n",
    "df_pd = df_pd.set_index('date').sort_index()\n",
    "\n",
    "print(f\"Prepared {len(df_pd)} daily observations from {df_pd.index.min()} to {df_pd.index.max()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. SIMPLE FORECASTING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def create_simple_forecast(train_data, forecast_date, n_days=14):\n",
    "    \"\"\"\n",
    "    Create a simple SARIMAX forecast with minimal covariates.\n",
    "    \n",
    "    Args:\n",
    "        train_data: DataFrame with close price and covariates\n",
    "        forecast_date: Date to generate forecast for\n",
    "        n_days: Number of days ahead to forecast\n",
    "        \n",
    "    Returns:\n",
    "        forecast_mean: Array of point forecasts\n",
    "        forecast_std: Array of standard errors\n",
    "        success: Boolean indicating if model converged\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare target and covariates\n",
    "    y = train_data['close']\n",
    "    \n",
    "    # Use minimal covariates for MVP (just VIX and temperature)\n",
    "    exog = train_data[['vix', 'avg_temp']]\n",
    "    \n",
    "    try:\n",
    "        # Fit SARIMAX(1,1,1) - same as V1 champion model\n",
    "        model = SARIMAX(\n",
    "            y,\n",
    "            exog=exog,\n",
    "            order=(1, 1, 1),\n",
    "            enforce_stationarity=False,\n",
    "            enforce_invertibility=False\n",
    "        )\n",
    "        \n",
    "        results = model.fit(disp=False, maxiter=100)\n",
    "        \n",
    "        # Generate forecast\n",
    "        # For exog forecast, use last available values (simple forward fill)\n",
    "        last_exog = exog.iloc[-1:].values\n",
    "        exog_forecast = np.tile(last_exog, (n_days, 1))\n",
    "        \n",
    "        forecast_obj = results.get_forecast(steps=n_days, exog=exog_forecast)\n",
    "        forecast_mean = forecast_obj.predicted_mean.values\n",
    "        forecast_std = forecast_obj.se_mean.values\n",
    "        \n",
    "        return forecast_mean, forecast_std, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Forecast failed for {forecast_date}: {str(e)}\")\n",
    "        # Return naive forecast (last value) if model fails\n",
    "        last_price = y.iloc[-1]\n",
    "        forecast_mean = np.full(n_days, last_price)\n",
    "        forecast_std = np.full(n_days, y.std())\n",
    "        return forecast_mean, forecast_std, False\n",
    "\n",
    "# ============================================================================\n",
    "# 5. GENERATE SAMPLE PATHS (MONTE CARLO DISTRIBUTION)\n",
    "# ============================================================================\n",
    "\n",
    "def generate_sample_paths(forecast_mean, forecast_std, n_paths=2000):\n",
    "    \"\"\"\n",
    "    Generate Monte Carlo sample paths for distribution.\n",
    "    \n",
    "    Simple approach: Normal distribution with increasing variance.\n",
    "    Future enhancement: Add skewness, regime-specific distributions, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_days = len(forecast_mean)\n",
    "    paths = np.zeros((n_paths, n_days))\n",
    "    \n",
    "    for day in range(n_days):\n",
    "        # Generate samples from normal distribution\n",
    "        # Variance increases with forecast horizon (uncertainty grows)\n",
    "        daily_std = forecast_std[day] * np.sqrt(day + 1)\n",
    "        paths[:, day] = np.random.normal(forecast_mean[day], daily_std, n_paths)\n",
    "    \n",
    "    return paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c1af42e-ef9f-4944-b3cd-f761efd6bfc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. BACKTESTING LOOP - GENERATE BACKDATED FORECASTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nGenerating backdated forecasts for backtesting...\")\n",
    "\n",
    "# Define backtesting period\n",
    "START_DATE = pd.Timestamp(BACKTESTING_START_DATE)\n",
    "END_DATE = df_pd.index.max() - timedelta(days=FORECAST_HORIZON)\n",
    "\n",
    "# Walk-forward backtesting - Daily forecasts for complete coverage\n",
    "backtest_dates = pd.date_range(start=START_DATE, end=END_DATE, freq=FORECAST_FREQUENCY)\n",
    "MIN_TRAIN_DAYS = 365\n",
    "\n",
    "forecasts_list = []\n",
    "distributions_list = []\n",
    "\n",
    "print(f\"Generating {len(backtest_dates)} forecasts from {START_DATE} to {END_DATE}\")\n",
    "print(f\"Forecast frequency: {FORECAST_FREQUENCY} (daily - covers all days of week)\")\n",
    "print(f\"Forecast horizon: {FORECAST_HORIZON} days ahead\")\n",
    "print(f\"This will create overlapping forecasts for realistic backtesting\\n\")\n",
    "\n",
    "# START TIMING\n",
    "import time\n",
    "start_time = time.time()\n",
    "forecast_times = []\n",
    "\n",
    "for i, forecast_date in enumerate(backtest_dates):\n",
    "    \n",
    "    # Time each forecast\n",
    "    forecast_start = time.time()\n",
    "    \n",
    "    if (i + 1) % 100 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        avg_time = np.mean(forecast_times[-100:]) if len(forecast_times) >= 100 else np.mean(forecast_times)\n",
    "        remaining = (len(backtest_dates) - i - 1) * avg_time\n",
    "        print(f\"Progress: {i+1}/{len(backtest_dates)} forecasts | \"\n",
    "              f\"Elapsed: {elapsed/60:.1f} min | \"\n",
    "              f\"Avg: {avg_time:.2f}s/forecast | \"\n",
    "              f\"ETA: {remaining/60:.1f} min\")\n",
    "    \n",
    "    # Get training data up to (but not including) forecast_date\n",
    "    train_end_date = forecast_date - timedelta(days=1)\n",
    "    train_start_date = train_end_date - timedelta(days=730)\n",
    "    \n",
    "    train_data = df_pd.loc[train_start_date:train_end_date]\n",
    "    \n",
    "    # Skip if insufficient data\n",
    "    if len(train_data) < MIN_TRAIN_DAYS:\n",
    "        continue\n",
    "    \n",
    "    # Generate forecast\n",
    "    forecast_mean, forecast_std, success = create_simple_forecast(\n",
    "        train_data, \n",
    "        forecast_date,\n",
    "        n_days=FORECAST_HORIZON\n",
    "    )\n",
    "    \n",
    "    # Create forecast dates\n",
    "    forecast_dates = pd.date_range(start=forecast_date, periods=FORECAST_HORIZON, freq='D')\n",
    "    \n",
    "    # Build point forecasts DataFrame\n",
    "    for day_idx, fcast_date in enumerate(forecast_dates):\n",
    "        forecasts_list.append({\n",
    "            'forecast_date': fcast_date,\n",
    "            'data_cutoff_date': train_end_date,\n",
    "            'generation_timestamp': datetime.now(),\n",
    "            'day_ahead': day_idx + 1,\n",
    "            'forecast_mean': float(forecast_mean[day_idx]),\n",
    "            'forecast_std': float(forecast_std[day_idx]),\n",
    "            'lower_95': float(forecast_mean[day_idx] - 1.96 * forecast_std[day_idx]),\n",
    "            'upper_95': float(forecast_mean[day_idx] + 1.96 * forecast_std[day_idx]),\n",
    "            'model_version': MODEL_VERSION,\n",
    "            'commodity': 'Coffee',\n",
    "            'model_success': success\n",
    "        })\n",
    "    \n",
    "    # Generate sample paths for distribution\n",
    "    sample_paths = generate_sample_paths(forecast_mean, forecast_std, N_PATHS)\n",
    "    \n",
    "    # Build distributions DataFrame\n",
    "    for path_id in range(N_PATHS):\n",
    "        path_data = {\n",
    "            'path_id': path_id + 1,\n",
    "            'forecast_start_date': forecast_date,\n",
    "            'data_cutoff_date': train_end_date,\n",
    "            'generation_timestamp': datetime.now(),\n",
    "            'model_version': MODEL_VERSION,\n",
    "            'commodity': 'Coffee'\n",
    "        }\n",
    "        \n",
    "        # Add each day's forecasted price\n",
    "        for day_idx in range(FORECAST_HORIZON):\n",
    "            path_data[f'day_{day_idx + 1}'] = float(sample_paths[path_id, day_idx])\n",
    "        \n",
    "        distributions_list.append(path_data)\n",
    "    \n",
    "    # Record forecast time\n",
    "    forecast_times.append(time.time() - forecast_start)\n",
    "\n",
    "# END TIMING\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nGenerated {len(forecasts_list)} point forecasts\")\n",
    "print(f\"Generated {len(distributions_list)} distribution paths\")\n",
    "print(f\"\\nTotal Runtime: {total_time/60:.2f} minutes ({total_time/3600:.2f} hours)\")\n",
    "print(f\"   Average: {np.mean(forecast_times):.2f} seconds per forecast\")\n",
    "print(f\"   Min: {np.min(forecast_times):.2f}s | Max: {np.max(forecast_times):.2f}s\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. CREATE SPARK DATAFRAMES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nCreating Spark DataFrames...\")\n",
    "\n",
    "# Point forecasts\n",
    "df_forecasts = spark.createDataFrame(pd.DataFrame(forecasts_list))\n",
    "\n",
    "# Distributions\n",
    "df_distributions = spark.createDataFrame(pd.DataFrame(distributions_list))\n",
    "\n",
    "print(f\"Point forecasts shape: {df_forecasts.count()} rows\")\n",
    "print(f\"Distributions shape: {df_distributions.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab1336e2-c566-4549-974d-89192046f261",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 8. WRITE TO DELTA TABLES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nWriting to Delta tables...\")\n",
    "\n",
    "# Write point forecasts\n",
    "print(f\"Writing point forecasts to {FORECAST_TABLE}...\")\n",
    "df_forecasts.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .partitionBy(\"model_version\", \"commodity\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(FORECAST_TABLE)\n",
    "\n",
    "print(f\"✓ Point forecasts written to: {FORECAST_TABLE}\")\n",
    "\n",
    "# Write distributions\n",
    "print(f\"Writing distributions to {DISTRIBUTION_TABLE}...\")\n",
    "df_distributions.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .partitionBy(\"model_version\", \"commodity\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(DISTRIBUTION_TABLE)\n",
    "\n",
    "print(f\"✓ Distributions written to: {DISTRIBUTION_TABLE}\")\n",
    "\n",
    "# Show table locations\n",
    "forecast_location = spark.sql(f\"DESCRIBE DETAIL {FORECAST_TABLE}\").select(\"location\").collect()[0][0]\n",
    "dist_location = spark.sql(f\"DESCRIBE DETAIL {DISTRIBUTION_TABLE}\").select(\"location\").collect()[0][0]\n",
    "\n",
    "print(f\"\\nTable locations:\")\n",
    "print(f\"  Point forecasts: {forecast_location}\")\n",
    "print(f\"  Distributions: {dist_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19c99320-9a9a-4ed5-8834-cc73192aa604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # ============================================================================\n",
    "# # 8. WRITE TO CSV FILES (While we wait on save permissions)\n",
    "# # ============================================================================\n",
    "\n",
    "# print(\"\\nWriting to CSV files...\")\n",
    "# write_start = time.time()\n",
    "\n",
    "# # Define output paths (local DBFS)\n",
    "# OUTPUT_BASE_PATH = \"/dbfs/FileStore/coffee_forecasts/\"\n",
    "# FORECAST_CSV_PATH = f\"{OUTPUT_BASE_PATH}point_forecasts/\"\n",
    "# DISTRIBUTION_CSV_PATH = f\"{OUTPUT_BASE_PATH}distributions/\"\n",
    "\n",
    "# # Create directories if they don't exist\n",
    "# import os\n",
    "# os.makedirs(FORECAST_CSV_PATH, exist_ok=True)\n",
    "# os.makedirs(DISTRIBUTION_CSV_PATH, exist_ok=True)\n",
    "\n",
    "# # Convert to pandas and write CSV\n",
    "# print(f\"Writing point forecasts to {FORECAST_CSV_PATH}...\")\n",
    "# df_forecasts_pd = df_forecasts.toPandas()\n",
    "# df_forecasts_pd.to_csv(\n",
    "#     f\"{FORECAST_CSV_PATH}coffee_point_forecasts_{MODEL_VERSION}.csv\",\n",
    "#     index=False\n",
    "# )\n",
    "# print(f\"✓ Point forecasts written: {len(df_forecasts_pd):,} rows\")\n",
    "\n",
    "# # Write distributions (this will be large!)\n",
    "# print(f\"Writing distributions to {DISTRIBUTION_CSV_PATH}...\")\n",
    "# df_distributions_pd = df_distributions.toPandas()\n",
    "# df_distributions_pd.to_csv(\n",
    "#     f\"{DISTRIBUTION_CSV_PATH}coffee_distributions_{MODEL_VERSION}.csv\",\n",
    "#     index=False\n",
    "# )\n",
    "# print(f\"✓ Distributions written: {len(df_distributions_pd):,} rows\")\n",
    "\n",
    "# write_time = time.time() - write_start\n",
    "# print(f\"\\n⏱️  Write time: {write_time:.2f} seconds\")\n",
    "\n",
    "# # Show file locations and sizes\n",
    "# print(f\"\\nFiles created:\")\n",
    "# print(f\"  Point forecasts: /FileStore/coffee_forecasts/point_forecasts/coffee_point_forecasts_{MODEL_VERSION}.csv\")\n",
    "# print(f\"  Distributions: /FileStore/coffee_forecasts/distributions/coffee_distributions_{MODEL_VERSION}.csv\")\n",
    "# print(f\"\\nDownload via Databricks UI: Workspace → FileStore → coffee_forecasts/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "491cb9b2-f798-4b76-bf7f-958ec1070830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Coffee Forecast Data - Risk Agent Quick Start\n",
    "\n",
    "## Data Locations\n",
    "\n",
    "**Point Forecasts:**\n",
    "`https://dbc-5474a94c-61c9.cloud.databricks.com/files/coffee_forecasts/point_forecasts/coffee_point_forecasts_sarimax_v0.csv`\n",
    "\n",
    "**Distributions:**\n",
    "`https://dbc-5474a94c-61c9.cloud.databricks.com/files/coffee_forecasts/distributions/coffee_distributions_sarimax_v0.csv`\n",
    "\n",
    "### Production (S3) - Coming Soon, pending access\n",
    "**Point Forecasts:**\n",
    "`commodity.default.coffee_point_forecasts`\n",
    "\n",
    "**Distributions:**\n",
    "`commodity.default.coffee_distributions`\n",
    "\n",
    "## Loading Data in Databricks\n",
    "```python\n",
    "# Point Forecasts\n",
    "df_forecasts = spark.read.csv(\n",
    "    \"/FileStore/coffee_forecasts/point_forecasts/coffee_point_forecasts_sarimax_v0.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Distributions\n",
    "df_distributions = spark.read.csv(\n",
    "    \"/FileStore/coffee_forecasts/distributions/coffee_distributions_sarimax_v0.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "```\n",
    "\n",
    "### From Delta Tables (Production) - Coming Soon\n",
    "```python\n",
    "# Point Forecasts\n",
    "df_forecasts = spark.table(\"commodity.default.coffee_point_forecasts\")\n",
    "\n",
    "# Distributions\n",
    "df_distributions = spark.table(\"commodity.default.coffee_distributions\")\n",
    "```\n",
    "\n",
    "\n",
    "## Point Forecasts Schema\n",
    "\n",
    "| Field | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| `forecast_date` | DATE | Target date being forecasted |\n",
    "| `data_cutoff_date` | DATE | Last training date (must be < forecast_date) |\n",
    "| `day_ahead` | INT | Horizon (1-14 days) |\n",
    "| `forecast_mean` | FLOAT | Point forecast (cents/lb) |\n",
    "| `forecast_std` | FLOAT | Forecast uncertainty |\n",
    "| `lower_95` | FLOAT | 95% CI lower bound |\n",
    "| `upper_95` | FLOAT | 95% CI upper bound |\n",
    "| `model_version` | STRING | 'sarimax_v0' |\n",
    "| `commodity` | STRING | 'Coffee' |\n",
    "\n",
    "**Usage:**\n",
    "```sql\n",
    "-- Get 7-day ahead forecasts for backtesting\n",
    "SELECT forecast_date, forecast_mean, lower_95, upper_95\n",
    "FROM point_forecasts\n",
    "WHERE day_ahead = 7\n",
    "AND data_cutoff_date < forecast_date  -- CRITICAL: No data leakage\n",
    "AND forecast_date BETWEEN '2023-01-01' AND '2023-12-31'\n",
    "```\n",
    "\n",
    "## Distributions Schema\n",
    "\n",
    "| Field | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| `path_id` | INT | Sample path ID (1-2000) |\n",
    "| `forecast_start_date` | DATE | First day of forecast |\n",
    "| `data_cutoff_date` | DATE | Last training date |\n",
    "| `day_1` to `day_14` | FLOAT | Forecasted prices for each day |\n",
    "| `model_version` | STRING | 'sarimax_v0' |\n",
    "| `commodity` | STRING | 'Coffee' |\n",
    "\n",
    "**Usage:**\n",
    "```sql\n",
    "-- Calculate 95% VaR for day 7\n",
    "SELECT \n",
    "    forecast_start_date,\n",
    "    PERCENTILE(day_7, 0.05) as var_95,\n",
    "    AVG(day_7) as mean_price\n",
    "FROM distributions\n",
    "WHERE forecast_start_date = '2024-01-15'\n",
    "AND data_cutoff_date < '2024-01-15'  -- No data leakage\n",
    "GROUP BY forecast_start_date\n",
    "```\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### 1. Data Leakage Prevention\n",
    "**ALWAYS filter:** `data_cutoff_date < forecast_date` or `forecast_start_date`\n",
    "\n",
    "### 2. Overlapping Forecasts\n",
    "Daily frequency = 14 forecasts exist for any target date\n",
    "- Forecast from Jan 7 (day_14) for Jan 21\n",
    "- Forecast from Jan 8 (day_13) for Jan 21\n",
    "- ...\n",
    "- Forecast from Jan 20 (day_1) for Jan 21\n",
    "\n",
    "### 3. Distribution = 2,000 Paths\n",
    "Each forecast date has 2,000 Monte Carlo samples for risk analysis\n",
    "\n",
    "\n",
    "## Common Queries\n",
    "\n",
    "### Backtest Forecast Accuracy\n",
    "```sql\n",
    "SELECT \n",
    "    pf.forecast_date,\n",
    "    pf.forecast_mean,\n",
    "    actual.close as actual_price,\n",
    "    ABS(pf.forecast_mean - actual.close) as error\n",
    "FROM point_forecasts pf\n",
    "JOIN unified_data actual \n",
    "    ON actual.date = pf.forecast_date \n",
    "    AND actual.commodity = 'Coffee'\n",
    "WHERE pf.day_ahead = 7\n",
    "AND pf.data_cutoff_date < pf.forecast_date\n",
    "```\n",
    "\n",
    "### Calculate VaR/CVaR\n",
    "```sql\n",
    "-- 95% VaR\n",
    "SELECT \n",
    "    forecast_start_date,\n",
    "    PERCENTILE(day_14, 0.05) as var_95\n",
    "FROM distributions\n",
    "WHERE forecast_start_date = CURRENT_DATE()\n",
    "GROUP BY forecast_start_date;\n",
    "\n",
    "-- CVaR (average loss in worst 5%)\n",
    "WITH var AS (\n",
    "    SELECT PERCENTILE(day_14, 0.05) as threshold\n",
    "    FROM distributions\n",
    "    WHERE forecast_start_date = CURRENT_DATE()\n",
    ")\n",
    "SELECT AVG(day_14) as cvar_95\n",
    "FROM distributions, var\n",
    "WHERE day_14 <= var.threshold\n",
    "AND forecast_start_date = CURRENT_DATE();\n",
    "```\n",
    "\n",
    "## Data Quality Checks\n",
    "```sql\n",
    "-- 1. Verify no data leakage (MUST return 0)\n",
    "SELECT COUNT(*) FROM point_forecasts \n",
    "WHERE forecast_date <= data_cutoff_date;\n",
    "\n",
    "-- 2. Verify 2,000 paths per date\n",
    "SELECT forecast_start_date, COUNT(*) as paths\n",
    "FROM distributions\n",
    "GROUP BY forecast_start_date\n",
    "HAVING COUNT(*) != 2000;\n",
    "\n",
    "-- 3. Check date coverage\n",
    "SELECT \n",
    "    MIN(forecast_date) as earliest,\n",
    "    MAX(forecast_date) as latest,\n",
    "    COUNT(DISTINCT forecast_date) as num_dates\n",
    "FROM point_forecasts;\n",
    "```\n",
    "\n",
    "## Detials\n",
    "\n",
    "**Data Period:** 2018-01-01 to present  \n",
    "**Update Frequency:** Daily (production)  \n",
    "**Model:** SARIMAX(1,1,1) baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc15a901-5190-43c1-9567-fc2852bbcf21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 9. VALIDATION QUERIES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDATION CHECKS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check 1: Point forecast summary\n",
    "print(\"\\n1. Point Forecasts Summary:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        model_version,\n",
    "        commodity,\n",
    "        COUNT(*) as total_forecasts,\n",
    "        COUNT(DISTINCT data_cutoff_date) as unique_cutoff_dates,\n",
    "        MIN(forecast_date) as earliest_forecast,\n",
    "        MAX(forecast_date) as latest_forecast,\n",
    "        AVG(forecast_mean) as avg_forecast_price,\n",
    "        AVG(forecast_std) as avg_std_error\n",
    "    FROM {FORECAST_TABLE}\n",
    "    GROUP BY model_version, commodity\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Check 2: Distribution summary\n",
    "print(\"\\n2. Distribution Summary:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        model_version,\n",
    "        commodity,\n",
    "        COUNT(*) as total_paths,\n",
    "        COUNT(DISTINCT forecast_start_date) as unique_forecast_dates,\n",
    "        MIN(forecast_start_date) as earliest_forecast,\n",
    "        MAX(forecast_start_date) as latest_forecast\n",
    "    FROM {DISTRIBUTION_TABLE}\n",
    "    GROUP BY model_version, commodity\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Check 3: Sample distribution statistics (Day 1 vs Day 14)\n",
    "print(\"\\n3. Sample Distribution Statistics (Day 1 vs Day 14):\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        'Day 1' as forecast_day,\n",
    "        AVG(day_1) as mean_price,\n",
    "        STDDEV(day_1) as std_price,\n",
    "        PERCENTILE(day_1, 0.05) as p5,\n",
    "        PERCENTILE(day_1, 0.50) as p50,\n",
    "        PERCENTILE(day_1, 0.95) as p95\n",
    "    FROM {DISTRIBUTION_TABLE}\n",
    "    WHERE model_version = 'sarimax_v0'\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'Day 14' as forecast_day,\n",
    "        AVG(day_14) as mean_price,\n",
    "        STDDEV(day_14) as std_price,\n",
    "        PERCENTILE(day_14, 0.05) as p5,\n",
    "        PERCENTILE(day_14, 0.50) as p50,\n",
    "        PERCENTILE(day_14, 0.95) as p95\n",
    "    FROM {DISTRIBUTION_TABLE}\n",
    "    WHERE model_version = 'sarimax_v0'\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Check 4: Data leakage validation (CRITICAL!)\n",
    "print(\"\\n4. Data Cutoff Validation (ensuring no data leakage):\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_forecasts,\n",
    "        SUM(CASE WHEN forecast_date > data_cutoff_date THEN 1 ELSE 0 END) as valid_forecasts,\n",
    "        SUM(CASE WHEN forecast_date <= data_cutoff_date THEN 1 ELSE 0 END) as data_leakage_errors\n",
    "    FROM {FORECAST_TABLE}\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Check 5: Sample overlapping forecasts\n",
    "print(\"\\n5. Sample Overlapping Forecasts (for one target date):\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        forecast_date as target_date,\n",
    "        data_cutoff_date,\n",
    "        day_ahead,\n",
    "        forecast_mean,\n",
    "        DATEDIFF(forecast_date, data_cutoff_date) as forecast_age_days\n",
    "    FROM {FORECAST_TABLE}\n",
    "    WHERE forecast_date = (\n",
    "        SELECT MAX(forecast_date) \n",
    "        FROM {FORECAST_TABLE} \n",
    "        WHERE day_ahead <= 14\n",
    "    )\n",
    "    AND data_cutoff_date < forecast_date\n",
    "    ORDER BY data_cutoff_date DESC\n",
    "    LIMIT 14\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA CONTRACT ESTABLISHED ✓\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "Risk Agent can now access backdated forecasts from Delta tables:\n",
    "- Point Forecasts: {FORECAST_TABLE}\n",
    "- Distributions: {DISTRIBUTION_TABLE}\n",
    "\n",
    "Query example for overlapping forecasts:\n",
    "SELECT \n",
    "    forecast_date,\n",
    "    data_cutoff_date,\n",
    "    day_ahead,\n",
    "    forecast_mean,\n",
    "    DATEDIFF(forecast_date, data_cutoff_date) as forecast_age_days\n",
    "FROM {FORECAST_TABLE}\n",
    "WHERE forecast_date = '2024-10-15'\n",
    "AND data_cutoff_date < '2024-10-15'\n",
    "ORDER BY data_cutoff_date DESC;\n",
    "\n",
    "This returns all 14 forecasts for Oct 15 (made from Oct 1-14)\n",
    "\n",
    "Next Steps:\n",
    "1. Risk Agent queries tables directly for backtesting\n",
    "2. Iterate on forecast accuracy (add more covariates, tune hyperparameters)\n",
    "3. Add Sugar commodity forecasts\n",
    "4. Implement skewed distributions (Phase 2)\n",
    "5. Add hierarchical regional forecasts (Phase 2)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nForecast generation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "430bacc9-0eda-4bbb-a1d9-8e5b7d922aba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7414832371369395,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Global Forecast Agent V1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
