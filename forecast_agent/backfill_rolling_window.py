"""
Backfill forecasts using rolling window cross-validation.

Key innovation: Train periodically, forecast daily.
- Train model every N months (e.g., semiannually for transformers)
- Generate forecasts for every day using most recently trained model
- Enables realistic backtesting without training 2,860 separate models!

Usage:
    # Semiannual training for expensive models (transformers)
    python backfill_rolling_window.py --commodity Coffee --models transformer_v1 --train-frequency semiannually

    # Monthly training for cheap models (ARIMA, XGBoost)
    python backfill_rolling_window.py --commodity Coffee --models naive arima_111 xgboost --train-frequency monthly

    # Weekly training for very cheap models
    python backfill_rolling_window.py --commodity Coffee --models naive --train-frequency weekly
"""

import argparse
import pandas as pd
import numpy as np
from datetime import datetime, timedelta, date
from typing import List, Dict, Optional
from dateutil.relativedelta import relativedelta
import sys
import os
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))

from databricks import sql
from databricks.sql.exc import RequestError
from ground_truth.config.model_registry import BASELINE_MODELS
from utils.model_persistence import load_model
from utils.monte_carlo_simulation import generate_monte_carlo_paths


def get_training_dates(
    start_date: date,
    end_date: date,
    frequency: str
) -> List[date]:
    """
    Generate training dates based on frequency.

    Args:
        start_date: First possible training date
        end_date: Last possible training date
        frequency: 'daily', 'weekly', 'monthly', 'quarterly', 'semiannually', 'annually'

    Returns:
        List of dates to train models on
    """
    training_dates = []
    current = start_date

    frequency_map = {
        'daily': timedelta(days=1),
        'weekly': timedelta(days=7),
        'biweekly': timedelta(days=14),
        'monthly': relativedelta(months=1),
        'quarterly': relativedelta(months=3),
        'semiannually': relativedelta(months=6),
        'annually': relativedelta(years=1)
    }

    if frequency not in frequency_map:
        raise ValueError(f"Unknown frequency: {frequency}. Choose from {list(frequency_map.keys())}")

    delta = frequency_map[frequency]

    while current <= end_date:
        training_dates.append(current)

        if isinstance(delta, timedelta):
            current = current + delta
        else:  # relativedelta
            current = current + delta

    return training_dates


def reconnect_if_needed(connection, databricks_host, databricks_token, databricks_http_path):
    """
    Check if connection is alive and reconnect if needed.
    Returns: connection (either existing or new)
    """
    try:
        # Try a simple query to test connection
        cursor = connection.cursor()
        cursor.execute("SELECT 1")
        cursor.fetchall()
        cursor.close()
        return connection
    except (RequestError, Exception) as e:
        # Session is dead, create new connection
        print(f"     üîÑ Session timeout detected, reconnecting...")
        try:
            connection.close()
        except:
            pass

        new_connection = sql.connect(
            server_hostname=databricks_host.replace('https://', ''),
            http_path=databricks_http_path,
            access_token=databricks_token
        )
        print(f"     ‚úÖ Reconnected successfully")
        return new_connection


def load_training_data(connection, commodity: str, cutoff_date: date, lookback_days: Optional[int] = None) -> pd.DataFrame:
    """
    Load data up to cutoff_date for training or inference.

    Args:
        connection: Databricks SQL connection
        commodity: 'Coffee' or 'Sugar'
        cutoff_date: Latest date to include
        lookback_days: If specified, only load last N days (for inference with pretrained models).
                       If None, load all historical data (for training).

    Returns:
        DataFrame with market data
    """
    cursor = connection.cursor()

    # For inference with pretrained models, only load recent data
    if lookback_days is not None:
        start_date = cutoff_date - timedelta(days=lookback_days)
        date_filter = f"AND date > '{start_date}' AND date <= '{cutoff_date}'"
    else:
        date_filter = f"AND date <= '{cutoff_date}'"

    query = f"""
        SELECT
            date,
            close,
            open,
            high,
            low,
            volume,
            temp_mean_c,
            humidity_mean_pct,
            precipitation_mm,
            vix,
            cop_usd
        FROM commodity.silver.unified_data
        WHERE commodity = '{commodity}'
          {date_filter}
        ORDER BY date
    """

    cursor.execute(query)
    rows = cursor.fetchall()
    columns = [desc[0] for desc in cursor.description]

    df = pd.DataFrame(rows, columns=columns)
    df['date'] = pd.to_datetime(df['date'])
    df = df.set_index('date')

    cursor.close()
    return df


def train_model(
    training_data: pd.DataFrame,
    model_config: Dict,
    commodity: str
) -> Dict:
    """
    Train a single model on training data.

    Returns:
        Dict with fitted model info or error
    """
    model_fn = model_config['function']
    model_params = model_config['params'].copy()

    try:
        # For this backfill, we just need to verify the model can train
        # The actual forecast generation happens per-date below
        print(f"     Training on {len(training_data)} days of data...")
        return {'success': True, 'training_end': training_data.index[-1]}
    except Exception as e:
        return {'success': False, 'error': str(e)}


def generate_forecast_for_date(
    forecast_start_date: date,
    training_df: pd.DataFrame,
    model_config: Dict,
    commodity: str,
    n_paths: int = 2000,
    forecast_horizon: int = 14,
    fitted_model: Optional[Dict] = None
) -> Dict:
    """Generate forecast for a single date using trained model.

    Args:
        fitted_model: Optional pre-trained model from database. If provided, uses inference-only mode.
    """
    model_fn = model_config['function']
    model_params = model_config['params'].copy()
    model_params['horizon'] = forecast_horizon

    # Initialize forecast_std to avoid UnboundLocalError in except block
    forecast_std = None

    try:
        # Pass fitted_model if available (inference-only mode)
        if fitted_model is not None:
            result = model_fn(df_pandas=training_df, commodity=commodity, fitted_model=fitted_model, **model_params)
        else:
            result = model_fn(df_pandas=training_df, commodity=commodity, **model_params)
        forecast_df = result['forecast_df']

        # Generate Monte Carlo paths using model-based simulation
        # For SARIMA: Simulates from actual ARIMA process
        # For other models: Uses appropriate stochastic process (GBM, random walk, etc.)
        if fitted_model is not None:
            # Use fitted model for model-based simulation
            paths = generate_monte_carlo_paths(
                fitted_model=fitted_model,
                forecast_df=forecast_df,
                n_paths=n_paths,
                horizon=forecast_horizon,
                training_df=training_df
            )

            # Compute forecast_std for metadata
            if 'yhat_std' in forecast_df.columns:
                forecast_std = forecast_df['yhat_std'].mean()
            elif 'std' in result:
                forecast_std = result['std']
            else:
                returns = training_df['close'].pct_change().dropna()
                daily_std = returns.std()
                forecast_std = training_df['close'].iloc[-1] * daily_std
        else:
            # Fallback to simple Gaussian noise if no fitted model
            if 'std' in result:
                forecast_std = result['std']
            else:
                returns = training_df['close'].pct_change().dropna()
                daily_std = returns.std()
                forecast_std = training_df['close'].iloc[-1] * daily_std

            paths = []
            for path_id in range(1, n_paths + 1):
                noise = np.random.normal(0, forecast_std, len(forecast_df))
                path_forecast = forecast_df['forecast'].values + noise
                paths.append({
                    'path_id': path_id,
                    'values': path_forecast.tolist()
                })

        return {
            'success': True,
            'forecast_df': forecast_df,
            'paths': paths,
            'mean_forecast': forecast_df['forecast'].values,
            'forecast_std': forecast_std,
            'error': None
        }

    except Exception as e:
        return {
            'success': False,
            'forecast_df': None,
            'paths': None,
            'mean_forecast': None,
            'forecast_std': None,
            'error': str(e)
        }


def write_batch_to_tables(
    connection,
    batch_data: List[Dict]
):
    """
    Write a batch of forecasts to all 3 tables at once.

    This dramatically speeds up writes by:
    - Reducing network round trips (1 batch vs N individual writes)
    - Allowing database to optimize bulk inserts

    Args:
        connection: Databricks SQL connection
        batch_data: List of dicts, each containing:
            - forecast_start_date
            - data_cutoff_date
            - paths
            - mean_forecast
            - forecast_std
            - model_version
            - commodity
    """
    if not batch_data:
        return

    cursor = connection.cursor()
    generation_timestamp = datetime.now()

    # Accumulate all rows across all forecasts in batch
    all_dist_rows = []
    all_point_rows = []
    actuals_to_fetch = []  # (start_date, commodity) tuples

    for forecast_data in batch_data:
        forecast_start_date = forecast_data['forecast_start_date']
        data_cutoff_date = forecast_data['data_cutoff_date']
        paths = forecast_data['paths']
        mean_forecast = forecast_data['mean_forecast']
        forecast_std = forecast_data['forecast_std']
        model_version = forecast_data['model_version']
        commodity = forecast_data['commodity']

        # Build distribution rows for this forecast
        for path in paths:
            path_id = path['path_id']
            values = path['values']

            day_values = []
            for i in range(14):
                if i < len(values):
                    day_values.append(f"{values[i]:.2f}")
                else:
                    day_values.append("NULL")

            has_data_leakage = 1 if forecast_start_date <= data_cutoff_date else 0
            row_sql = f"({path_id}, '{forecast_start_date}', '{data_cutoff_date}', '{generation_timestamp}', '{model_version}', '{commodity}', {', '.join(day_values)}, FALSE, {has_data_leakage})"
            all_dist_rows.append(row_sql)

        # Build point forecast rows for this forecast
        for day_idx in range(len(mean_forecast)):
            forecast_date = forecast_start_date + timedelta(days=day_idx)
            day_ahead = day_idx + 1

            vol_scaled = forecast_std * np.sqrt(day_ahead)
            lower_95 = mean_forecast[day_idx] - 1.96 * vol_scaled
            upper_95 = mean_forecast[day_idx] + 1.96 * vol_scaled

            has_data_leakage = 1 if forecast_date <= data_cutoff_date else 0

            all_point_rows.append(
                f"('{forecast_date}', '{data_cutoff_date}', '{generation_timestamp}', "
                f"{day_ahead}, {mean_forecast[day_idx]:.2f}, {forecast_std:.2f}, "
                f"{lower_95:.2f}, {upper_95:.2f}, '{model_version}', '{commodity}', "
                f"TRUE, NULL, {has_data_leakage})"
            )

        # Track actuals we need to fetch
        actuals_to_fetch.append((forecast_start_date, commodity))

    # ============================================================
    # 1. DISTRIBUTIONS TABLE - Write in chunks of 500
    # ============================================================
    chunk_size = 500
    for i in range(0, len(all_dist_rows), chunk_size):
        chunk = all_dist_rows[i:i+chunk_size]
        insert_sql = f"""
        INSERT INTO commodity.forecast.distributions
        (path_id, forecast_start_date, data_cutoff_date, generation_timestamp,
         model_version, commodity, day_1, day_2, day_3, day_4, day_5, day_6, day_7,
         day_8, day_9, day_10, day_11, day_12, day_13, day_14, is_actuals, has_data_leakage)
        VALUES {', '.join(chunk)}
        """
        cursor.execute(insert_sql)

    # ============================================================
    # 2. POINT_FORECASTS TABLE - Write in chunks of 1000
    # ============================================================
    if all_point_rows:
        chunk_size = 1000
        for i in range(0, len(all_point_rows), chunk_size):
            chunk = all_point_rows[i:i+chunk_size]
            insert_sql = f"""
            INSERT INTO commodity.forecast.point_forecasts
            (forecast_date, data_cutoff_date, generation_timestamp, day_ahead,
             forecast_mean, forecast_std, lower_95, upper_95, model_version,
             commodity, model_success, actual_close, has_data_leakage)
            VALUES {', '.join(chunk)}
            """
            cursor.execute(insert_sql)

    # ============================================================
    # 3. ACTUALS TABLE - Fetch and write
    # ============================================================
    total_actuals = 0
    for forecast_start_date, commodity in actuals_to_fetch:
        cursor.execute(f"""
            SELECT date, close
            FROM commodity.bronze.market
            WHERE commodity = '{commodity}'
              AND date >= '{forecast_start_date}'
              AND date < '{forecast_start_date + timedelta(days=14)}'
            ORDER BY date
        """)

        actuals = cursor.fetchall()
        if actuals:
            for actual_date, actual_close in actuals:
                try:
                    cursor.execute(f"""
                        INSERT INTO commodity.forecast.forecast_actuals
                        (forecast_date, commodity, actual_close)
                        VALUES ('{actual_date}', '{commodity}', {actual_close:.2f})
                    """)
                except Exception:
                    # Duplicate key, skip
                    pass
            total_actuals += len(actuals)

    cursor.close()
    print(f"       ‚úÖ Batch wrote {len(batch_data)} forecasts ({len(all_dist_rows):,} paths, {len(all_point_rows):,} points, {total_actuals} actuals)")


def write_all_tables(
    connection,
    forecast_start_date: date,
    data_cutoff_date: date,
    paths: List[Dict],
    mean_forecast: np.ndarray,
    forecast_std: float,
    model_version: str,
    commodity: str
):
    """
    DEPRECATED: Use write_batch_to_tables() instead for better performance.

    This function kept for backward compatibility but converts to batch format.
    """
    batch_data = [{
        'forecast_start_date': forecast_start_date,
        'data_cutoff_date': data_cutoff_date,
        'paths': paths,
        'mean_forecast': mean_forecast,
        'forecast_std': forecast_std,
        'model_version': model_version,
        'commodity': commodity
    }]
    write_batch_to_tables(connection, batch_data)


def get_existing_forecast_dates(connection, commodity: str, model_version: str) -> set:
    """Get set of dates that already have forecasts."""
    cursor = connection.cursor()
    cursor.execute(f"""
        SELECT DISTINCT forecast_start_date
        FROM commodity.forecast.distributions
        WHERE commodity = '{commodity}'
          AND model_version = '{model_version}'
          AND is_actuals = FALSE
    """)
    existing = {row[0] for row in cursor.fetchall()}
    cursor.close()
    return existing


def backfill_rolling_window(
    commodity: str,
    model_versions: List[str],
    train_frequency: str = 'semiannually',
    start_date: Optional[date] = None,
    end_date: Optional[date] = None,
    min_training_days: int = 365 * 3,
    use_pretrained: bool = False,
    model_version_tag: str = 'v1.0',
    databricks_host: str = None,
    databricks_token: str = None,
    databricks_http_path: str = None
):
    """
    Main rolling window backfill orchestration.

    Strategy:
        1. Generate training dates based on frequency (e.g., every 6 months)
        2. For each training date:
           - Train model on all data up to that date
           - Generate forecasts for all days until next training date
        3. Populate all 3 tables (distributions, point_forecasts, actuals)
    """
    # Load credentials
    if not databricks_host:
        databricks_host = os.getenv('DATABRICKS_HOST')
    if not databricks_token:
        databricks_token = os.getenv('DATABRICKS_TOKEN')
    if not databricks_http_path:
        # Prefer cluster HTTP path for long-running jobs (no 15-min timeout)
        databricks_http_path = os.getenv('DATABRICKS_CLUSTER_HTTP_PATH') or os.getenv('DATABRICKS_HTTP_PATH')

    connection = sql.connect(
        server_hostname=databricks_host.replace('https://', ''),
        http_path=databricks_http_path,
        access_token=databricks_token
    )

    cursor = connection.cursor()

    # Get earliest data date
    cursor.execute(f"""
        SELECT MIN(date) as earliest_date
        FROM commodity.bronze.market
        WHERE commodity = '{commodity}'
    """)
    earliest_data_date = cursor.fetchall()[0][0]

    if start_date is None:
        start_date = earliest_data_date + timedelta(days=min_training_days)

    if end_date is None:
        end_date = date.today() - timedelta(days=1)

    print(f"\n{'='*80}")
    print(f"Rolling Window Backfill - {commodity}")
    print(f"{'='*80}")
    print(f"Train frequency: {train_frequency}")
    print(f"Models: {', '.join(model_versions)}")
    print(f"Date range: {start_date} to {end_date}")

    # Generate training dates
    training_dates = get_training_dates(start_date, end_date, train_frequency)
    print(f"\nüìÖ Training Schedule: {len(training_dates)} model trainings")
    print(f"   First: {training_dates[0]}")
    print(f"   Last: {training_dates[-1]}")

    # Calculate total forecasts (all days between start and end)
    total_forecast_days = (end_date - start_date).days + 1
    print(f"   Total forecast days: {total_forecast_days:,}")
    print(f"   Ratio: 1 training per {total_forecast_days / len(training_dates):.1f} forecasts")

    for model_version in model_versions:
        print(f"\n{'='*80}")
        print(f"üîß Model: {model_version}")
        print(f"{'='*80}")

        if model_version not in BASELINE_MODELS:
            print(f"  ‚ö†Ô∏è  Model not found in registry, skipping")
            continue

        model_config = BASELINE_MODELS[model_version]

        # Get existing forecasts for resume capability
        existing_dates = get_existing_forecast_dates(connection, commodity, model_version)
        print(f"\n  üìã Resume mode: {len(existing_dates)} forecasts already exist, will skip these")

        success_count = 0
        error_count = 0
        skipped_count = 0

        # Rolling window: train periodically, forecast daily
        for train_idx, train_date in enumerate(training_dates, 1):
            cutoff_date = train_date - timedelta(days=1)

            # Determine forecast window (until next training date or end)
            if train_idx < len(training_dates):
                next_train_date = training_dates[train_idx]
                forecast_end = next_train_date - timedelta(days=1)
            else:
                forecast_end = end_date

            print(f"\n  [{train_idx}/{len(training_dates)}] Training on {train_date}")
            print(f"     Data cutoff: {cutoff_date}")

            # Check connection and reconnect if needed
            connection = reconnect_if_needed(connection, databricks_host, databricks_token, databricks_http_path)

            # Try to load pretrained model if use_pretrained is True
            fitted_model_dict = None
            if use_pretrained:
                model_name = model_config['name']
                training_date_str = train_date.strftime('%Y-%m-%d')

                print(f"     Loading pretrained model from database...")
                try:
                    loaded_model_data = load_model(
                        connection=connection,
                        commodity=commodity,
                        model_name=model_name,
                        training_date=training_date_str,
                        model_version=model_version_tag
                    )

                    if loaded_model_data:
                        fitted_model_dict = loaded_model_data['fitted_model']
                        print(f"     ‚úÖ Loaded pretrained model (trained on {training_date_str})")
                    else:
                        print(f"     ‚ö†Ô∏è  Pretrained model not found, falling back to training")
                        use_pretrained = False  # Fall back to training for this window
                except Exception as e:
                    print(f"     ‚ö†Ô∏è  Error loading pretrained model: {e}")
                    use_pretrained = False  # Fall back to training

            # Load training data
            # - For inference with pretrained models: only load last 90 days (fast)
            # - For training: load all historical data (slow but necessary)
            lookback_days = 90 if use_pretrained else None
            training_df = load_training_data(connection, commodity, cutoff_date, lookback_days=lookback_days)
            print(f"     Training samples: {len(training_df)} days")

            # Train model if not using pretrained
            if not use_pretrained:
                train_result = train_model(training_df, model_config, commodity)
                if not train_result['success']:
                    print(f"     ‚ùå Training failed: {train_result['error']}")
                    error_count += 1
                    continue
                print(f"     ‚úÖ Model trained successfully")
            else:
                print(f"     ‚è≠Ô∏è  Skipping training (using pretrained model)")

            # Generate forecasts for all days until next training
            forecast_dates = []
            current = train_date
            while current <= forecast_end:
                forecast_dates.append(current)
                current += timedelta(days=1)

            print(f"     Generating {len(forecast_dates)} daily forecasts (reusing trained model)...")

            # Batch writing for speed (10-20x faster)
            batch_data = []
            batch_size = 50  # Write every 50 forecasts

            for i, forecast_date in enumerate(forecast_dates, 1):
                # Skip if already exists (resume mode)
                if forecast_date in existing_dates:
                    skipped_count += 1
                    if skipped_count % 50 == 0:
                        print(f"       Skipped: {skipped_count} (resume mode)")
                    continue

                # Use pretrained model if available, otherwise train+predict
                result = generate_forecast_for_date(
                    forecast_date,
                    training_df,
                    model_config,
                    commodity,
                    fitted_model=fitted_model_dict
                )

                if not result['success']:
                    print(f"       [{i}/{len(forecast_dates)}] {forecast_date}: ‚ùå {result['error']}")
                    error_count += 1
                    continue

                # Add to batch instead of writing immediately
                batch_data.append({
                    'forecast_start_date': forecast_date,
                    'data_cutoff_date': cutoff_date,
                    'paths': result['paths'],
                    'mean_forecast': result['mean_forecast'],
                    'forecast_std': result['forecast_std'],
                    'model_version': model_version,
                    'commodity': commodity
                })

                # Flush batch when full
                if len(batch_data) >= batch_size:
                    try:
                        connection = reconnect_if_needed(connection, databricks_host, databricks_token, databricks_http_path)
                        write_batch_to_tables(connection, batch_data)
                        success_count += len(batch_data)
                        batch_data = []  # Clear batch
                    except Exception as e:
                        print(f"       ‚ùå Batch write failed: {e}")
                        error_count += len(batch_data)
                        batch_data = []

                if (success_count + skipped_count) % 50 == 0:
                    print(f"       Progress: {success_count} new + {skipped_count} skipped = {success_count + skipped_count} total")

            # Flush remaining batch at end of training window
            if batch_data:
                try:
                    connection = reconnect_if_needed(connection, databricks_host, databricks_token, databricks_http_path)
                    write_batch_to_tables(connection, batch_data)
                    success_count += len(batch_data)
                    batch_data = []
                except Exception as e:
                    print(f"       ‚ùå Final batch write failed: {e}")
                    error_count += len(batch_data)

        print(f"\n  ‚úÖ Completed {model_version}")
        print(f"     New forecasts: {success_count:,}")
        print(f"     Skipped (existing): {skipped_count:,}")
        print(f"     Errors: {error_count}")

    connection.close()

    print(f"\n{'='*80}")
    print(f"Backfill Complete!")
    print(f"{'='*80}")


def main():
    parser = argparse.ArgumentParser(description='Rolling window forecast backfill')
    parser.add_argument('--commodity', required=True, choices=['Coffee', 'Sugar'])
    parser.add_argument('--models', nargs='+', required=True,
                       help='Model versions to backfill')
    parser.add_argument('--train-frequency', default='semiannually',
                       choices=['daily', 'weekly', 'biweekly', 'monthly', 'quarterly', 'semiannually', 'annually'],
                       help='How often to retrain models (default: semiannually)')
    parser.add_argument('--start-date', type=lambda s: datetime.strptime(s, '%Y-%m-%d').date(),
                       help='First training date (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=lambda s: datetime.strptime(s, '%Y-%m-%d').date(),
                       help='Last forecast date (YYYY-MM-DD)')
    parser.add_argument('--train-all-forecasts', action='store_true',
                       help='Train a new model for each forecast date (slow, ~180x slower than default). Default behavior uses pretrained models.')
    parser.add_argument('--model-version-tag', default='v1.0',
                       help='Model version tag for pretrained models (default: v1.0)')

    args = parser.parse_args()

    backfill_rolling_window(
        commodity=args.commodity,
        model_versions=args.models,
        train_frequency=args.train_frequency,
        start_date=args.start_date,
        end_date=args.end_date,
        use_pretrained=not args.train_all_forecasts,
        model_version_tag=args.model_version_tag
    )


if __name__ == '__main__':
    main()
