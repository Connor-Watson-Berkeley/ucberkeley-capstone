{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bef9fc8f-04c9-40e7-b79a-29016e66ba55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_setup_and_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2412751f-c29b-4de5-b24e-d7b50a51a29b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nFEATURE IMPORTANCE ANALYSIS: COFFEE\n================================================================================\n\nDiscovering model versions...\n‚úì Found 16 model versions\n\n--------------------------------------------------------------------------------\nMODEL: random_walk_v1_test\n--------------------------------------------------------------------------------\n\nLoading prepared data...\n‚úì Loaded 965 days of prices\n‚úì Loaded 1 prediction matrices\n\nExtracting features...\n‚ö†Ô∏è  No features extracted\n\n--------------------------------------------------------------------------------\nMODEL: arima_v1\n--------------------------------------------------------------------------------\n\nLoading prepared data...\n‚úì Loaded 965 days of prices\n‚úì Loaded 1 prediction matrices\n\nExtracting features...\n‚ö†Ô∏è  No features extracted\n\n--------------------------------------------------------------------------------\nMODEL: sarimax_auto_weather_v1\n--------------------------------------------------------------------------------\n\nLoading prepared data...\n‚úì Loaded 965 days of prices\n‚úì Loaded 41 prediction matrices\n\nExtracting features...\n‚ö†Ô∏è  No features extracted\n\n--------------------------------------------------------------------------------\nMODEL: xgboost_weather_v1\n--------------------------------------------------------------------------------\n\nLoading prepared data...\n‚úì Loaded 965 days of prices\n‚úì Loaded 41 prediction matrices\n\nExtracting features...\n‚ö†Ô∏è  No features extracted\n\n--------------------------------------------------------------------------------\nMODEL: xgboost\n--------------------------------------------------------------------------------\n\nLoading prepared data...\n‚úì Loaded 965 days of prices\n‚úì Loaded 260 prediction matrices\n\nExtracting features...\n‚ö†Ô∏è  No features extracted\n\n--------------------------------------------------------------------------------\nMODEL: arima_111_v1\n--------------------------------------------------------------------------------\n\nLoading prepared data...\n‚úì Loaded 965 days of prices\n‚úì Loaded 41 prediction matrices\n\nExtracting features...\n‚ö†Ô∏è  No features extracted\n\n--------------------------------------------------------------------------------\nMODEL: synthetic_acc70\n--------------------------------------------------------------------------------\n\nLoading prepared data...\n‚úì Loaded 965 days of prices\n‚úì Loaded 700 prediction matrices\n\nExtracting features...\n‚úì Extracted features for 700 days\n\nTraining Random Forest model...\nüèÉ View run unleashed-pug-94 at: https://dbc-5e4780f4-fcec.cloud.databricks.com/ml/experiments/62668c78720e4078a87849c5966f32af/runs/630e7406227342f2a75c6eb5426e93d6\nüß™ View experiment at: https://dbc-5e4780f4-fcec.cloud.databricks.com/ml/experiments/62668c78720e4078a87849c5966f32af\n‚úì Model trained\n  R¬≤ score: 1.000\n  CV R¬≤ score: 0.999 (+/- 0.001)\n\nAnalyzing feature importance...\n\nFeature Importances:\n  expected_return          : 0.452\n  directional_consensus    : 0.327\n  downside_risk            : 0.220\n  uncertainty              : 0.000\n  prediction_range         : 0.000\n  skewness                 : 0.000\n\nGenerating visualization...\n‚úì Saved: /Volumes/commodity/trading_agent/files/feature_importance_coffee_synthetic_acc70.png\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nSaving results...\n‚úì Saved: /Volumes/commodity/trading_agent/files/feature_analysis_coffee_synthetic_acc70.pkl\n‚úì Saved: /Volumes/commodity/trading_agent/files/feature_importance_coffee_synthetic_acc70.csv\n\n‚úì Feature analysis complete for synthetic_acc70\n\n--------------------------------------------------------------------------------\nMODEL: sarimax_weather_v1\n--------------------------------------------------------------------------------\n\nLoading prepared data...\n‚úì Loaded 965 days of prices\n‚úì Loaded 1 prediction matrices\n\nExtracting features...\n‚ö†Ô∏è  No features extracted\n\n--------------------------------------------------------------------------------\nMODEL: prophet_v1\n--------------------------------------------------------------------------------\n\nLoading prepared data...\n‚úì Loaded 965 days of prices\n‚úì Loaded 42 prediction matrices\n\nExtracting features...\n‚ö†Ô∏è  No features extracted\n\n--------------------------------------------------------------------------------\nMODEL: synthetic_acc80\n--------------------------------------------------------------------------------\n\nLoading prepared data...\n‚úì Loaded 965 days of prices\n‚úì Loaded 700 prediction matrices\n\nExtracting features...\n‚úì Extracted features for 700 days\n\nTraining Random Forest model...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nCOMMODITY CONFIGURATIONS\n================================================================================\n\nCOFFEE:\n  Harvest volume: 50 tons/year\n  Harvest windows: [(5, 9)]\n  Total harvest weeks: 21\n  Weekly harvest rate: 2.38 tons/week\n\n  Costs (percentage-based):\n    Storage: 0.025% of value per day\n    Transaction: 0.25% of sale value\n    Max holding: 365 days from harvest start\n\n  Example at $150/ton:\n    Transaction cost (full harvest): $18.75\n    Storage per day (full harvest): $1.88\n    Storage per month (full harvest): $56.25\n    Storage for 6 months: $337.50\n\nReal Prediction Data:\n  ‚úì Real prediction data found in table: commodity.forecast.distributions\n  Model versions available: 12\n    - arima_111_v1\n    - arima_v1\n    - naive\n    - naive_baseline\n    - prophet_v1\n    - random_walk_baseline\n    - random_walk_v1\n    - random_walk_v1_test\n    - sarimax_auto_weather_v1\n    - sarimax_weather_v1\n    - xgboost\n    - xgboost_weather_v1\n\nSUGAR:\n  Harvest volume: 50 tons/year\n  Harvest windows: [(10, 12)]\n  Total harvest weeks: 12\n  Weekly harvest rate: 4.17 tons/week\n\n  Costs (percentage-based):\n    Storage: 0.025% of value per day\n    Transaction: 0.25% of sale value\n    Max holding: 365 days from harvest start\n\n  Example at $400/ton:\n    Transaction cost (full harvest): $50.00\n    Storage per day (full harvest): $5.00\n    Storage per month (full harvest): $150.00\n    Storage for 6 months: $900.00\n\nReal Prediction Data:\n  ‚úì Real prediction data found in table: commodity.forecast.distributions\n  Model versions available: 5\n    - arima_111_v1\n    - prophet_v1\n    - random_walk_v1\n    - sarimax_auto_weather_v1\n    - xgboost_weather_v1\n\n================================================================================\nSTRATEGY PARAMETERS (SHARED)\n================================================================================\n\nBaseline Strategy Parameters:\n{\n  \"equal_batch\": {\n    \"batch_size\": 0.25,\n    \"frequency_days\": 30\n  },\n  \"price_threshold\": {\n    \"threshold_pct\": 0.05\n  },\n  \"moving_average\": {\n    \"ma_period\": 30\n  }\n}\n\nPrediction Strategy Parameters:\n{\n  \"consensus\": {\n    \"consensus_threshold\": 0.7,\n    \"min_return\": 0.03,\n    \"evaluation_day\": 14\n  },\n  \"expected_value\": {\n    \"min_ev_improvement\": 50,\n    \"baseline_batch\": 0.15,\n    \"baseline_frequency\": 10\n  },\n  \"risk_adjusted\": {\n    \"min_return\": 0.03,\n    \"max_uncertainty\": 0.35,\n    \"consensus_threshold\": 0.6,\n    \"evaluation_day\": 14\n  }\n}\n\n================================================================================\nANALYSIS CONFIGURATION\n================================================================================\n{\n  \"backtest_start_date\": \"2018-01-01\",\n  \"backtest_end_date\": \"2025-09-24\",\n  \"bootstrap_iterations\": 1000,\n  \"confidence_level\": 0.95,\n  \"random_seed\": 42,\n  \"prediction_runs\": 500,\n  \"forecast_horizon\": 14\n}\n\nForecast table: commodity.forecast.distributions\nOutput schema: commodity.trading_agent\nVolume path: /Volumes/commodity/trading_agent/files\n\n================================================================================\nCOMMODITIES TO ANALYZE\n================================================================================\nWill run analysis for: ['coffee', 'sugar']\n\n‚úì Configuration complete\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:132)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:132)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:750)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:84)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:728)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:913)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:939)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:938)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:993)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:778)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:132)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:132)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:750)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:84)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:728)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:913)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:939)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:938)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:993)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:778)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTEBOOK 07: FEATURE IMPORTANCE ANALYSIS (MULTI-MODEL)\n",
    "# ============================================================================\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Feature Importance Analysis - All Commodities and Model Versions\n",
    "# MAGIC \n",
    "# MAGIC Analyzes which prediction features are most important for forecasting returns.\n",
    "# MAGIC Runs for all configured commodities and model versions.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./00_setup_and_config\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Feature Extraction and Analysis\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def extract_features(predictions, current_price, eval_day=14):\n",
    "    \"\"\"\n",
    "    Extract features from prediction ensemble for a given evaluation day.\n",
    "    \n",
    "    Args:\n",
    "        predictions: N√óH matrix of predictions (N runs √ó H horizons)\n",
    "        current_price: Current market price\n",
    "        eval_day: Which day ahead to evaluate (1-14)\n",
    "    \n",
    "    Returns:\n",
    "        dict of features or None if invalid\n",
    "    \"\"\"\n",
    "    if predictions is None or len(predictions) == 0:\n",
    "        return None\n",
    "    \n",
    "    day_preds = predictions[:, eval_day - 1]\n",
    "    \n",
    "    return {\n",
    "        'directional_consensus': np.mean(day_preds > current_price),\n",
    "        'expected_return': (np.median(day_preds) - current_price) / current_price,\n",
    "        'uncertainty': (np.percentile(day_preds, 75) - np.percentile(day_preds, 25)) / np.median(day_preds),\n",
    "        'skewness': float(pd.Series(day_preds).skew()),\n",
    "        'prediction_range': (np.max(day_preds) - np.min(day_preds)) / current_price,\n",
    "        'downside_risk': (np.percentile(day_preds, 10) - current_price) / current_price\n",
    "    }\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Loop through all commodities\n",
    "for CURRENT_COMMODITY in COMMODITY_CONFIGS.keys():\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"FEATURE IMPORTANCE ANALYSIS: {CURRENT_COMMODITY.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Discover all model versions for this commodity\n",
    "    print(f\"\\nDiscovering model versions...\")\n",
    "    \n",
    "    synthetic_versions = []\n",
    "    try:\n",
    "        DATA_PATHS = get_data_paths(CURRENT_COMMODITY)\n",
    "        synthetic_df = spark.table(DATA_PATHS['predictions']).select(\"model_version\").distinct()\n",
    "        synthetic_versions = [row.model_version for row in synthetic_df.collect()]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    real_versions = []\n",
    "    try:\n",
    "        real_versions = get_model_versions(CURRENT_COMMODITY)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    all_model_versions = list(set(synthetic_versions + real_versions))\n",
    "    \n",
    "    if len(all_model_versions) == 0:\n",
    "        print(f\"‚ö†Ô∏è  No model versions found for {CURRENT_COMMODITY}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"‚úì Found {len(all_model_versions)} model versions\")\n",
    "    \n",
    "    # Loop through each model version\n",
    "    for MODEL_VERSION in all_model_versions:\n",
    "        print(f\"\\n{'-' * 80}\")\n",
    "        print(f\"MODEL: {MODEL_VERSION}\")\n",
    "        print(f\"{'-' * 80}\")\n",
    "        \n",
    "        MODEL_DATA_PATHS = get_data_paths(CURRENT_COMMODITY, MODEL_VERSION)\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Load Data\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nLoading prepared data...\")\n",
    "        \n",
    "        try:\n",
    "            prices = spark.table(get_data_paths(CURRENT_COMMODITY)['prices_prepared']).toPandas()\n",
    "            prices['date'] = pd.to_datetime(prices['date']).dt.normalize()\n",
    "            prices = prices.reset_index(drop=True)\n",
    "            \n",
    "            # Load prediction matrices for this model version\n",
    "            if MODEL_VERSION.startswith('synthetic_'):\n",
    "                matrices_path = MODEL_DATA_PATHS['prediction_matrices']\n",
    "            else:\n",
    "                matrices_path = MODEL_DATA_PATHS['prediction_matrices_real']\n",
    "            \n",
    "            with open(matrices_path, 'rb') as f:\n",
    "                prediction_matrices = pickle.load(f)\n",
    "            \n",
    "            # Normalize prediction matrix keys\n",
    "            prediction_matrices = {\n",
    "                pd.Timestamp(k).normalize(): v \n",
    "                for k, v in prediction_matrices.items()\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úì Loaded {len(prices)} days of prices\")\n",
    "            print(f\"‚úì Loaded {len(prediction_matrices)} prediction matrices\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not load data: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Build Feature Dataset\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nExtracting features...\")\n",
    "        \n",
    "        eval_day = 14\n",
    "        feature_data = []\n",
    "        \n",
    "        # Create date-to-index mapping\n",
    "        date_to_idx = {date: idx for idx, date in enumerate(prices['date'])}\n",
    "        \n",
    "        # Iterate over prediction matrices\n",
    "        for pred_date, predictions in prediction_matrices.items():\n",
    "            if pred_date not in date_to_idx:\n",
    "                continue\n",
    "            \n",
    "            current_idx = date_to_idx[pred_date]\n",
    "            \n",
    "            # Need enough future data\n",
    "            if current_idx + eval_day >= len(prices):\n",
    "                continue\n",
    "            \n",
    "            current_price = prices.loc[current_idx, 'price']\n",
    "            future_price = prices.loc[current_idx + eval_day, 'price']\n",
    "            \n",
    "            features = extract_features(predictions, current_price, eval_day)\n",
    "            if features is None:\n",
    "                continue\n",
    "            \n",
    "            actual_return = (future_price - current_price) / current_price\n",
    "            \n",
    "            feature_data.append({\n",
    "                'date': pred_date,\n",
    "                'current_price': current_price,\n",
    "                'actual_return': actual_return,\n",
    "                **features\n",
    "            })\n",
    "        \n",
    "        if len(feature_data) == 0:\n",
    "            print(\"‚ö†Ô∏è  No features extracted\")\n",
    "            continue\n",
    "        \n",
    "        feature_df = pd.DataFrame(feature_data)\n",
    "        print(f\"‚úì Extracted features for {len(feature_df)} days\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Train Random Forest Model\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nTraining Random Forest model...\")\n",
    "        \n",
    "        feature_cols = ['directional_consensus', 'expected_return', 'uncertainty', \n",
    "                       'skewness', 'prediction_range', 'downside_risk']\n",
    "        \n",
    "        X = feature_df[feature_cols]\n",
    "        y = feature_df['actual_return']\n",
    "        \n",
    "        rf_model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        rf_model.fit(X, y)\n",
    "        \n",
    "        cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='r2')\n",
    "        \n",
    "        print(f\"‚úì Model trained\")\n",
    "        print(f\"  R¬≤ score: {rf_model.score(X, y):.3f}\")\n",
    "        print(f\"  CV R¬≤ score: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Feature Importance\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nAnalyzing feature importance...\")\n",
    "        \n",
    "        importances = rf_model.feature_importances_\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nFeature Importances:\")\n",
    "        for _, row in importance_df.iterrows():\n",
    "            print(f\"  {row['feature']:25s}: {row['importance']:.3f}\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Visualization\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nGenerating visualization...\")\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        ax.barh(importance_df['feature'], importance_df['importance'], color='steelblue', alpha=0.7)\n",
    "        ax.set_xlabel('Importance', fontsize=12)\n",
    "        ax.set_title(f'Feature Importance - {CURRENT_COMMODITY.upper()} - {MODEL_VERSION}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        viz_path = f'{VOLUME_PATH}/feature_importance_{CURRENT_COMMODITY}_{MODEL_VERSION}.png'\n",
    "        plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"‚úì Saved: {viz_path}\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Save Results\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nSaving results...\")\n",
    "        \n",
    "        feature_analysis = {\n",
    "            'commodity': CURRENT_COMMODITY,\n",
    "            'model_version': MODEL_VERSION,\n",
    "            'feature_importance': importance_df,\n",
    "            'model': rf_model,\n",
    "            'feature_data': feature_df,\n",
    "            'cv_scores': cv_scores,\n",
    "            'r2_score': rf_model.score(X, y)\n",
    "        }\n",
    "        \n",
    "        with open(MODEL_DATA_PATHS['feature_analysis'], 'wb') as f:\n",
    "            pickle.dump(feature_analysis, f)\n",
    "        \n",
    "        print(f\"‚úì Saved: {MODEL_DATA_PATHS['feature_analysis']}\")\n",
    "        \n",
    "        importance_csv = f'{VOLUME_PATH}/feature_importance_{CURRENT_COMMODITY}_{MODEL_VERSION}.csv'\n",
    "        importance_df.to_csv(importance_csv, index=False)\n",
    "        print(f\"‚úì Saved: {importance_csv}\")\n",
    "        \n",
    "        print(f\"\\n‚úì Feature analysis complete for {MODEL_VERSION}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"‚úì {CURRENT_COMMODITY.upper()} COMPLETE\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL FEATURE IMPORTANCE ANALYSES COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Commodities analyzed: {', '.join([c.upper() for c in COMMODITY_CONFIGS.keys()])}\")\n",
    "print(\"\\n‚úì Feature importance analysis complete for all commodities and models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a38a5c12-7f0b-4641-afc9-e8d9c6f3b8ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prices: 965 days (2022-01-03 00:00:00 to 2025-10-31 00:00:00)\nPredictions: 42\nPrediction range: 2018-07-06 00:00:00 to 2025-11-01 00:00:00\n\nChecking all 42 predictions:\n\nSummary:\n  Not in price data: 42\n  Too late to look ahead: 0\n  Valid: 0\n"
     ]
    }
   ],
   "source": [
    "# PROPER DIAGNOSTIC - Check overlap correctly\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "TEST_COMMODITY = 'coffee'\n",
    "TEST_MODEL = 'prophet_v1'\n",
    "\n",
    "MODEL_DATA_PATHS = get_data_paths(TEST_COMMODITY, TEST_MODEL)\n",
    "\n",
    "prices = spark.table(get_data_paths(TEST_COMMODITY)['prices_prepared']).toPandas()\n",
    "prices['date'] = pd.to_datetime(prices['date']).dt.normalize()\n",
    "prices = prices.reset_index(drop=True)\n",
    "\n",
    "matrices_path = MODEL_DATA_PATHS['prediction_matrices_real']\n",
    "with open(matrices_path, 'rb') as f:\n",
    "    prediction_matrices = pickle.load(f)\n",
    "\n",
    "prediction_matrices = {\n",
    "    pd.Timestamp(k).normalize(): v \n",
    "    for k, v in prediction_matrices.items()\n",
    "}\n",
    "\n",
    "eval_day = 14\n",
    "date_to_idx = {date: idx for idx, date in enumerate(prices['date'])}\n",
    "\n",
    "print(f\"Prices: {len(prices)} days ({prices['date'].min()} to {prices['date'].max()})\")\n",
    "print(f\"Predictions: {len(prediction_matrices)}\")\n",
    "\n",
    "pred_dates = sorted(prediction_matrices.keys())\n",
    "print(f\"Prediction range: {pred_dates[0]} to {pred_dates[-1]}\")\n",
    "\n",
    "print(f\"\\nChecking all 42 predictions:\")\n",
    "not_in_prices = 0\n",
    "too_late = 0\n",
    "valid = 0\n",
    "\n",
    "for pred_date in pred_dates:\n",
    "    if pred_date not in date_to_idx:\n",
    "        not_in_prices += 1\n",
    "        continue\n",
    "    \n",
    "    idx = date_to_idx[pred_date]\n",
    "    if idx + eval_day >= len(prices):\n",
    "        too_late += 1\n",
    "        print(f\"  {pred_date}: idx={idx}, TOO LATE (idx+14={idx+14} >= {len(prices)})\")\n",
    "    else:\n",
    "        valid += 1\n",
    "        print(f\"  {pred_date}: idx={idx}, VALID\")\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Not in price data: {not_in_prices}\")\n",
    "print(f\"  Too late to look ahead: {too_late}\")\n",
    "print(f\"  Valid: {valid}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07_feature_importance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}