{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bef9fc8f-04c9-40e7-b79a-29016e66ba55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_setup_and_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2412751f-c29b-4de5-b24e-d7b50a51a29b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NOTEBOOK 07: FEATURE IMPORTANCE ANALYSIS (MULTI-MODEL)\n",
    "# ============================================================================\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Feature Importance Analysis - All Commodities and Model Versions\n",
    "# MAGIC \n",
    "# MAGIC Analyzes which prediction features are most important for forecasting returns.\n",
    "# MAGIC Runs for all configured commodities and model versions.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./00_setup_and_config\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Feature Extraction and Analysis\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def extract_features(predictions, current_price, eval_day=14):\n",
    "    \"\"\"\n",
    "    Extract features from prediction ensemble for a given evaluation day.\n",
    "    \n",
    "    Args:\n",
    "        predictions: N×H matrix of predictions (N runs × H horizons)\n",
    "        current_price: Current market price\n",
    "        eval_day: Which day ahead to evaluate (1-14)\n",
    "    \n",
    "    Returns:\n",
    "        dict of features or None if invalid\n",
    "    \"\"\"\n",
    "    if predictions is None or len(predictions) == 0:\n",
    "        return None\n",
    "    \n",
    "    day_preds = predictions[:, eval_day - 1]\n",
    "    \n",
    "    return {\n",
    "        'directional_consensus': np.mean(day_preds > current_price),\n",
    "        'expected_return': (np.median(day_preds) - current_price) / current_price,\n",
    "        'uncertainty': (np.percentile(day_preds, 75) - np.percentile(day_preds, 25)) / np.median(day_preds),\n",
    "        'skewness': float(pd.Series(day_preds).skew()),\n",
    "        'prediction_range': (np.max(day_preds) - np.min(day_preds)) / current_price,\n",
    "        'downside_risk': (np.percentile(day_preds, 10) - current_price) / current_price\n",
    "    }\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Loop through all commodities\n",
    "for CURRENT_COMMODITY in COMMODITY_CONFIGS.keys():\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"FEATURE IMPORTANCE ANALYSIS: {CURRENT_COMMODITY.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Discover all model versions for this commodity\n",
    "    print(f\"\\nDiscovering model versions...\")\n",
    "    \n",
    "    synthetic_versions = []\n",
    "    try:\n",
    "        DATA_PATHS = get_data_paths(CURRENT_COMMODITY)\n",
    "        synthetic_df = spark.table(DATA_PATHS['predictions']).select(\"model_version\").distinct()\n",
    "        synthetic_versions = [row.model_version for row in synthetic_df.collect()]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    real_versions = []\n",
    "    try:\n",
    "        real_versions = get_model_versions(CURRENT_COMMODITY)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    all_model_versions = list(set(synthetic_versions + real_versions))\n",
    "    \n",
    "    if len(all_model_versions) == 0:\n",
    "        print(f\"⚠️  No model versions found for {CURRENT_COMMODITY}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"✓ Found {len(all_model_versions)} model versions\")\n",
    "    \n",
    "    # Loop through each model version\n",
    "    for MODEL_VERSION in all_model_versions:\n",
    "        print(f\"\\n{'-' * 80}\")\n",
    "        print(f\"MODEL: {MODEL_VERSION}\")\n",
    "        print(f\"{'-' * 80}\")\n",
    "        \n",
    "        MODEL_DATA_PATHS = get_data_paths(CURRENT_COMMODITY, MODEL_VERSION)\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Load Data\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nLoading prepared data...\")\n",
    "        \n",
    "        try:\n",
    "            # Load FULL price history from bronze market table\n",
    "            prices = spark.table('commodity.bronze.market') \\\n",
    "                .filter(f\"commodity = '{CURRENT_COMMODITY}'\") \\\n",
    "                .select(\"date\", \"close\") \\\n",
    "                .toPandas()\n",
    "            \n",
    "            prices = prices.rename(columns={'close': 'price'})\n",
    "            prices['date'] = pd.to_datetime(prices['date']).dt.normalize()\n",
    "            prices = prices.sort_values('date').reset_index(drop=True)\n",
    "            \n",
    "            # Load prediction matrices for this model version\n",
    "            if MODEL_VERSION.startswith('synthetic_'):\n",
    "                matrices_path = MODEL_DATA_PATHS['prediction_matrices']\n",
    "            else:\n",
    "                matrices_path = MODEL_DATA_PATHS['prediction_matrices_real']\n",
    "            \n",
    "            with open(matrices_path, 'rb') as f:\n",
    "                prediction_matrices = pickle.load(f)\n",
    "            \n",
    "            # Normalize prediction matrix keys\n",
    "            prediction_matrices = {\n",
    "                pd.Timestamp(k).normalize(): v \n",
    "                for k, v in prediction_matrices.items()\n",
    "            }\n",
    "            \n",
    "            print(f\"✓ Loaded {len(prices)} days of prices\")\n",
    "            print(f\"  Date range: {prices['date'].min()} to {prices['date'].max()}\")\n",
    "            print(f\"✓ Loaded {len(prediction_matrices)} prediction matrices\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Could not load data: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Build Feature Dataset\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nExtracting features...\")\n",
    "        \n",
    "        eval_day = 14\n",
    "        feature_data = []\n",
    "        \n",
    "        # Create date-to-index mapping\n",
    "        date_to_idx = {date: idx for idx, date in enumerate(prices['date'])}\n",
    "        \n",
    "        # Iterate over prediction matrices\n",
    "        for pred_date, predictions in prediction_matrices.items():\n",
    "            if pred_date not in date_to_idx:\n",
    "                continue\n",
    "            \n",
    "            current_idx = date_to_idx[pred_date]\n",
    "            \n",
    "            # Need enough future data\n",
    "            if current_idx + eval_day >= len(prices):\n",
    "                continue\n",
    "            \n",
    "            current_price = prices.loc[current_idx, 'price']\n",
    "            future_price = prices.loc[current_idx + eval_day, 'price']\n",
    "            \n",
    "            features = extract_features(predictions, current_price, eval_day)\n",
    "            if features is None:\n",
    "                continue\n",
    "            \n",
    "            actual_return = (future_price - current_price) / current_price\n",
    "            \n",
    "            feature_data.append({\n",
    "                'date': pred_date,\n",
    "                'current_price': current_price,\n",
    "                'actual_return': actual_return,\n",
    "                **features\n",
    "            })\n",
    "        \n",
    "        if len(feature_data) == 0:\n",
    "            print(\"⚠️  No features extracted\")\n",
    "            continue\n",
    "        \n",
    "        feature_df = pd.DataFrame(feature_data)\n",
    "        print(f\"✓ Extracted features for {len(feature_df)} days\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Train Random Forest Model\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nTraining Random Forest model...\")\n",
    "        \n",
    "        feature_cols = ['directional_consensus', 'expected_return', 'uncertainty', \n",
    "                       'skewness', 'prediction_range', 'downside_risk']\n",
    "        \n",
    "        X = feature_df[feature_cols]\n",
    "        y = feature_df['actual_return']\n",
    "        \n",
    "        rf_model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        rf_model.fit(X, y)\n",
    "        \n",
    "        cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='r2')\n",
    "        \n",
    "        print(f\"✓ Model trained\")\n",
    "        print(f\"  R² score: {rf_model.score(X, y):.3f}\")\n",
    "        print(f\"  CV R² score: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Feature Importance\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nAnalyzing feature importance...\")\n",
    "        \n",
    "        importances = rf_model.feature_importances_\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nFeature Importances:\")\n",
    "        for _, row in importance_df.iterrows():\n",
    "            print(f\"  {row['feature']:25s}: {row['importance']:.3f}\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Visualization\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nGenerating visualization...\")\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        ax.barh(importance_df['feature'], importance_df['importance'], color='steelblue', alpha=0.7)\n",
    "        ax.set_xlabel('Importance', fontsize=12)\n",
    "        ax.set_title(f'Feature Importance - {CURRENT_COMMODITY.upper()} - {MODEL_VERSION}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        viz_path = f'{VOLUME_PATH}/feature_importance_{CURRENT_COMMODITY}_{MODEL_VERSION}.png'\n",
    "        plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✓ Saved: {viz_path}\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Save Results\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nSaving results...\")\n",
    "        \n",
    "        feature_analysis = {\n",
    "            'commodity': CURRENT_COMMODITY,\n",
    "            'model_version': MODEL_VERSION,\n",
    "            'feature_importance': importance_df,\n",
    "            'model': rf_model,\n",
    "            'feature_data': feature_df,\n",
    "            'cv_scores': cv_scores,\n",
    "            'r2_score': rf_model.score(X, y)\n",
    "        }\n",
    "        \n",
    "        with open(MODEL_DATA_PATHS['feature_analysis'], 'wb') as f:\n",
    "            pickle.dump(feature_analysis, f)\n",
    "        \n",
    "        print(f\"✓ Saved: {MODEL_DATA_PATHS['feature_analysis']}\")\n",
    "        \n",
    "        importance_csv = f'{VOLUME_PATH}/feature_importance_{CURRENT_COMMODITY}_{MODEL_VERSION}.csv'\n",
    "        importance_df.to_csv(importance_csv, index=False)\n",
    "        print(f\"✓ Saved: {importance_csv}\")\n",
    "        \n",
    "        print(f\"\\n✓ Feature analysis complete for {MODEL_VERSION}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"✓ {CURRENT_COMMODITY.upper()} COMPLETE\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL FEATURE IMPORTANCE ANALYSES COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Commodities analyzed: {', '.join([c.upper() for c in COMMODITY_CONFIGS.keys()])}\")\n",
    "print(\"\\n✓ Feature importance analysis complete for all commodities and models\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07_feature_importance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
