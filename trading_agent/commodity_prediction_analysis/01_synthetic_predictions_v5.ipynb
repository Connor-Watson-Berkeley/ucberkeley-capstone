{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./00_setup_and_config\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Calibrated Synthetic Predictions - All Commodities\n",
    "\n",
    "**Enhanced with comprehensive validation output saved to pickle files:**\n",
    "- Point accuracy: Median prediction has target MAPE (aligned with forecast_agent)\n",
    "- Distribution calibration: Prediction intervals properly calibrated\n",
    "- Includes 100% accurate scenario (perfect foresight for testing)\n",
    "- **NEW**: All validation details saved to pickle files for detailed review\n",
    "\n",
    "**Accuracy levels:**\n",
    "- 100% accurate: MAPE = 0%, MAE = 0 (all predictions exactly match actuals)\n",
    "- 90% accurate: MAPE = 10%\n",
    "- 80% accurate: MAPE = 20%\n",
    "- 70% accurate: MAPE = 30%\n",
    "- 60% accurate: MAPE = 40%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from builtins import min as builtin_min, max as builtin_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SYNTHETIC_START_DATE = '2022-01-01'\n",
    "ACCURACY_LEVELS = [1.00, 0.90, 0.80, 0.70, 0.60]  # 100%, 90%, 80%, 70%, 60%\n",
    "VALIDATION_OUTPUT_FILE = 'validation_results_full.pkl'\n",
    "\n",
    "print(f\"Synthetic prediction configuration:\")\n",
    "print(f\"  Synthetic start date: {SYNTHETIC_START_DATE}\")\n",
    "print(f\"  Accuracy levels: {[f'{a:.0%}' for a in ACCURACY_LEVELS]}\")\n",
    "print(f\"  Validation output: {VALIDATION_OUTPUT_FILE}\")\n",
    "print(f\"\\nAccuracy definition (aligned with forecast_agent):\")\n",
    "print(f\"  - Point forecast: Median has target MAPE\")\n",
    "print(f\"  - Distribution: Calibrated prediction intervals\")\n",
    "print(f\"  - Validation: MAE, MAPE, Directional Accuracy, CRPS (saved to pickle)\")\n",
    "print(f\"  - 100% accurate: Perfect foresight (MAPE = 0%, MAE = 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKET_TABLE = \"commodity.bronze.market\"\n",
    "print(f\"\\nLoading price data from {MARKET_TABLE}...\")\n",
    "\n",
    "market_df = spark.table(MARKET_TABLE).toPandas()\n",
    "market_df['date'] = pd.to_datetime(market_df['date'])\n",
    "\n",
    "print(f\"✓ Loaded market price data (FULL HISTORY)\")\n",
    "commodity_counts = market_df.groupby('commodity').size()\n",
    "print(f\"Available commodities:\")\n",
    "for commodity, count in commodity_counts.items():\n",
    "    print(f\"  - {commodity}: {count} rows\")\n",
    "print(f\"\\nDate range: {market_df['date'].min()} to {market_df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrated Prediction Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_calibrated_predictions(prices_df, model_version, target_accuracy=0.90, \n",
    "                                    n_runs=2000, n_horizons=14, chunk_size=20):\n",
    "    \"\"\"\n",
    "    Generate calibrated synthetic predictions.\n",
    "    \"\"\"\n",
    "    n_dates = len(prices_df) - n_horizons\n",
    "    target_mape = 1.0 - target_accuracy\n",
    "    \n",
    "    print(f\"    Target MAPE: {target_mape:.1%}\")\n",
    "    print(f\"    Calibration: 80% interval should contain actual ~80% of time\")\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    for chunk_start in range(0, n_dates, chunk_size):\n",
    "        chunk_end = builtin_min(chunk_start + chunk_size, n_dates)\n",
    "        chunk_records = []\n",
    "        \n",
    "        for i in range(chunk_start, chunk_end):\n",
    "            current_date = prices_df.loc[i, 'date']\n",
    "            future_prices = prices_df.loc[i+1:i+n_horizons, 'price'].values\n",
    "            \n",
    "            if target_accuracy == 1.0:\n",
    "                predicted_prices_matrix = np.tile(future_prices, (n_runs, 1))\n",
    "            else:\n",
    "                sigma_lognormal = target_mape * np.sqrt(np.pi / 2)\n",
    "                log_errors = np.random.normal(0, sigma_lognormal, (n_runs, n_horizons))\n",
    "                multiplicative_errors = np.exp(log_errors)\n",
    "                future_prices_matrix = np.tile(future_prices, (n_runs, 1))\n",
    "                predicted_prices_matrix = future_prices_matrix * multiplicative_errors\n",
    "                run_biases = np.random.normal(1.0, 0.02, (n_runs, 1))\n",
    "                predicted_prices_matrix *= run_biases\n",
    "            \n",
    "            for run_id in range(1, n_runs + 1):\n",
    "                for day_ahead in range(1, n_horizons + 1):\n",
    "                    chunk_records.append({\n",
    "                        'timestamp': current_date,\n",
    "                        'run_id': run_id,\n",
    "                        'day_ahead': day_ahead,\n",
    "                        'predicted_price': predicted_prices_matrix[run_id-1, day_ahead-1],\n",
    "                        'model_version': model_version\n",
    "                    })\n",
    "        \n",
    "        chunk_df = pd.DataFrame(chunk_records)\n",
    "        all_chunks.append(chunk_df)\n",
    "        del chunk_records\n",
    "        gc.collect()\n",
    "        \n",
    "        if chunk_end % 100 == 0 or chunk_end == n_dates:\n",
    "            print(f\"    Progress: {chunk_end}/{n_dates} dates...\")\n",
    "    \n",
    "    final_df = pd.concat(all_chunks, ignore_index=True)\n",
    "    del all_chunks\n",
    "    gc.collect()\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Functions (forecast_agent-aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_crps(actuals: np.ndarray, forecast_paths: np.ndarray) -> float:\n",
    "    \"\"\"Calculate CRPS (aligned with forecast_agent)\"\"\"\n",
    "    n_paths, horizon = forecast_paths.shape\n",
    "    crps_values = []\n",
    "    \n",
    "    for t in range(horizon):\n",
    "        if np.isnan(actuals[t]):\n",
    "            continue\n",
    "        \n",
    "        actual = actuals[t]\n",
    "        forecast_samples = forecast_paths[:, t]\n",
    "        sorted_samples = np.sort(forecast_samples)\n",
    "        \n",
    "        term1 = np.mean(np.abs(sorted_samples - actual))\n",
    "        n = len(sorted_samples)\n",
    "        indices = np.arange(1, n + 1)\n",
    "        term2 = np.sum((2 * indices - 1) * sorted_samples) / (n ** 2) - np.mean(sorted_samples)\n",
    "        \n",
    "        crps = term1 - 0.5 * term2\n",
    "        crps_values.append(crps)\n",
    "    \n",
    "    return crps_values  # Return list for detailed analysis\n",
    "\n",
    "\n",
    "def calculate_directional_accuracy(actuals: pd.Series, forecasts: pd.Series) -> dict:\n",
    "    \"\"\"Calculate directional accuracy (aligned with forecast_agent)\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    if len(actuals) > 1:\n",
    "        actual_direction = np.sign(actuals.diff().dropna())\n",
    "        forecast_direction = np.sign(forecasts.diff().dropna())\n",
    "        correct_direction = (actual_direction == forecast_direction).sum()\n",
    "        metrics['directional_accuracy'] = float(correct_direction / len(actual_direction) * 100)\n",
    "    \n",
    "    if len(actuals) > 1:\n",
    "        day_0_actual = actuals.iloc[0]\n",
    "        day_0_forecast = forecasts.iloc[0]\n",
    "        \n",
    "        correct_from_day0 = 0\n",
    "        total_from_day0 = 0\n",
    "        \n",
    "        for i in range(1, len(actuals)):\n",
    "            actual_higher = actuals.iloc[i] > day_0_actual\n",
    "            forecast_higher = forecasts.iloc[i] > day_0_forecast\n",
    "            \n",
    "            if actual_higher == forecast_higher:\n",
    "                correct_from_day0 += 1\n",
    "            total_from_day0 += 1\n",
    "        \n",
    "        if total_from_day0 > 0:\n",
    "            metrics['directional_accuracy_from_day0'] = float(correct_from_day0 / total_from_day0 * 100)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_predictions(predictions_df, prices_df, commodity, model_version, target_accuracy, n_horizons=14):\n",
    "    \"\"\"\n",
    "    Comprehensive validation with ALL details saved for review.\n",
    "    Returns complete validation data structure.\n",
    "    \"\"\"\n",
    "    print(f\"\\n  Validating predictions (forecast_agent-aligned metrics)...\")\n",
    "    \n",
    "    # Compute medians\n",
    "    medians = predictions_df.groupby(['timestamp', 'day_ahead'])['predicted_price'].median().reset_index()\n",
    "    medians.columns = ['timestamp', 'day_ahead', 'median_pred']\n",
    "    \n",
    "    prices_df = prices_df.copy()\n",
    "    prices_df['date'] = pd.to_datetime(prices_df['date'])\n",
    "    \n",
    "    # Merge with actuals\n",
    "    results = []\n",
    "    for _, row in medians.iterrows():\n",
    "        timestamp = row['timestamp']\n",
    "        day_ahead = int(row['day_ahead'])\n",
    "        median_pred = row['median_pred']\n",
    "        \n",
    "        future_date = timestamp + pd.Timedelta(days=day_ahead)\n",
    "        actual_row = prices_df[prices_df['date'] == future_date]\n",
    "        \n",
    "        if len(actual_row) > 0:\n",
    "            actual_price = actual_row['price'].values[0]\n",
    "            ape = abs(median_pred - actual_price) / actual_price\n",
    "            ae = abs(median_pred - actual_price)\n",
    "            results.append({\n",
    "                'timestamp': timestamp,\n",
    "                'day_ahead': day_ahead,\n",
    "                'median_pred': median_pred,\n",
    "                'actual': actual_price,\n",
    "                'ape': ape,\n",
    "                'ae': ae\n",
    "            })\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        print(f\"    ⚠️  No matching actuals found\")\n",
    "        return None\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    target_mape = 1.0 - target_accuracy\n",
    "    \n",
    "    # Overall metrics\n",
    "    overall_mae = results_df['ae'].mean()\n",
    "    overall_mape = results_df['ape'].mean()\n",
    "    median_ape = results_df['ape'].median()\n",
    "    pct90_ape = results_df['ape'].quantile(0.9)\n",
    "    \n",
    "    print(f\"\\n    Overall: MAE=${overall_mae:.2f}, MAPE={overall_mape:.1%} (target: {target_mape:.1%})\")\n",
    "    \n",
    "    # Per-horizon metrics\n",
    "    per_horizon = results_df.groupby('day_ahead').agg({\n",
    "        'ae': ['mean', 'std', 'min', 'max'],\n",
    "        'ape': ['mean', 'std', 'min', 'max'],\n",
    "        'timestamp': 'count'\n",
    "    })\n",
    "    per_horizon.columns = ['mae_mean', 'mae_std', 'mae_min', 'mae_max', \n",
    "                          'mape_mean', 'mape_std', 'mape_min', 'mape_max', 'n_samples']\n",
    "    \n",
    "    print(f\"\\n    Per-Horizon Summary:\")\n",
    "    for horizon in sorted(per_horizon.index):\n",
    "        h_mape = per_horizon.loc[horizon, 'mape_mean']\n",
    "        status = '✓' if h_mape <= target_mape * 1.15 else '⚠️'\n",
    "        print(f\"      Day {horizon:2d}: MAPE={h_mape:5.1%} {status}\")\n",
    "    \n",
    "    # Directional accuracy by timestamp\n",
    "    timestamps = results_df['timestamp'].unique()\n",
    "    directional_by_timestamp = []\n",
    "    \n",
    "    for ts in timestamps:\n",
    "        ts_data = results_df[results_df['timestamp'] == ts].sort_values('day_ahead')\n",
    "        if len(ts_data) >= 2:\n",
    "            actuals_series = pd.Series(ts_data['actual'].values)\n",
    "            forecasts_series = pd.Series(ts_data['median_pred'].values)\n",
    "            \n",
    "            dir_metrics = calculate_directional_accuracy(actuals_series, forecasts_series)\n",
    "            dir_metrics['timestamp'] = ts\n",
    "            directional_by_timestamp.append(dir_metrics)\n",
    "    \n",
    "    directional_df = pd.DataFrame(directional_by_timestamp)\n",
    "    \n",
    "    if len(directional_df) > 0:\n",
    "        avg_dir = directional_df['directional_accuracy'].mean()\n",
    "        avg_dir_day0 = directional_df['directional_accuracy_from_day0'].mean()\n",
    "        print(f\"\\n    Directional: {avg_dir:.1f}% (day-to-day), {avg_dir_day0:.1f}% (from day 0)\")\n",
    "    \n",
    "    # CRPS by timestamp (sample)\n",
    "    crps_by_timestamp = []\n",
    "    sample_timestamps = np.random.choice(timestamps, size=min(50, len(timestamps)), replace=False)\n",
    "    \n",
    "    for ts in sample_timestamps:\n",
    "        ts_predictions = predictions_df[predictions_df['timestamp'] == ts]\n",
    "        forecast_matrix = ts_predictions.pivot_table(\n",
    "            index='run_id', columns='day_ahead', values='predicted_price'\n",
    "        ).values\n",
    "        \n",
    "        ts_actuals = results_df[results_df['timestamp'] == ts].sort_values('day_ahead')['actual'].values\n",
    "        \n",
    "        if len(ts_actuals) == forecast_matrix.shape[1]:\n",
    "            crps_values = calculate_crps(ts_actuals, forecast_matrix)\n",
    "            if crps_values:\n",
    "                crps_by_timestamp.append({\n",
    "                    'timestamp': ts,\n",
    "                    'crps_mean': np.mean(crps_values),\n",
    "                    'crps_values': crps_values\n",
    "                })\n",
    "    \n",
    "    crps_df = pd.DataFrame(crps_by_timestamp)\n",
    "    if len(crps_df) > 0:\n",
    "        print(f\"    CRPS: ${crps_df['crps_mean'].mean():.2f}\")\n",
    "    \n",
    "    # Coverage\n",
    "    intervals = predictions_df.groupby(['timestamp', 'day_ahead'])['predicted_price'].agg(\n",
    "        p10=lambda x: x.quantile(0.1),\n",
    "        p90=lambda x: x.quantile(0.9),\n",
    "        p05=lambda x: x.quantile(0.05),\n",
    "        p95=lambda x: x.quantile(0.95)\n",
    "    ).reset_index()\n",
    "    \n",
    "    validation = results_df.merge(intervals, on=['timestamp', 'day_ahead'])\n",
    "    coverage_80 = ((validation['actual'] >= validation['p10']) & \n",
    "                   (validation['actual'] <= validation['p90'])).mean()\n",
    "    coverage_90 = ((validation['actual'] >= validation['p05']) & \n",
    "                   (validation['actual'] <= validation['p95'])).mean()\n",
    "    \n",
    "    print(f\"    Coverage: 80%={coverage_80:.1%}, 90%={coverage_90:.1%}\")\n",
    "    print(f\"  ✓ Validation complete\")\n",
    "    \n",
    "    # Return COMPLETE validation data\n",
    "    return {\n",
    "        'commodity': commodity,\n",
    "        'model_version': model_version,\n",
    "        'target_accuracy': target_accuracy,\n",
    "        'target_mape': target_mape,\n",
    "        'generation_timestamp': datetime.now(),\n",
    "        \n",
    "        # Overall metrics\n",
    "        'overall_mae': float(overall_mae),\n",
    "        'overall_mape': float(overall_mape),\n",
    "        'median_ape': float(median_ape),\n",
    "        'pct90_ape': float(pct90_ape),\n",
    "        \n",
    "        # Detailed data\n",
    "        'results_df': results_df,  # Full predictions vs actuals\n",
    "        'per_horizon_metrics': per_horizon,  # Per-horizon statistics\n",
    "        'directional_by_timestamp': directional_df,  # Directional accuracy per forecast\n",
    "        'crps_by_timestamp': crps_df,  # CRPS per forecast\n",
    "        'intervals_df': validation,  # Prediction intervals with coverage\n",
    "        \n",
    "        # Summary stats\n",
    "        'avg_directional_accuracy': float(directional_df['directional_accuracy'].mean()) if len(directional_df) > 0 else None,\n",
    "        'avg_directional_accuracy_day0': float(directional_df['directional_accuracy_from_day0'].mean()) if len(directional_df) > 0 else None,\n",
    "        'avg_crps': float(crps_df['crps_mean'].mean()) if len(crps_df) > 0 else None,\n",
    "        'coverage_80': float(coverage_80),\n",
    "        'coverage_90': float(coverage_90),\n",
    "        'n_samples': len(results_df)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Commodities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_commodity(commodity_name, prices_raw_pd, analysis_config, output_schema, \n",
    "                            accuracy_levels, synthetic_start_date):\n",
    "    \"\"\"\n",
    "    Process single commodity and return validation data.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PROCESSING: {commodity_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Prepare prices\n",
    "    print(f\"\\nPreparing price data...\")\n",
    "    prices_full = prices_raw_pd[prices_raw_pd['commodity'].str.lower() == commodity_name.lower()].copy()\n",
    "    prices_full['date'] = pd.to_datetime(prices_full['date'])\n",
    "    prices_full['price'] = prices_full['close']\n",
    "    prices_full = prices_full[['date', 'price']].sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"✓ Full history: {len(prices_full)} days\")\n",
    "    \n",
    "    prices = prices_full[prices_full['date'] >= synthetic_start_date].copy().reset_index(drop=True)\n",
    "    print(f\"✓ Filtered to {len(prices)} days\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    all_predictions = []\n",
    "    validation_data = []\n",
    "    \n",
    "    for accuracy in accuracy_levels:\n",
    "        model_version = f\"synthetic_acc{int(accuracy*100)}\"\n",
    "        print(f\"\\n  {model_version}: {accuracy:.0%} accurate\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        predictions_df = generate_calibrated_predictions(\n",
    "            prices, model_version, accuracy,\n",
    "            analysis_config['prediction_runs'],\n",
    "            analysis_config['forecast_horizon'], 20\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"    ✓ Generated {len(predictions_df):,} rows in {elapsed:.1f}s\")\n",
    "        \n",
    "        # Validate\n",
    "        val_data = validate_predictions(\n",
    "            predictions_df, prices, commodity_name, model_version, \n",
    "            accuracy, analysis_config['forecast_horizon']\n",
    "        )\n",
    "        \n",
    "        if val_data:\n",
    "            validation_data.append(val_data)\n",
    "        \n",
    "        all_predictions.append(predictions_df)\n",
    "        del predictions_df\n",
    "        gc.collect()\n",
    "    \n",
    "    # Combine and save\n",
    "    print(f\"\\nCombining all accuracy levels...\")\n",
    "    combined = pd.concat(all_predictions, ignore_index=True)\n",
    "    print(f\"✓ Combined: {len(combined):,} rows\")\n",
    "    \n",
    "    del all_predictions\n",
    "    gc.collect()\n",
    "    \n",
    "    # Save to Delta\n",
    "    predictions_table = f\"{output_schema}.predictions_{commodity_name.lower()}\"\n",
    "    print(f\"\\nSaving to {predictions_table}...\")\n",
    "    spark.createDataFrame(combined).write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(predictions_table)\n",
    "    print(f\"✓ Saved {spark.table(predictions_table).count():,} rows\")\n",
    "    \n",
    "    del combined\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\n✓ {commodity_name.upper()} COMPLETE\")\n",
    "    \n",
    "    return {\n",
    "        'commodity': commodity_name,\n",
    "        'table': predictions_table,\n",
    "        'validation_data': validation_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all commodities\n",
    "all_results = []\n",
    "all_validation_data = {}\n",
    "\n",
    "for commodity_name in COMMODITY_CONFIGS.keys():\n",
    "    try:\n",
    "        result = process_single_commodity(\n",
    "            commodity_name, market_df, ANALYSIS_CONFIG, OUTPUT_SCHEMA,\n",
    "            ACCURACY_LEVELS, SYNTHETIC_START_DATE\n",
    "        )\n",
    "        \n",
    "        all_results.append({\n",
    "            'commodity': result['commodity'],\n",
    "            'table': result['table']\n",
    "        })\n",
    "        \n",
    "        # Store validation data by commodity\n",
    "        all_validation_data[commodity_name] = result['validation_data']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error processing {commodity_name.upper()}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Complete Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ALL validation details to pickle\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SAVING VALIDATION DATA\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "validation_output = {\n",
    "    'generation_timestamp': datetime.now(),\n",
    "    'config': {\n",
    "        'synthetic_start_date': SYNTHETIC_START_DATE,\n",
    "        'accuracy_levels': ACCURACY_LEVELS,\n",
    "        'prediction_runs': ANALYSIS_CONFIG['prediction_runs'],\n",
    "        'forecast_horizon': ANALYSIS_CONFIG['forecast_horizon']\n",
    "    },\n",
    "    'commodities': all_validation_data,\n",
    "    'summary': all_results\n",
    "}\n",
    "\n",
    "with open(VALIDATION_OUTPUT_FILE, 'wb') as f:\n",
    "    pickle.dump(validation_output, f)\n",
    "\n",
    "print(f\"\\n✓ Saved complete validation data to: {VALIDATION_OUTPUT_FILE}\")\n",
    "print(f\"  File size: {os.path.getsize(VALIDATION_OUTPUT_FILE) / (1024*1024):.1f} MB\")\n",
    "print(f\"\\n  Contains:\")\n",
    "print(f\"    - Full predictions vs actuals DataFrames\")\n",
    "print(f\"    - Per-horizon metrics (mean, std, min, max)\")\n",
    "print(f\"    - Directional accuracy by timestamp\")\n",
    "print(f\"    - CRPS values by timestamp\")\n",
    "print(f\"    - Prediction intervals with coverage\")\n",
    "print(f\"    - All summary statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    print(f\"\\n✓ Processed {len(all_results)} commodities\")\n",
    "    print(f\"\\n✓ Prediction tables:\")\n",
    "    for r in all_results:\n",
    "        print(f\"  - {r['table']}\")\n",
    "    \n",
    "    print(f\"\\n✓ Validation data saved to: {VALIDATION_OUTPUT_FILE}\")\n",
    "    print(f\"  Ready for detailed review and debugging\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No commodities processed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
