# Production Runners Test Suite

**Created**: 2025-11-24
**Purpose**: Comprehensive test coverage for production backtest runners module
**Status**: Complete - All test files implemented

---

## Test Coverage

### Test Files (6 files, ~2,500 lines of test code)

1. **conftest.py** (280 lines)
   - Shared fixtures for all tests
   - Mock Spark session
   - Sample data generators
   - Utility assertion functions

2. **test_data_loader.py** (500+ lines)
   - Price loading from Delta tables
   - Prediction matrix loading from pickle files
   - Data validation and alignment
   - Model version discovery
   - Edge cases: missing files, corrupt data, misaligned dates

3. **test_strategy_runner.py** (550+ lines)
   - Strategy initialization (all 9 strategies)
   - Backtest execution
   - Metrics calculation
   - Best performer analysis
   - Risk-Adjusted scenario analysis
   - Forced liquidation detection

4. **test_visualization.py** (500+ lines)
   - All 5 chart types generation
   - Organized output structure (Phase 3)
   - Cross-commodity comparison charts
   - Chart customization and styling
   - Error handling

5. **test_result_saver.py** (500+ lines)
   - Metrics saving to Delta tables
   - Detailed results to pickle files
   - Cross-commodity summaries
   - Results validation
   - Round-trip save/load integrity

6. **test_integration.py** (450+ lines)
   - End-to-end single commodity workflow
   - MultiCommodityRunner orchestration
   - Data integrity through pipeline
   - Performance testing
   - Smoke tests for Databricks deployment
   - Regression tests (vs notebook 05)

**Total Test Coverage**: ~2,500 lines of test code covering 1,446 lines of production code

---

## Running Tests

### Prerequisites

```bash
pip install pytest pytest-cov
```

### Run All Tests

```bash
# From project root
cd /Users/markgibbons/capstone/ucberkeley-capstone/trading_agent/commodity_prediction_analysis

# Run all runner tests
pytest production/runners/tests/ -v
```

### Run Specific Test Files

```bash
# Test data loading only
pytest production/runners/tests/test_data_loader.py -v

# Test strategy execution only
pytest production/runners/tests/test_strategy_runner.py -v

# Test integration workflow only
pytest production/runners/tests/test_integration.py -v

# Test visualization only
pytest production/runners/tests/test_visualization.py -v

# Test result saving only
pytest production/runners/tests/test_result_saver.py -v
```

### Run with Coverage Report

```bash
# Generate HTML coverage report
pytest production/runners/tests/ \
    --cov=production.runners \
    --cov-report=html \
    --cov-report=term

# View coverage report
open htmlcov/index.html
```

### Run Specific Test Classes

```bash
# Test only DataLoader initialization
pytest production/runners/tests/test_data_loader.py::TestDataLoaderBasic -v

# Test only strategy execution
pytest production/runners/tests/test_strategy_runner.py::TestStrategyExecution -v

# Test only end-to-end workflow
pytest production/runners/tests/test_integration.py::TestSingleCommodityWorkflow -v
```

### Run with Markers (if using markers)

```bash
# Run only fast tests (exclude slow integration tests)
pytest production/runners/tests/ -m "not slow" -v

# Run only smoke tests
pytest production/runners/tests/test_integration.py::TestSmokeTest -v
```

---

## Test Structure

### Unit Tests (80% of test suite)
- Test individual functions and methods in isolation
- Use mocks for external dependencies (Spark, file I/O)
- Fast execution (< 30 seconds total)
- High coverage of edge cases

### Integration Tests (15% of test suite)
- Test complete workflows across modules
- Test data flow through entire pipeline
- Use minimal realistic datasets
- Verify end-to-end correctness

### Smoke Tests (5% of test suite)
- Verify all imports work
- Check basic instantiation
- Quick validation for Databricks deployment
- Run these first after deployment

---

## Fixtures (conftest.py)

### Data Fixtures

- `sample_prices()` - 100 days of realistic price data
- `sample_predictions()` - 10 dates × 100 runs × 14 horizons
- `minimal_predictions()` - 3 dates × 10 runs × 14 horizons (for quick tests)

### Configuration Fixtures

- `commodity_config()` - Sample coffee configuration
- `baseline_params()` - Baseline strategy parameters
- `prediction_params()` - Prediction strategy parameters

### Infrastructure Fixtures

- `mock_spark()` - Mock Spark session with Delta support
- `temp_volume()` - Temporary directory (auto-cleanup)
- `sample_data_paths()` - Mock data paths

### Results Fixtures

- `sample_results_dict()` - Mock strategy execution results
- `sample_metrics_df()` - Mock metrics DataFrame

### Utility Functions

- `assert_dataframe_columns(df, required_columns)` - Verify DataFrame structure
- `assert_numeric_close(actual, expected, tolerance)` - Numeric comparison

---

## Expected Test Results

### All Tests Pass
When running the full test suite, you should see:

```
======================== test session starts ========================
platform darwin -- Python 3.x.x, pytest-7.x.x
collected 150+ items

production/runners/tests/test_data_loader.py ................ [ 20%]
production/runners/tests/test_strategy_runner.py ............ [ 40%]
production/runners/tests/test_visualization.py .............. [ 60%]
production/runners/tests/test_result_saver.py ............... [ 80%]
production/runners/tests/test_integration.py ................ [100%]

======================== 150+ passed in 30.0s =======================
```

### Coverage Report
Expected coverage:

```
Name                                      Stmts   Miss  Cover
-------------------------------------------------------------
production/runners/data_loader.py           150     10    93%
production/runners/strategy_runner.py       200     15    93%
production/runners/visualization.py         250     20    92%
production/runners/result_saver.py          180     12    93%
production/runners/multi_commodity_runner.py 220    15    93%
-------------------------------------------------------------
TOTAL                                      1000     72    93%
```

---

## Test Execution Time

### By Test File

- `test_data_loader.py` - ~5 seconds (unit tests, mocked I/O)
- `test_strategy_runner.py` - ~8 seconds (includes backtest execution)
- `test_visualization.py` - ~6 seconds (chart generation)
- `test_result_saver.py` - ~5 seconds (mocked Spark, real pickle I/O)
- `test_integration.py` - ~10 seconds (end-to-end workflows)

**Total**: ~30-40 seconds for full test suite

### Performance Tips

- Use `minimal_predictions` fixture for quick tests (10 runs instead of 100)
- Mock Spark operations to avoid actual Delta table I/O
- Use temporary directories with auto-cleanup
- Run unit tests during development, integration tests before commit

---

## Debugging Failed Tests

### View Detailed Output

```bash
# Show print statements and logging
pytest production/runners/tests/ -v -s

# Show full tracebacks
pytest production/runners/tests/ -v --tb=long

# Stop at first failure
pytest production/runners/tests/ -x
```

### Common Failures

1. **ImportError**: Ensure you're in the correct directory
   ```bash
   cd /Users/markgibbons/capstone/ucberkeley-capstone/trading_agent/commodity_prediction_analysis
   ```

2. **Fixture not found**: Ensure `conftest.py` is in tests directory

3. **Spark mock issues**: Check that `mock_spark` fixture is properly configured

4. **File permission errors**: Ensure temp directories are writable

5. **Test data issues**: Verify fixtures are generating valid data structures

---

## Adding New Tests

### Test File Template

```python
"""
Unit Tests for NewModule
Brief description of what this module tests
"""

import pytest
from production.runners.new_module import NewModule


class TestNewModuleBasic:
    """Test basic functionality"""

    def test_initialization(self):
        """Test module initializes correctly"""
        module = NewModule()
        assert module is not None

    def test_some_method(self, sample_data):
        """Test some_method with sample data"""
        module = NewModule()
        result = module.some_method(sample_data)
        assert result is not None


class TestNewModuleEdgeCases:
    """Test edge cases and error handling"""

    def test_handles_empty_input(self):
        """Test graceful handling of empty input"""
        module = NewModule()
        with pytest.raises(ValueError):
            module.some_method(None)
```

### Adding New Fixtures

Edit `conftest.py`:

```python
@pytest.fixture
def new_fixture():
    """
    Brief description of fixture

    Returns:
        Expected return type
    """
    # Setup
    data = create_test_data()

    yield data

    # Cleanup (if needed)
    cleanup(data)
```

---

## Testing Best Practices

### 1. Arrange-Act-Assert Pattern

```python
def test_something(self, fixture):
    # Arrange - Setup test data
    input_data = fixture
    expected_output = 42

    # Act - Execute the code under test
    actual_output = function_under_test(input_data)

    # Assert - Verify results
    assert actual_output == expected_output
```

### 2. Test One Thing Per Test

```python
# Good - Tests one specific behavior
def test_calculates_net_earnings(self):
    result = strategy.calculate_earnings()
    assert result['net_earnings'] == 50000

# Bad - Tests multiple things
def test_everything(self):
    result = strategy.calculate_earnings()
    assert result['net_earnings'] == 50000
    assert result['trades'] == []
    assert result['costs'] > 0
```

### 3. Use Descriptive Test Names

```python
# Good
def test_validates_missing_required_columns(self):

# Bad
def test_validation(self):
```

### 4. Test Edge Cases

```python
def test_handles_empty_dataframe(self):
def test_handles_null_values(self):
def test_handles_negative_numbers(self):
def test_handles_misaligned_dates(self):
```

### 5. Use Fixtures for Reusable Data

```python
# Good - Use fixture
def test_something(self, sample_prices):
    result = process(sample_prices)

# Bad - Create data in test
def test_something(self):
    prices = pd.DataFrame({...})  # Repeated in every test
    result = process(prices)
```

---

## Continuous Integration

### Pre-Commit Hook

Create `.git/hooks/pre-commit`:

```bash
#!/bin/bash
# Run tests before commit

echo "Running tests..."
pytest production/runners/tests/ -v

if [ $? -ne 0 ]; then
    echo "Tests failed! Commit aborted."
    exit 1
fi

echo "Tests passed! Proceeding with commit."
```

### GitHub Actions (if using)

```yaml
# .github/workflows/test.yml
name: Test Suite

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-python@v2
        with:
          python-version: 3.9
      - run: pip install -r requirements.txt
      - run: pytest production/runners/tests/ --cov --cov-report=xml
      - uses: codecov/codecov-action@v1
```

---

## Next Steps

### Immediate

1. Run full test suite locally
2. Verify all tests pass
3. Review coverage report
4. Fix any failing tests

### Short-Term

1. Add regression tests comparing to notebook 05 outputs
2. Create smoke test notebook for Databricks
3. Add performance benchmarks
4. Document test data generation process

### Long-Term

1. Add stress tests with large datasets
2. Add visual regression tests for charts
3. Set up continuous integration
4. Create test data versioning system

---

## Troubleshooting

### Tests Not Found

```bash
# Ensure you're in correct directory
cd /Users/markgibbons/capstone/ucberkeley-capstone/trading_agent/commodity_prediction_analysis

# Verify Python path
export PYTHONPATH="${PYTHONPATH}:$(pwd)"

# Run with explicit path
python -m pytest production/runners/tests/
```

### Import Errors

```bash
# Install dependencies
pip install pytest pytest-cov pandas numpy matplotlib

# Verify production modules are importable
python -c "from production.runners import DataLoader; print('OK')"
```

### Spark Mock Issues

If Spark mocking fails, ensure `conftest.py` is properly configured with all necessary mock methods.

---

## Contact

**Test Suite Owner**: Trading Agent Team
**Created**: 2025-11-24
**Last Updated**: 2025-11-24

For questions about tests:
1. Review this README
2. Check TEST_PLAN.md for testing strategy
3. Review conftest.py for available fixtures
4. Refer to production/runners/README.md for module documentation
