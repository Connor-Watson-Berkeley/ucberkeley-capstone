{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "047bf8dd-7b96-436c-af51-0fcea55964f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NOTEBOOK 00: SETUP AND CONFIGURATION (UPDATED)\n",
    "# ============================================================================\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Configuration and Setup\n",
    "# MAGIC Define all parameters for both commodities\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Commodity-Specific Configurations\n",
    "# Each commodity has its own harvest schedule, costs, and constraints\n",
    "COMMODITY_CONFIGS = {\n",
    "    'coffee': {\n",
    "        'commodity': 'coffee',\n",
    "        'harvest_volume': 50,  # tons per year\n",
    "        'harvest_windows': [(5, 9)],  # May-September (list of tuples: (start_month, end_month))\n",
    "        'storage_cost_pct_per_day': 0.025,\n",
    "        'transaction_cost_pct': 0.25,\n",
    "        'min_inventory_to_trade': 1.0,\n",
    "        'max_holding_days': 365  # 12 months from harvest start\n",
    "    },\n",
    "    'sugar': {\n",
    "        'commodity': 'sugar',\n",
    "        'harvest_volume': 50,  # tons per year (same as coffee for MVP)\n",
    "        'harvest_windows': [(10, 12)],  # October-December (simplified assumption)\n",
    "        'storage_cost_pct_per_day': 0.025,  # Same as coffee for MVP\n",
    "        'transaction_cost_pct': 0.25,  # Same as coffee for MVP\n",
    "        'min_inventory_to_trade': 1.0,\n",
    "        'max_holding_days': 365  # Same constraint for MVP\n",
    "    }\n",
    "}\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Strategy Parameters (shared across commodities)\n",
    "BASELINE_PARAMS = {\n",
    "    'equal_batch': {\n",
    "        'batch_size': 0.25,\n",
    "        'frequency_days': 30\n",
    "    },\n",
    "    'price_threshold': {\n",
    "        'threshold_pct': 0.05\n",
    "        # batch_fraction added directly in strategy init: 0.33\n",
    "    },\n",
    "    'moving_average': {\n",
    "        'ma_period': 30\n",
    "        # batch_fraction added directly in strategy init: 0.50\n",
    "    }\n",
    "}\n",
    "\n",
    "PREDICTION_PARAMS = {\n",
    "    'consensus': {\n",
    "        'consensus_threshold': 0.70,\n",
    "        'min_return': 0.03,\n",
    "        'evaluation_day': 14\n",
    "    },\n",
    "    'expected_value': {\n",
    "        'min_ev_improvement': 50,  # Minimum $ improvement to defer sale\n",
    "        'baseline_batch': 0.15,    # Baseline batch size (15%)\n",
    "        'baseline_frequency': 10   # Days between scheduled sales\n",
    "    },\n",
    "    'risk_adjusted': {\n",
    "        'min_return': 0.03,  # Lower from 5% to 3% (realistic for commodity markets)\n",
    "        'max_uncertainty': 0.35, # Increase from 8% to 35% (matches your prediction range)\n",
    "        'consensus_threshold': 0.60, # Lower from 65% to 60% (more lenient)\n",
    "        'evaluation_day': 14\n",
    "    }\n",
    "}\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Data Configuration - Unity Catalog\n",
    "# Source table (contains both forecasts and actuals)\n",
    "FORECAST_TABLE = \"commodity.forecast.distributions\"\n",
    "\n",
    "# Output schema for Delta tables\n",
    "OUTPUT_SCHEMA = \"commodity.trading_agent\"\n",
    "\n",
    "# Volume path for binary files (pickle, png)\n",
    "VOLUME_PATH = \"/Volumes/commodity/trading_agent/files\"\n",
    "\n",
    "def get_model_versions(commodity_name):\n",
    "    \"\"\"Get all model versions available for a commodity\"\"\"\n",
    "    df = spark.table(FORECAST_TABLE) \\\n",
    "        .filter(f\"commodity = '{commodity_name.title()}' AND is_actuals = false\") \\\n",
    "        .select(\"model_version\") \\\n",
    "        .distinct() \\\n",
    "        .orderBy(\"model_version\")\n",
    "    \n",
    "    versions = [row.model_version for row in df.collect()]\n",
    "    return versions\n",
    "\n",
    "def load_forecast_data(commodity_name, model_version, spark):\n",
    "    \"\"\"Load forecast data from commodity.forecast.distributions table\"\"\"\n",
    "    df = spark.table(FORECAST_TABLE) \\\n",
    "        .filter(f\"commodity = '{commodity_name.title()}' AND is_actuals = false AND model_version = '{model_version}'\")\n",
    "    \n",
    "    return df.toPandas()\n",
    "\n",
    "def load_actual_prices(commodity_name, spark):\n",
    "    \"\"\"Load actual price data from commodity.forecast.distributions table\"\"\"\n",
    "    df = spark.table(FORECAST_TABLE) \\\n",
    "        .filter(f\"commodity = '{commodity_name.title()}' AND is_actuals = true\")\n",
    "    \n",
    "    return df.toPandas()\n",
    "\n",
    "def check_real_prediction_exists(commodity_name, model_version=None):\n",
    "    \"\"\"Check if real prediction data exists in table\"\"\"\n",
    "    try:\n",
    "        if model_version:\n",
    "            count = spark.table(FORECAST_TABLE) \\\n",
    "                .filter(f\"commodity = '{commodity_name.title()}' AND is_actuals = false AND model_version = '{model_version}'\") \\\n",
    "                .count()\n",
    "        else:\n",
    "            count = spark.table(FORECAST_TABLE) \\\n",
    "                .filter(f\"commodity = '{commodity_name.title()}' AND is_actuals = false\") \\\n",
    "                .count()\n",
    "        return count > 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking predictions: {e}\")\n",
    "        return False\n",
    "\n",
    "def get_data_paths(commodity_name, model_version=None):\n",
    "    \"\"\"Generate data paths for a specific commodity and model version\"\"\"\n",
    "    # Clean model version for use in file/table names\n",
    "    model_suffix = f\"_{model_version.replace('.', '_')}\" if model_version else \"\"\n",
    "    \n",
    "    return {\n",
    "        # Delta tables in Unity Catalog\n",
    "        'historical_prices': f'{OUTPUT_SCHEMA}.historical_prices_{commodity_name.lower()}',\n",
    "        'predictions': f'{OUTPUT_SCHEMA}.predictions_{commodity_name.lower()}{model_suffix}',\n",
    "        'prices_prepared': f'{OUTPUT_SCHEMA}.prices_prepared_{commodity_name.lower()}',\n",
    "        'predictions_prepared': f'{OUTPUT_SCHEMA}.predictions_prepared_{commodity_name.lower()}{model_suffix}',\n",
    "        'results': f'{OUTPUT_SCHEMA}.results_{commodity_name.lower()}{model_suffix}',\n",
    "        \n",
    "        # Binary files in volume\n",
    "        'prediction_matrices': f'{VOLUME_PATH}/prediction_matrices_{commodity_name.lower()}{model_suffix}.pkl',\n",
    "        'prediction_matrices_real': f'{VOLUME_PATH}/prediction_matrices_{commodity_name.lower()}{model_suffix}_real.pkl',\n",
    "        'results_detailed': f'{VOLUME_PATH}/results_detailed_{commodity_name.lower()}{model_suffix}.pkl',\n",
    "        'statistical_results': f'{VOLUME_PATH}/statistical_results_{commodity_name.lower()}{model_suffix}.pkl',\n",
    "        'feature_analysis': f'{VOLUME_PATH}/feature_analysis_{commodity_name.lower()}{model_suffix}.pkl',\n",
    "        'sensitivity_results': f'{VOLUME_PATH}/sensitivity_results_{commodity_name.lower()}{model_suffix}.pkl',\n",
    "        \n",
    "        # Images in volume\n",
    "        'cumulative_returns': f'{VOLUME_PATH}/cumulative_returns_{commodity_name.lower()}{model_suffix}.png',\n",
    "        'final_dashboard': f'{VOLUME_PATH}/final_dashboard_{commodity_name.lower()}{model_suffix}.png',\n",
    "        \n",
    "        # CSV exports in volume\n",
    "        'final_summary': f'{VOLUME_PATH}/final_summary_{commodity_name.lower()}{model_suffix}.csv',\n",
    "        'statistical_comparisons': f'{VOLUME_PATH}/statistical_comparisons_{commodity_name.lower()}{model_suffix}.csv',\n",
    "        'bootstrap_summary': f'{VOLUME_PATH}/bootstrap_summary_{commodity_name.lower()}{model_suffix}.csv',\n",
    "        'summary_stats': f'{VOLUME_PATH}/summary_stats_{commodity_name.lower()}{model_suffix}.csv'\n",
    "    }\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Analysis Configuration\n",
    "ANALYSIS_CONFIG = {\n",
    "    'backtest_start_date': '2018-01-01',  # Match prediction start\n",
    "    'backtest_end_date': '2025-09-24',    # Match prediction end\n",
    "    'bootstrap_iterations': 1000,\n",
    "    'confidence_level': 0.95,\n",
    "    'random_seed': 42,\n",
    "    'prediction_runs': 500,  # For synthetic generation\n",
    "    'forecast_horizon': 14    # Days ahead\n",
    "}\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Harvest Schedule Calculation Functions\n",
    "\n",
    "def calculate_weeks_in_window(start_month, end_month):\n",
    "    \"\"\"\n",
    "    Calculate approximate number of weeks in a harvest window.\n",
    "    \n",
    "    Args:\n",
    "        start_month: 1-12 (January = 1)\n",
    "        end_month: 1-12 (December = 12)\n",
    "    \n",
    "    Returns:\n",
    "        int: Approximate number of weeks\n",
    "    \"\"\"\n",
    "    if end_month >= start_month:\n",
    "        months = end_month - start_month + 1\n",
    "    else:\n",
    "        months = (12 - start_month + 1) + end_month\n",
    "    \n",
    "    return int(months * 4.33)\n",
    "\n",
    "def get_harvest_schedule(commodity_name):\n",
    "    \"\"\"\n",
    "    Get harvest schedule for a commodity.\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'windows' (list of tuples) and 'total_weeks'\n",
    "    \"\"\"\n",
    "    config = COMMODITY_CONFIGS[commodity_name]\n",
    "    windows = config['harvest_windows']\n",
    "    \n",
    "    total_weeks = sum(\n",
    "        calculate_weeks_in_window(start, end) \n",
    "        for start, end in windows\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'windows': windows,\n",
    "        'total_weeks': total_weeks\n",
    "    }\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Display Configuration\n",
    "print(\"=\" * 80)\n",
    "print(\"COMMODITY CONFIGURATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Typical market prices for cost calculations\n",
    "typical_prices = {\n",
    "    'coffee': 150,  # $/ton\n",
    "    'sugar': 400    # $/ton\n",
    "}\n",
    "\n",
    "for commodity_name, config in COMMODITY_CONFIGS.items():\n",
    "    print(f\"\\n{commodity_name.upper()}:\")\n",
    "    print(f\"  Harvest volume: {config['harvest_volume']} tons/year\")\n",
    "    \n",
    "    schedule = get_harvest_schedule(commodity_name)\n",
    "    print(f\"  Harvest windows: {schedule['windows']}\")\n",
    "    print(f\"  Total harvest weeks: {schedule['total_weeks']}\")\n",
    "    print(f\"  Weekly harvest rate: {config['harvest_volume'] / schedule['total_weeks']:.2f} tons/week\")\n",
    "    \n",
    "    print(f\"\\n  Costs (percentage-based):\")\n",
    "    print(f\"    Storage: {config['storage_cost_pct_per_day']}% of value per day\")\n",
    "    print(f\"    Transaction: {config['transaction_cost_pct']}% of sale value\")\n",
    "    print(f\"    Max holding: {config['max_holding_days']} days from harvest start\")\n",
    "    \n",
    "    if commodity_name in typical_prices:\n",
    "        typical_price = typical_prices[commodity_name]\n",
    "        storage_per_day = config['harvest_volume'] * typical_price * (config['storage_cost_pct_per_day'] / 100)\n",
    "        transaction_full = config['harvest_volume'] * typical_price * (config['transaction_cost_pct'] / 100)\n",
    "        print(f\"\\n  Example at ${typical_price}/ton:\")\n",
    "        print(f\"    Transaction cost (full harvest): ${transaction_full:,.2f}\")\n",
    "        print(f\"    Storage per day (full harvest): ${storage_per_day:.2f}\")\n",
    "        print(f\"    Storage per month (full harvest): ${storage_per_day * 30:,.2f}\")\n",
    "        print(f\"    Storage for 6 months: ${storage_per_day * 180:,.2f}\")\n",
    "    \n",
    "    # Check for real prediction data and list model versions\n",
    "    print(f\"\\nReal Prediction Data:\")\n",
    "    has_real = check_real_prediction_exists(commodity_name)\n",
    "    if has_real:\n",
    "        model_versions = get_model_versions(commodity_name)\n",
    "        print(f\"  ✓ Real prediction data found in table: {FORECAST_TABLE}\")\n",
    "        print(f\"  Model versions available: {len(model_versions)}\")\n",
    "        for mv in model_versions:\n",
    "            print(f\"    - {mv}\")\n",
    "    else:\n",
    "        print(f\"  ⓘ No real prediction data found (will use synthetic)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STRATEGY PARAMETERS (SHARED)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nBaseline Strategy Parameters:\")\n",
    "print(json.dumps(BASELINE_PARAMS, indent=2))\n",
    "print(\"\\nPrediction Strategy Parameters:\")\n",
    "print(json.dumps(PREDICTION_PARAMS, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "print(json.dumps(ANALYSIS_CONFIG, indent=2))\n",
    "print(f\"\\nForecast table: {FORECAST_TABLE}\")\n",
    "print(f\"Output schema: {OUTPUT_SCHEMA}\")\n",
    "print(f\"Volume path: {VOLUME_PATH}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMMODITIES TO ANALYZE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Will run analysis for: {list(COMMODITY_CONFIGS.keys())}\")\n",
    "\n",
    "print(\"\\n✓ Configuration complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3d46c972-efb4-47d5-9d83-0f48e8f05ca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NOTEBOOK 00A: GENERATE SYNTHETIC PREDICTIONS (MEMORY-EFFICIENT) - FIXED\n",
    "# ============================================================================\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Generate Synthetic Predictions - All Commodities (Memory-Efficient)\n",
    "# MAGIC \n",
    "# MAGIC Generates synthetic predictions using chunk-based processing for Unity Catalog tables.\n",
    "# MAGIC Creates multiple versions with different accuracy levels.\n",
    "# MAGIC \n",
    "# MAGIC **FIXED**: Now also saves prepared prices for use in Block 01A\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./00_setup_and_config\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "from builtins import min as builtin_min, max as builtin_max\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Configuration for synthetic predictions\n",
    "SYNTHETIC_START_DATE = '2022-01-01'  # Only generate from 2022 onward\n",
    "ACCURACY_LEVELS = [0.60, 0.70, 0.80, 0.90]  # Test multiple accuracy levels\n",
    "\n",
    "print(f\"Synthetic prediction configuration:\")\n",
    "print(f\"  Start date: {SYNTHETIC_START_DATE}\")\n",
    "print(f\"  Accuracy levels: {ACCURACY_LEVELS}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Generate Predictions for All Commodities\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Load price data from bronze market table\n",
    "MARKET_TABLE = \"commodity.bronze.market\"\n",
    "print(f\"\\nLoading price data from {MARKET_TABLE}...\")\n",
    "\n",
    "# Get all market data\n",
    "market_df = spark.table(MARKET_TABLE).toPandas()\n",
    "\n",
    "# Filter to 2023 onward\n",
    "market_df['date'] = pd.to_datetime(market_df['date'])\n",
    "market_df = market_df[market_df['date'] >= SYNTHETIC_START_DATE].copy()\n",
    "\n",
    "print(f\"✓ Loaded market price data (filtered to {SYNTHETIC_START_DATE}+)\")\n",
    "commodity_counts = market_df.groupby('commodity').size()\n",
    "print(f\"Available commodities:\")\n",
    "for commodity, count in commodity_counts.items():\n",
    "    print(f\"  - {commodity}: {count} rows\")\n",
    "print(f\"\\nDate range: {market_df['date'].min()} to {market_df['date'].max()}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def generate_predictions_for_accuracy(prices_df, model_version, n_runs=2000, n_horizons=14,\n",
    "                                     base_accuracy=0.65, noise_level=0.10, chunk_size=20):\n",
    "    \"\"\"\n",
    "    Generate synthetic predictions for a single accuracy level.\n",
    "    Returns a DataFrame with a model_version column.\n",
    "    \n",
    "    Parameters:\n",
    "    - chunk_size: Number of dates to process per chunk (smaller = less memory per chunk)\n",
    "    \"\"\"\n",
    "    n_dates = len(prices_df) - n_horizons\n",
    "    \n",
    "    # Collect chunks\n",
    "    all_chunks = []\n",
    "    \n",
    "    # Process in small chunks to manage memory\n",
    "    for chunk_start in range(0, n_dates, chunk_size):\n",
    "        chunk_end = builtin_min(chunk_start + chunk_size, n_dates)\n",
    "        \n",
    "        # Build predictions for this chunk\n",
    "        chunk_records = []\n",
    "        \n",
    "        for i in range(chunk_start, chunk_end):\n",
    "            current_date = prices_df.loc[i, 'date']\n",
    "            current_price = prices_df.loc[i, 'price']\n",
    "            future_prices = prices_df.loc[i+1:i+n_horizons, 'price'].values\n",
    "            \n",
    "            # Generate predictions for this date (vectorized per date)\n",
    "            random_components = current_price * (1 + np.random.normal(0, noise_level, (n_runs, n_horizons)))\n",
    "            run_biases = np.random.normal(0, noise_level * 0.3, (n_runs, 1))\n",
    "            \n",
    "            future_prices_matrix = np.tile(future_prices, (n_runs, 1))\n",
    "            predicted_prices_matrix = (base_accuracy * future_prices_matrix + \n",
    "                                      (1 - base_accuracy) * random_components)\n",
    "            predicted_prices_matrix *= (1 + run_biases)\n",
    "            \n",
    "            # Append to chunk records with model_version\n",
    "            for run_id in range(1, n_runs + 1):\n",
    "                for day_ahead in range(1, n_horizons + 1):\n",
    "                    chunk_records.append({\n",
    "                        'timestamp': current_date,\n",
    "                        'run_id': run_id,\n",
    "                        'day_ahead': day_ahead,\n",
    "                        'predicted_price': predicted_prices_matrix[run_id-1, day_ahead-1],\n",
    "                        'model_version': model_version\n",
    "                    })\n",
    "        \n",
    "        # Convert chunk to DataFrame and store\n",
    "        chunk_df = pd.DataFrame(chunk_records)\n",
    "        all_chunks.append(chunk_df)\n",
    "        \n",
    "        # Clear memory\n",
    "        del chunk_records\n",
    "        gc.collect()\n",
    "        \n",
    "        # Progress update\n",
    "        if chunk_end % 100 == 0 or chunk_end == n_dates:\n",
    "            print(f\"    Progress: {chunk_end}/{n_dates} dates... ({len(all_chunks)} chunks collected)\")\n",
    "    \n",
    "    # Concatenate all chunks\n",
    "    final_df = pd.concat(all_chunks, ignore_index=True)\n",
    "    \n",
    "    # Clear memory\n",
    "    del all_chunks\n",
    "    gc.collect()\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def process_single_commodity(commodity_name, prices_raw_pd, analysis_config, output_schema, accuracy_levels):\n",
    "    \"\"\"\n",
    "    Process a single commodity with multiple accuracy levels.\n",
    "    All accuracy levels stored in one table with model_version column.\n",
    "    Also saves prepared prices table.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PROCESSING: {commodity_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Filter and prepare price data\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(f\"\\nPreparing price data...\")\n",
    "    \n",
    "    # Filter to commodity\n",
    "    prices = prices_raw_pd[prices_raw_pd['commodity'].str.lower() == commodity_name.lower()].copy()\n",
    "    \n",
    "    # Extract date and close price\n",
    "    prices['date'] = pd.to_datetime(prices['date'])\n",
    "    prices['price'] = prices['close']\n",
    "    prices = prices[['date', 'price']].sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"✓ Loaded {len(prices)} days of prices\")\n",
    "    print(f\"  Date range: {prices['date'].min()} to {prices['date'].max()}\")\n",
    "    print(f\"  Price range: ${prices['price'].min():.2f} to ${prices['price'].max():.2f}\")\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Save prepared prices table (for use in Block 01A)\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(f\"\\nSaving prepared prices...\")\n",
    "    \n",
    "    DATA_PATHS = get_data_paths(commodity_name)\n",
    "    prices_spark = spark.createDataFrame(prices)\n",
    "    prices_spark.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(DATA_PATHS['prices_prepared'])\n",
    "    print(f\"✓ Saved: {DATA_PATHS['prices_prepared']}\")\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Generate predictions for all accuracy levels\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(f\"\\nGenerating predictions for {len(accuracy_levels)} accuracy levels...\")\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    for accuracy in accuracy_levels:\n",
    "        model_version = f\"synthetic_acc{int(accuracy*100)}\"\n",
    "        \n",
    "        print(f\"\\n  Generating {model_version}...\")\n",
    "        print(f\"    Accuracy: {accuracy:.0%}\")\n",
    "        print(f\"    Runs per date: {analysis_config['prediction_runs']}\")\n",
    "        print(f\"    Horizon: {analysis_config['forecast_horizon']} days\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        predictions_df = generate_predictions_for_accuracy(\n",
    "            prices,\n",
    "            model_version=model_version,\n",
    "            n_runs=analysis_config['prediction_runs'],\n",
    "            n_horizons=analysis_config['forecast_horizon'],\n",
    "            base_accuracy=accuracy,\n",
    "            noise_level=0.10,\n",
    "            chunk_size=20\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"    ✓ Generated {len(predictions_df):,} prediction rows in {elapsed:.1f}s\")\n",
    "        \n",
    "        all_predictions.append(predictions_df)\n",
    "        \n",
    "        # Clear memory\n",
    "        del predictions_df\n",
    "        gc.collect()\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Combine all accuracy levels\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(f\"\\nCombining all accuracy levels...\")\n",
    "    \n",
    "    combined_predictions = pd.concat(all_predictions, ignore_index=True)\n",
    "    print(f\"✓ Combined: {len(combined_predictions):,} total rows\")\n",
    "    \n",
    "    # Clear memory\n",
    "    del all_predictions\n",
    "    gc.collect()\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Save to Delta table\n",
    "    # --------------------------------------------------------------------------\n",
    "    predictions_table = f\"{output_schema}.predictions_{commodity_name.lower()}\"\n",
    "    \n",
    "    print(f\"\\nSaving to Delta table: {predictions_table}\")\n",
    "    print(f\"  Total rows: {len(combined_predictions):,}\")\n",
    "    print(f\"  Model versions: {combined_predictions['model_version'].nunique()}\")\n",
    "    \n",
    "    # Convert to Spark and save\n",
    "    predictions_spark = spark.createDataFrame(combined_predictions)\n",
    "    predictions_spark.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(predictions_table)\n",
    "    \n",
    "    print(f\"✓ Saved successfully\")\n",
    "    \n",
    "    # Verify\n",
    "    saved_count = spark.table(predictions_table).count()\n",
    "    print(f\"✓ Verified: {saved_count:,} rows in table\")\n",
    "    \n",
    "    # Clear memory\n",
    "    del combined_predictions\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\n✓ {commodity_name.upper()} COMPLETE\")\n",
    "    \n",
    "    return {\n",
    "        'commodity': commodity_name,\n",
    "        'n_dates': len(prices),\n",
    "        'n_accuracy_levels': len(accuracy_levels),\n",
    "        'n_predictions_per_level': len(prices) - analysis_config['forecast_horizon'],\n",
    "        'total_predictions': len(accuracy_levels) * (len(prices) - analysis_config['forecast_horizon']) * analysis_config['prediction_runs'] * analysis_config['forecast_horizon'],\n",
    "        'table': predictions_table,\n",
    "        'prices_table': DATA_PATHS['prices_prepared']\n",
    "    }\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Process all commodities with all accuracy levels\n",
    "all_results = []\n",
    "\n",
    "for commodity_name in COMMODITY_CONFIGS.keys():\n",
    "    try:\n",
    "        result = process_single_commodity(\n",
    "            commodity_name,\n",
    "            market_df,\n",
    "            ANALYSIS_CONFIG,\n",
    "            OUTPUT_SCHEMA,\n",
    "            ACCURACY_LEVELS\n",
    "        )\n",
    "        all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error processing {commodity_name.upper()}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(f\"   Skipping...\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATION COMPLETE - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    summary_df = pd.DataFrame(all_results)\n",
    "    print(f\"\\nSuccessfully processed {len(all_results)} commodities:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nPrediction tables created:\")\n",
    "    for table in sorted(summary_df['table'].unique()):\n",
    "        print(f\"  - {table}\")\n",
    "        # Show model versions in each table\n",
    "        model_versions = spark.table(table).select(\"model_version\").distinct().collect()\n",
    "        for mv in model_versions:\n",
    "            print(f\"      • {mv.model_version}\")\n",
    "    \n",
    "    print(f\"\\nPrice tables created:\")\n",
    "    for table in sorted(summary_df['prices_table'].unique()):\n",
    "        row_count = spark.table(table).count()\n",
    "        print(f\"  - {table} ({row_count} days)\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No commodities were successfully processed\")\n",
    "\n",
    "print(\"\\n✓ Block 00A complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b64231b3-5b91-40f5-b310-f0303490de88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NOTEBOOK 01A: DATA PREPARATION (Synthetic Predictions) - FIXED\n",
    "# ============================================================================\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Data Preparation - All Commodities (Synthetic)\n",
    "# MAGIC \n",
    "# MAGIC Prepares data for all configured commodities and synthetic model versions.\n",
    "# MAGIC Uses memory-efficient Spark PIVOT to handle large synthetic prediction datasets.\n",
    "# MAGIC \n",
    "# MAGIC **FIXED**: Now loads prices from table saved by Block 00A (no reloading needed)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./00_setup_and_config\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from pyspark.sql.functions import to_date, col\n",
    "from builtins import min as builtin_min, max as builtin_max\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Process All Commodities and Model Versions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Loop through all commodities\n",
    "for CURRENT_COMMODITY in COMMODITY_CONFIGS.keys():\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"PROCESSING: {CURRENT_COMMODITY.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Get configuration for this commodity\n",
    "    CURRENT_CONFIG = COMMODITY_CONFIGS[CURRENT_COMMODITY]\n",
    "    DATA_PATHS = get_data_paths(CURRENT_COMMODITY)\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  Harvest windows: {CURRENT_CONFIG['harvest_windows']}\")\n",
    "    print(f\"  Annual volume: {CURRENT_CONFIG['harvest_volume']} tons\")\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Load Prepared Prices (saved by Block 00A)\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(f\"\\nLoading prepared prices from {DATA_PATHS['prices_prepared']}...\")\n",
    "    \n",
    "    prices = spark.table(DATA_PATHS['prices_prepared']).toPandas()\n",
    "    prices['date'] = pd.to_datetime(prices['date'])\n",
    "    \n",
    "    print(f\"✓ Loaded {len(prices)} days of {CURRENT_COMMODITY.upper()} price data\")\n",
    "    print(f\"  Date range: {prices['date'].min()} to {prices['date'].max()}\")\n",
    "    print(f\"  Price range: ${prices['price'].min():.2f} to ${prices['price'].max():.2f}\")\n",
    "    \n",
    "    # Validation\n",
    "    assert prices['date'].is_unique, \"Duplicate dates found\"\n",
    "    assert prices['price'].isnull().sum() == 0, \"Missing prices\"\n",
    "    assert (prices['price'] > 0).all(), \"Non-positive prices\"\n",
    "    print(\"✓ Price data validated\")\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Calculate Harvest Information\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(f\"\\nCalculating harvest schedule...\")\n",
    "    \n",
    "    harvest_schedule = get_harvest_schedule(CURRENT_COMMODITY)\n",
    "    harvest_weeks = harvest_schedule['total_weeks']\n",
    "    weekly_harvest = CURRENT_CONFIG['harvest_volume'] / harvest_weeks\n",
    "    \n",
    "    print(f\"✓ Harvest schedule:\")\n",
    "    print(f\"  Total weeks: {harvest_weeks}\")\n",
    "    print(f\"  Weekly harvest: {weekly_harvest:.2f} tons\")\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Discover synthetic model versions for this commodity\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(f\"\\nDiscovering synthetic model versions...\")\n",
    "    \n",
    "    try:\n",
    "        synthetic_df = spark.table(DATA_PATHS['predictions']).select(\"model_version\").distinct()\n",
    "        model_versions = [row.model_version for row in synthetic_df.collect()]\n",
    "        print(f\"✓ Found {len(model_versions)} synthetic models: {model_versions}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  No synthetic predictions found: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Process each synthetic model version\n",
    "    # --------------------------------------------------------------------------\n",
    "    for MODEL_VERSION in model_versions:\n",
    "        print(f\"\\n{'-' * 80}\")\n",
    "        print(f\"MODEL VERSION: {MODEL_VERSION}\")\n",
    "        print(f\"{'-' * 80}\")\n",
    "        \n",
    "        MODEL_DATA_PATHS = get_data_paths(CURRENT_COMMODITY, MODEL_VERSION)\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Load Synthetic Predictions for this model version\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nLoading synthetic predictions for {MODEL_VERSION}...\")\n",
    "        \n",
    "        predictions_table = DATA_PATHS['predictions']\n",
    "        \n",
    "        # Load predictions for this model version\n",
    "        predictions_spark = spark.table(predictions_table) \\\n",
    "            .filter(f\"model_version = '{MODEL_VERSION}'\")\n",
    "        \n",
    "        n_predictions = predictions_spark.count()\n",
    "        print(f\"✓ Loaded {n_predictions:,} prediction rows from: {predictions_table}\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Transform to Prediction Matrices using Spark PIVOT (memory-efficient)\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nTransforming to prediction matrices...\")\n",
    "        \n",
    "        # Pivot: timestamp × run_id → day_ahead columns\n",
    "        pivot_df = predictions_spark.groupBy(\"timestamp\", \"run_id\").pivot(\"day_ahead\").agg({\"predicted_price\": \"first\"})\n",
    "        \n",
    "        # Rename columns to day_1, day_2, etc.\n",
    "        day_cols = [str(i) for i in range(1, 15)]\n",
    "        new_cols = [\"timestamp\", \"run_id\"] + [f\"day_{i}\" for i in range(1, 15)]\n",
    "        \n",
    "        for old_col, new_col in zip(pivot_df.columns, new_cols):\n",
    "            pivot_df = pivot_df.withColumnRenamed(old_col, new_col)\n",
    "        \n",
    "        # Convert to Pandas\n",
    "        predictions_pivot_pd = pivot_df.toPandas()\n",
    "        predictions_pivot_pd['timestamp'] = pd.to_datetime(predictions_pivot_pd['timestamp'])\n",
    "        \n",
    "        print(f\"✓ Transformed to pivot format: {len(predictions_pivot_pd):,} rows\")\n",
    "        \n",
    "        # Save prepared predictions to Delta table\n",
    "        pivot_spark = spark.createDataFrame(predictions_pivot_pd)\n",
    "        pivot_spark.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(MODEL_DATA_PATHS['predictions_prepared'])\n",
    "        print(f\"✓ Saved: {MODEL_DATA_PATHS['predictions_prepared']}\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Convert to Prediction Matrices Dictionary\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nBuilding prediction matrices...\")\n",
    "        \n",
    "        prediction_matrices = {}\n",
    "        day_cols = [f'day_{i}' for i in range(1, 15)]\n",
    "        \n",
    "        for timestamp, group in predictions_pivot_pd.groupby('timestamp'):\n",
    "            # Each row is a run, columns are days\n",
    "            matrix = group[day_cols].values\n",
    "            prediction_matrices[pd.Timestamp(timestamp)] = matrix\n",
    "        \n",
    "        print(f\"✓ Created {len(prediction_matrices)} prediction matrices\")\n",
    "        \n",
    "        if len(prediction_matrices) > 0:\n",
    "            sample_matrix = list(prediction_matrices.values())[0]\n",
    "            print(f\"  Matrix shape: {sample_matrix.shape[0]} runs × {sample_matrix.shape[1]} days\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Save Prediction Matrices to Pickle\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nSaving prediction matrices...\")\n",
    "        \n",
    "        with open(MODEL_DATA_PATHS['prediction_matrices'], 'wb') as f:\n",
    "            pickle.dump(prediction_matrices, f)\n",
    "        \n",
    "        print(f\"✓ Saved: {MODEL_DATA_PATHS['prediction_matrices']}\")\n",
    "        print(f\"✓ {MODEL_VERSION} complete\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"✓ {CURRENT_COMMODITY.upper()} COMPLETE\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL COMMODITIES PROCESSED\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n✓ Block 01A complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a1799082-9e8d-4379-afd5-1f365820bdc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NOTEBOOK 01B: DATA PREPARATION (Real Predictions)\n",
    "# ============================================================================\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Data Preparation - Real Predictions from Table\n",
    "# MAGIC Load real predictions from commodity.forecast.distributions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./00_setup_and_config\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Load Real Predictions from Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "for CURRENT_COMMODITY in COMMODITY_CONFIGS.keys():\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"PROCESSING: {CURRENT_COMMODITY.upper()}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    # Get configuration for this commodity\n",
    "    CURRENT_CONFIG = COMMODITY_CONFIGS[CURRENT_COMMODITY]\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Get Model Versions Available for This Commodity\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(f\"\\nDiscovering model versions for {CURRENT_COMMODITY}...\")\n",
    "    \n",
    "    model_versions = get_model_versions(CURRENT_COMMODITY)\n",
    "    \n",
    "    if len(model_versions) == 0:\n",
    "        print(f\"\\n⓵ No forecast data found in table for {CURRENT_COMMODITY}\")\n",
    "        print(f\"   Skipping {CURRENT_COMMODITY.upper()}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"✓ Found {len(model_versions)} model versions:\")\n",
    "    for mv in model_versions:\n",
    "        print(f\"  - {mv}\")\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Process Each Model Version\n",
    "    # --------------------------------------------------------------------------\n",
    "    for MODEL_VERSION in model_versions:\n",
    "        print(f\"\\n{'-' * 80}\")\n",
    "        print(f\"PROCESSING MODEL VERSION: {MODEL_VERSION}\")\n",
    "        print(f\"{'-' * 80}\")\n",
    "        \n",
    "        # Get data paths for this model version\n",
    "        DATA_PATHS = get_data_paths(CURRENT_COMMODITY, MODEL_VERSION)\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Load Real Predictions from Table\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nLoading predictions from {FORECAST_TABLE}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load from table for this specific model version\n",
    "            predictions_wide = load_forecast_data(CURRENT_COMMODITY, MODEL_VERSION, spark)\n",
    "            print(f\"✓ Loaded {len(predictions_wide):,} prediction paths\")\n",
    "            \n",
    "            # Display structure\n",
    "            print(f\"\\nData structure:\")\n",
    "            print(f\"  Columns: {list(predictions_wide.columns)}\")\n",
    "            print(f\"  Shape: {predictions_wide.shape}\")\n",
    "            print(f\"  Date range: {predictions_wide['forecast_start_date'].min()} to {predictions_wide['forecast_start_date'].max()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error loading predictions for {CURRENT_COMMODITY.upper()} - {MODEL_VERSION}: {e}\")\n",
    "            print(f\"   Skipping this model version\")\n",
    "            continue\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Transform to Matrix Format\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nTransforming to matrix format...\")\n",
    "        \n",
    "        # Convert forecast_start_date to datetime\n",
    "        predictions_wide['forecast_start_date'] = pd.to_datetime(predictions_wide['forecast_start_date']).dt.normalize()\n",
    "        \n",
    "        # Identify day columns (day_1 through day_14)\n",
    "        day_columns = [f'day_{i}' for i in range(1, 15)]\n",
    "        \n",
    "        # Verify all day columns exist\n",
    "        missing_cols = [col for col in day_columns if col not in predictions_wide.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"\\n❌ Error: Missing day columns: {missing_cols}\")\n",
    "            print(f\"   Skipping {MODEL_VERSION}\")\n",
    "            continue\n",
    "        \n",
    "        # Create prediction matrices dictionary\n",
    "        # Structure: {timestamp: numpy_array(n_paths, 14)}\n",
    "        prediction_matrices = {}\n",
    "        \n",
    "        for timestamp in predictions_wide['forecast_start_date'].unique():\n",
    "            # Get all prediction paths for this timestamp\n",
    "            day_data = predictions_wide[predictions_wide['forecast_start_date'] == timestamp]\n",
    "            \n",
    "            # Extract the 14-day forecast values into a matrix\n",
    "            # Each row is a prediction path, each column is a day ahead\n",
    "            matrix = day_data[day_columns].values\n",
    "            \n",
    "            # Store in dictionary with timestamp as key\n",
    "            prediction_matrices[pd.Timestamp(timestamp)] = matrix\n",
    "        \n",
    "        print(f\"✓ Created {len(prediction_matrices)} prediction matrices\")\n",
    "        \n",
    "        # Verify structure\n",
    "        if len(prediction_matrices) > 0:\n",
    "            sample_timestamp = list(prediction_matrices.keys())[0]\n",
    "            sample_matrix = prediction_matrices[sample_timestamp]\n",
    "            print(f\"  Sample matrix shape: {sample_matrix.shape}\")\n",
    "            print(f\"  (n_paths={sample_matrix.shape[0]}, n_days={sample_matrix.shape[1]})\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Save Prediction Matrices to Volume\n",
    "        # ----------------------------------------------------------------------\n",
    "        matrices_path = DATA_PATHS['prediction_matrices_real']\n",
    "        \n",
    "        with open(matrices_path, 'wb') as f:\n",
    "            pickle.dump(prediction_matrices, f)\n",
    "        \n",
    "        print(f\"\\n✓ Saved prediction matrices: {matrices_path}\")\n",
    "        \n",
    "        print(f\"\\n✓ {MODEL_VERSION} complete\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"✓ {CURRENT_COMMODITY.upper()} COMPLETE\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "print(\"\\n✓ All commodities and model versions processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0dfb89af-09a8-43f6-937a-d06d451d44b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NOTEBOOK 02: STRATEGY IMPLEMENTATIONS (UPDATED - MATCHED PAIRS + INDICATORS)\n",
    "# ============================================================================\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Strategy Implementations - Enhanced with Technical Indicators\n",
    "# MAGIC \n",
    "# MAGIC **VERSION: 3.0 - Complete Restructure with Backward Compatibility**\n",
    "# MAGIC \n",
    "# MAGIC **KEY CHANGES:**\n",
    "# MAGIC - Matched pairs: PriceThreshold and MovingAverage (baseline identical, predictions add overlay)\n",
    "# MAGIC - Daily evaluation for all signal-based strategies\n",
    "# MAGIC - Technical indicators: RSI, ADX, Std Dev (both historical and predicted)\n",
    "# MAGIC - Cost-benefit analysis for prediction strategies\n",
    "# MAGIC - ALL ORIGINAL CONSTRUCTOR SIGNATURES PRESERVED - No downstream changes required\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./00_setup_and_config\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# =============================================================================\n",
    "# TECHNICAL INDICATOR CALCULATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_rsi(prices, period=14):\n",
    "    \"\"\"Calculate Relative Strength Index\"\"\"\n",
    "    if len(prices) < period + 1:\n",
    "        return 50.0\n",
    "    \n",
    "    deltas = np.diff(prices[-period-1:])\n",
    "    gains = np.where(deltas > 0, deltas, 0)\n",
    "    losses = np.where(deltas < 0, -deltas, 0)\n",
    "    \n",
    "    avg_gain = np.mean(gains)\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    if avg_loss == 0:\n",
    "        return 100.0\n",
    "    \n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "\n",
    "def calculate_adx(price_history, period=14):\n",
    "    \"\"\"Calculate Average Directional Index\"\"\"\n",
    "    if len(price_history) < period + 1:\n",
    "        return 20.0, 0.0, 0.0\n",
    "    \n",
    "    if 'high' in price_history.columns and 'low' in price_history.columns:\n",
    "        high = price_history['high'].values\n",
    "        low = price_history['low'].values\n",
    "    else:\n",
    "        high = price_history['price'].values\n",
    "        low = price_history['price'].values\n",
    "    \n",
    "    close = price_history['price'].values\n",
    "    \n",
    "    tr = np.maximum(high[1:] - low[1:], \n",
    "                    np.maximum(abs(high[1:] - close[:-1]), \n",
    "                              abs(low[1:] - close[:-1])))\n",
    "    \n",
    "    plus_dm = np.where((high[1:] - high[:-1]) > (low[:-1] - low[1:]), \n",
    "                       np.maximum(high[1:] - high[:-1], 0), 0)\n",
    "    minus_dm = np.where((low[:-1] - low[1:]) > (high[1:] - high[:-1]), \n",
    "                        np.maximum(low[:-1] - low[1:], 0), 0)\n",
    "    \n",
    "    atr = np.mean(tr[-period:])\n",
    "    if atr > 0:\n",
    "        plus_di = 100 * np.mean(plus_dm[-period:]) / atr\n",
    "        minus_di = 100 * np.mean(minus_dm[-period:]) / atr\n",
    "    else:\n",
    "        plus_di = 0.0\n",
    "        minus_di = 0.0\n",
    "    \n",
    "    di_sum = plus_di + minus_di\n",
    "    if di_sum > 0:\n",
    "        dx = 100 * abs(plus_di - minus_di) / di_sum\n",
    "        adx = dx\n",
    "    else:\n",
    "        adx = 0.0\n",
    "    \n",
    "    return adx, plus_di, minus_di\n",
    "\n",
    "\n",
    "def calculate_std_dev_historical(prices, period=14):\n",
    "    \"\"\"Calculate standard deviation of recent price returns\"\"\"\n",
    "    if len(prices) < period + 1:\n",
    "        return 0.10\n",
    "    \n",
    "    recent_prices = prices[-period:]\n",
    "    returns = np.diff(recent_prices) / recent_prices[:-1]\n",
    "    std_dev = np.std(returns)\n",
    "    \n",
    "    return std_dev\n",
    "\n",
    "\n",
    "def calculate_prediction_confidence(predictions, horizon_day):\n",
    "    \"\"\"Calculate confidence from prediction ensemble using std dev\"\"\"\n",
    "    if predictions is None or predictions.size == 0:\n",
    "        return 1.0\n",
    "    \n",
    "    if horizon_day >= predictions.shape[1]:\n",
    "        horizon_day = predictions.shape[1] - 1\n",
    "    \n",
    "    day_predictions = predictions[:, horizon_day]\n",
    "    median_pred = np.median(day_predictions)\n",
    "    std_dev = np.std(day_predictions)\n",
    "    \n",
    "    cv = std_dev / median_pred if median_pred > 0 else 1.0\n",
    "    \n",
    "    return cv\n",
    "\n",
    "\n",
    "def calculate_rsi_predicted(predictions, period=14):\n",
    "    \"\"\"Calculate RSI on predicted price trajectory\"\"\"\n",
    "    if predictions is None or predictions.size == 0:\n",
    "        return 50.0\n",
    "    \n",
    "    predicted_medians = np.array([np.median(predictions[:, h]) \n",
    "                                 for h in range(predictions.shape[1])])\n",
    "    \n",
    "    return calculate_rsi(predicted_medians, period=min(period, len(predicted_medians)-1))\n",
    "\n",
    "\n",
    "def calculate_adx_predicted(predictions):\n",
    "    \"\"\"Calculate ADX on predicted price trajectory\"\"\"\n",
    "    if predictions is None or predictions.size == 0:\n",
    "        return 20.0, 0.0, 0.0\n",
    "    \n",
    "    predicted_medians = np.array([np.median(predictions[:, h]) \n",
    "                                 for h in range(predictions.shape[1])])\n",
    "    \n",
    "    pred_df = pd.DataFrame({'price': predicted_medians})\n",
    "    \n",
    "    return calculate_adx(pred_df, period=min(14, len(predicted_medians)-1))\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# =============================================================================\n",
    "# BASE STRATEGY CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class Strategy(ABC):\n",
    "    \"\"\"Base class for all strategies - UNCHANGED\"\"\"\n",
    "    \n",
    "    def __init__(self, name, max_holding_days=365):\n",
    "        self.name = name\n",
    "        self.history = []\n",
    "        self.max_holding_days = max_holding_days\n",
    "        self.harvest_start_day = None\n",
    "    \n",
    "    @abstractmethod\n",
    "    def decide(self, day, inventory, current_price, price_history, predictions=None):\n",
    "        pass\n",
    "    \n",
    "    def set_harvest_start(self, day):\n",
    "        self.harvest_start_day = day\n",
    "    \n",
    "    def reset(self):\n",
    "        self.history = []\n",
    "        self.harvest_start_day = None\n",
    "    \n",
    "    def _days_held(self, day):\n",
    "        if self.harvest_start_day is None:\n",
    "            return 0\n",
    "        return day - self.harvest_start_day\n",
    "    \n",
    "    def _force_liquidation_check(self, day, inventory):\n",
    "        if self.harvest_start_day is None:\n",
    "            return None\n",
    "        \n",
    "        days_held = self._days_held(day)\n",
    "        days_remaining = self.max_holding_days - days_held\n",
    "        \n",
    "        if days_remaining <= 0 and inventory > 0:\n",
    "            return {'action': 'SELL', 'amount': inventory, \n",
    "                   'reason': 'max_holding_365d_reached'}\n",
    "        elif days_remaining <= 30 and inventory > 0:\n",
    "            sell_fraction = min(1.0, 0.05 * (31 - days_remaining))\n",
    "            amount = inventory * sell_fraction\n",
    "            return {'action': 'SELL', 'amount': amount,\n",
    "                   'reason': f'approaching_365d_deadline_{days_remaining}d_left'}\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def force_liquidate_before_new_harvest(self, inventory):\n",
    "        if inventory > 0:\n",
    "            return {'action': 'SELL', 'amount': inventory, \n",
    "                   'reason': 'new_harvest_starting_liquidate_old_inventory'}\n",
    "        return None\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## BASELINE STRATEGIES\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "class ImmediateSaleStrategy(Strategy):\n",
    "    \"\"\"\n",
    "    Baseline 1: Sell all inventory weekly - NO CHANGES\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_batch_size=5.0, sale_frequency_days=7):\n",
    "        super().__init__(\"Immediate Sale\")\n",
    "        self.min_batch_size = min_batch_size\n",
    "        self.sale_frequency_days = sale_frequency_days\n",
    "        self.days_since_last_sale = sale_frequency_days\n",
    "    \n",
    "    def decide(self, day, inventory, current_price, price_history, predictions=None):\n",
    "        if inventory <= 0:\n",
    "            return {'action': 'HOLD', 'amount': 0, 'reason': 'no_inventory'}\n",
    "        \n",
    "        forced = self._force_liquidation_check(day, inventory)\n",
    "        if forced:\n",
    "            return forced\n",
    "        \n",
    "        ready_to_sell = (self.days_since_last_sale >= self.sale_frequency_days)\n",
    "        enough_inventory = (inventory >= self.min_batch_size)\n",
    "        \n",
    "        if ready_to_sell and enough_inventory:\n",
    "            self.days_since_last_sale = 0\n",
    "            return {'action': 'SELL', 'amount': inventory, \n",
    "                   'reason': f'immediate_weekly_sale_{inventory:.1f}t'}\n",
    "        \n",
    "        self.days_since_last_sale += 1\n",
    "        if not enough_inventory:\n",
    "            return {'action': 'HOLD', 'amount': 0, \n",
    "                   'reason': f'accumulating_need_{self.min_batch_size:.1f}t_have_{inventory:.1f}t'}\n",
    "        else:\n",
    "            return {'action': 'HOLD', 'amount': 0, \n",
    "                   'reason': f'waiting_for_weekly_sale_day_{self.days_since_last_sale}'}\n",
    "    \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.days_since_last_sale = self.sale_frequency_days\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "class EqualBatchStrategy(Strategy):\n",
    "    \"\"\"\n",
    "    Baseline 2: Sell equal batches on fixed schedule - NO CHANGES\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size=0.25, frequency_days=30):\n",
    "        super().__init__(\"Equal Batches\")\n",
    "        self.batch_size = batch_size\n",
    "        self.frequency = frequency_days\n",
    "        self.last_sale_day = -frequency_days\n",
    "        self.num_sales = 0\n",
    "    \n",
    "    def decide(self, day, inventory, current_price, price_history, predictions=None):\n",
    "        if inventory <= 0:\n",
    "            return {'action': 'HOLD', 'amount': 0, 'reason': 'no_inventory'}\n",
    "        \n",
    "        forced = self._force_liquidation_check(day, inventory)\n",
    "        if forced:\n",
    "            return forced\n",
    "        \n",
    "        days_since_sale = day - self.last_sale_day\n",
    "        \n",
    "        if days_since_sale >= self.frequency:\n",
    "            amount = inventory * self.batch_size\n",
    "            self.last_sale_day = day\n",
    "            self.num_sales += 1\n",
    "            return {'action': 'SELL', 'amount': amount, 'reason': f'scheduled_batch_{self.num_sales}'}\n",
    "        \n",
    "        return {'action': 'HOLD', 'amount': 0, 'reason': 'waiting_for_schedule'}\n",
    "    \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.last_sale_day = -self.frequency\n",
    "        self.num_sales = 0\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## MATCHED PAIR 1: PRICE THRESHOLD\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "class PriceThresholdStrategy(Strategy):\n",
    "    \"\"\"\n",
    "    MATCHED PAIR BASELINE: Price Threshold with Historical Indicators\n",
    "    \n",
    "    CHANGES FROM ORIGINAL:\n",
    "    - Fixed: Use 30-day MA threshold (not reference price)\n",
    "    - Added: Daily evaluation (already had it)\n",
    "    - Added: RSI_historical, ADX_historical, Std_dev_historical\n",
    "    - Changed: Batch sizing 20-35% based on historical signals\n",
    "    - Changed: Cooldown to 7 days (was variable)\n",
    "    \n",
    "    CONSTRUCTOR: UNCHANGED for backward compatibility\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, threshold_pct=0.05, batch_fraction=0.25, max_days_without_sale=60):\n",
    "        super().__init__(\"Price Threshold\")\n",
    "        self.threshold_pct = threshold_pct\n",
    "        self.baseline_batch = batch_fraction  # Used as baseline for dynamic sizing\n",
    "        self.max_days_without_sale = max_days_without_sale\n",
    "        self.cooldown_days = 7  # Standardized cooldown\n",
    "        self.last_sale_day = -self.max_days_without_sale\n",
    "    \n",
    "    def decide(self, day, inventory, current_price, price_history, predictions=None):\n",
    "        if inventory <= 0:\n",
    "            return {'action': 'HOLD', 'amount': 0, 'reason': 'no_inventory'}\n",
    "        \n",
    "        forced = self._force_liquidation_check(day, inventory)\n",
    "        if forced:\n",
    "            return forced\n",
    "        \n",
    "        days_since_sale = day - self.last_sale_day\n",
    "        \n",
    "        # Calculate 30-day MA threshold (FIXED from reference price)\n",
    "        if len(price_history) >= 30:\n",
    "            ma_30 = price_history['price'].tail(30).mean()\n",
    "            threshold = ma_30 * (1 + self.threshold_pct)\n",
    "        else:\n",
    "            threshold = current_price * (1 + self.threshold_pct)\n",
    "        \n",
    "        signal_triggered = current_price > threshold\n",
    "        can_trade = days_since_sale >= self.cooldown_days\n",
    "        \n",
    "        if not signal_triggered:\n",
    "            if days_since_sale >= self.max_days_without_sale:\n",
    "                batch_size = self.baseline_batch\n",
    "                return self._execute_trade(day, inventory, batch_size,\n",
    "                                          f'fallback_{days_since_sale}d')\n",
    "            return {'action': 'HOLD', 'amount': 0, \n",
    "                   'reason': f'below_threshold_{current_price:.2f}<{threshold:.2f}'}\n",
    "        \n",
    "        if not can_trade:\n",
    "            return {'action': 'HOLD', 'amount': 0, \n",
    "                   'reason': f'cooldown_{self.cooldown_days - days_since_sale}d'}\n",
    "        \n",
    "        # ADDED: Analyze with historical indicators\n",
    "        batch_size, reason = self._analyze_with_historical(current_price, price_history)\n",
    "        \n",
    "        return self._execute_trade(day, inventory, batch_size, reason)\n",
    "    \n",
    "    def _analyze_with_historical(self, current_price, price_history):\n",
    "        \"\"\"NEW: Analyze using historical technical indicators\"\"\"\n",
    "        \n",
    "        prices = price_history['price'].values\n",
    "        rsi = calculate_rsi(prices, period=14)\n",
    "        adx, plus_di, minus_di = calculate_adx(price_history, period=14)\n",
    "        std_dev = calculate_std_dev_historical(prices, period=14)\n",
    "        \n",
    "        batch_size = self.baseline_batch\n",
    "        \n",
    "        if rsi > 70 and adx > 25:\n",
    "            batch_size = 0.35\n",
    "            reason = f'overbought_strong_trend_rsi{rsi:.0f}_adx{adx:.0f}'\n",
    "        elif rsi > 70:\n",
    "            batch_size = 0.30\n",
    "            reason = f'overbought_rsi{rsi:.0f}_adx{adx:.0f}'\n",
    "        elif std_dev > 0.03:\n",
    "            batch_size = 0.30\n",
    "            reason = f'high_volatility_stddev{std_dev:.3f}'\n",
    "        elif adx > 25 and rsi < 65:\n",
    "            batch_size = 0.20\n",
    "            reason = f'strong_trend_not_overbought_rsi{rsi:.0f}_adx{adx:.0f}'\n",
    "        else:\n",
    "            reason = f'baseline_rsi{rsi:.0f}_adx{adx:.0f}'\n",
    "        \n",
    "        return batch_size, reason\n",
    "    \n",
    "    def _execute_trade(self, day, inventory, batch_size, reason):\n",
    "        amount = inventory * batch_size\n",
    "        self.last_sale_day = day\n",
    "        return {'action': 'SELL', 'amount': amount, 'reason': reason}\n",
    "    \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.last_sale_day = -self.max_days_without_sale\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "class PriceThresholdPredictive(Strategy):\n",
    "    \"\"\"\n",
    "    MATCHED PAIR PREDICTIVE: Price Threshold with Historical + Predicted Indicators\n",
    "    \n",
    "    CHANGES FROM ORIGINAL:\n",
    "    - MATCHED: Same 30-day MA threshold as baseline\n",
    "    - MATCHED: Same cooldown (7 days)\n",
    "    - MATCHED: Same historical indicator logic as baseline\n",
    "    - Added: RSI_predicted, ADX_predicted, Std_dev_predictions\n",
    "    - Added: Cost-benefit check\n",
    "    - Changed: Batch adjustments ±10% from baseline based on predictions\n",
    "    \n",
    "    CONSTRUCTOR: Optional cost parameters added for cost-benefit (backward compatible)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, threshold_pct=0.05, batch_fraction=0.25, max_days_without_sale=60,\n",
    "                 storage_cost_pct_per_day=0.025, transaction_cost_pct=0.25):\n",
    "        super().__init__(\"Price Threshold Predictive\")\n",
    "        self.threshold_pct = threshold_pct\n",
    "        self.baseline_batch = batch_fraction\n",
    "        self.max_days_without_sale = max_days_without_sale\n",
    "        self.cooldown_days = 7\n",
    "        self.last_sale_day = -self.max_days_without_sale\n",
    "        \n",
    "        # Cost parameters for cost-benefit\n",
    "        self.storage_cost_pct = storage_cost_pct_per_day\n",
    "        self.transaction_cost_pct = transaction_cost_pct\n",
    "    \n",
    "    def decide(self, day, inventory, current_price, price_history, predictions=None):\n",
    "        if inventory <= 0:\n",
    "            return {'action': 'HOLD', 'amount': 0, 'reason': 'no_inventory'}\n",
    "        \n",
    "        forced = self._force_liquidation_check(day, inventory)\n",
    "        if forced:\n",
    "            return forced\n",
    "        \n",
    "        days_since_sale = day - self.last_sale_day\n",
    "        \n",
    "        # IDENTICAL baseline logic as PriceThresholdStrategy\n",
    "        if len(price_history) >= 30:\n",
    "            ma_30 = price_history['price'].tail(30).mean()\n",
    "            threshold = ma_30 * (1 + self.threshold_pct)\n",
    "        else:\n",
    "            threshold = current_price * (1 + self.threshold_pct)\n",
    "        \n",
    "        signal_triggered = current_price > threshold\n",
    "        can_trade = days_since_sale >= self.cooldown_days\n",
    "        \n",
    "        if not signal_triggered:\n",
    "            if days_since_sale >= self.max_days_without_sale:\n",
    "                batch_size = self.baseline_batch\n",
    "                return self._execute_trade(day, inventory, batch_size,\n",
    "                                          f'fallback_{days_since_sale}d')\n",
    "            return {'action': 'HOLD', 'amount': 0, \n",
    "                   'reason': f'below_threshold_{current_price:.2f}<{threshold:.2f}'}\n",
    "        \n",
    "        if not can_trade:\n",
    "            return {'action': 'HOLD', 'amount': 0, \n",
    "                   'reason': f'cooldown_{self.cooldown_days - days_since_sale}d'}\n",
    "        \n",
    "        # Choose analysis path\n",
    "        if predictions is None or predictions.size == 0:\n",
    "            # No predictions - use baseline historical analysis\n",
    "            batch_size, reason = self._analyze_with_historical(current_price, price_history)\n",
    "        else:\n",
    "            # Full predictive analysis\n",
    "            batch_size, reason = self._analyze_with_predictions(\n",
    "                current_price, price_history, predictions\n",
    "            )\n",
    "        \n",
    "        return self._execute_trade(day, inventory, batch_size, reason)\n",
    "    \n",
    "    def _analyze_with_historical(self, current_price, price_history):\n",
    "        \"\"\"IDENTICAL to PriceThresholdStrategy baseline analysis\"\"\"\n",
    "        \n",
    "        prices = price_history['price'].values\n",
    "        rsi = calculate_rsi(prices, period=14)\n",
    "        adx, plus_di, minus_di = calculate_adx(price_history, period=14)\n",
    "        std_dev = calculate_std_dev_historical(prices, period=14)\n",
    "        \n",
    "        batch_size = self.baseline_batch\n",
    "        \n",
    "        if rsi > 70 and adx > 25:\n",
    "            batch_size = 0.35\n",
    "            reason = f'overbought_strong_trend_rsi{rsi:.0f}_adx{adx:.0f}'\n",
    "        elif rsi > 70:\n",
    "            batch_size = 0.30\n",
    "            reason = f'overbought_rsi{rsi:.0f}_adx{adx:.0f}'\n",
    "        elif std_dev > 0.03:\n",
    "            batch_size = 0.30\n",
    "            reason = f'high_volatility_stddev{std_dev:.3f}'\n",
    "        elif adx > 25 and rsi < 65:\n",
    "            batch_size = 0.20\n",
    "            reason = f'strong_trend_not_overbought_rsi{rsi:.0f}_adx{adx:.0f}'\n",
    "        else:\n",
    "            reason = f'baseline_rsi{rsi:.0f}_adx{adx:.0f}'\n",
    "        \n",
    "        return batch_size, reason\n",
    "    \n",
    "    def _analyze_with_predictions(self, current_price, price_history, predictions):\n",
    "        \"\"\"ADDED: Full predictive analysis with historical + predicted indicators\"\"\"\n",
    "        \n",
    "        # Start with baseline historical analysis\n",
    "        prices = price_history['price'].values\n",
    "        rsi_hist = calculate_rsi(prices, period=14)\n",
    "        adx_hist, _, _ = calculate_adx(price_history, period=14)\n",
    "        std_dev_hist = calculate_std_dev_historical(prices, period=14)\n",
    "        \n",
    "        # Get baseline batch from historical signals\n",
    "        batch_size = self.baseline_batch\n",
    "        if rsi_hist > 70 and adx_hist > 25:\n",
    "            batch_size = 0.35\n",
    "        elif rsi_hist > 70:\n",
    "            batch_size = 0.30\n",
    "        elif std_dev_hist > 0.03:\n",
    "            batch_size = 0.30\n",
    "        elif adx_hist > 25 and rsi_hist < 65:\n",
    "            batch_size = 0.20\n",
    "        \n",
    "        # Calculate predicted indicators\n",
    "        rsi_pred = calculate_rsi_predicted(predictions, period=14)\n",
    "        adx_pred, _, _ = calculate_adx_predicted(predictions)\n",
    "        cv_pred = calculate_prediction_confidence(predictions, horizon_day=13)\n",
    "        \n",
    "        # Calculate cost-benefit\n",
    "        net_benefit = self._calculate_cost_benefit(current_price, predictions)\n",
    "        \n",
    "        # Adjust batch based on predictions (±10% from baseline)\n",
    "        adjustment = 0.0\n",
    "        reasons = []\n",
    "        \n",
    "        if cv_pred < 0.05 and net_benefit > 100:\n",
    "            adjustment -= 0.10\n",
    "            reasons.append(f'high_conf_defer_cv{cv_pred:.2%}_netben${net_benefit:.0f}')\n",
    "        elif rsi_hist < rsi_pred and rsi_pred > 75:\n",
    "            adjustment += 0.10\n",
    "            reasons.append(f'pred_reversal_rsi_hist{rsi_hist:.0f}_pred{rsi_pred:.0f}')\n",
    "        elif cv_pred > 0.20 or net_benefit < 0:\n",
    "            adjustment += 0.10\n",
    "            reasons.append(f'low_conf_cv{cv_pred:.2%}_netben${net_benefit:.0f}')\n",
    "        elif adx_pred > 30 and rsi_pred < 70:\n",
    "            adjustment -= 0.05\n",
    "            reasons.append(f'strong_pred_trend_adx{adx_pred:.0f}')\n",
    "        \n",
    "        batch_size = np.clip(batch_size + adjustment, 0.10, 0.45)\n",
    "        \n",
    "        reason = f'hist_rsi{rsi_hist:.0f}_adx{adx_hist:.0f}_' + '_'.join(reasons)\n",
    "        \n",
    "        return batch_size, reason\n",
    "    \n",
    "    def _calculate_cost_benefit(self, current_price, predictions):\n",
    "        \"\"\"Calculate net benefit of waiting vs selling today\"\"\"\n",
    "        \n",
    "        ev_by_day = []\n",
    "        max_horizon = predictions.shape[1]\n",
    "        \n",
    "        for h in range(max_horizon):\n",
    "            future_price = np.median(predictions[:, h])\n",
    "            days_to_wait = h + 1\n",
    "            storage_cost = current_price * (self.storage_cost_pct / 100) * days_to_wait\n",
    "            transaction_cost = future_price * (self.transaction_cost_pct / 100)\n",
    "            ev = future_price - storage_cost - transaction_cost\n",
    "            ev_by_day.append(ev)\n",
    "        \n",
    "        transaction_cost_today = current_price * (self.transaction_cost_pct / 100)\n",
    "        ev_today = current_price - transaction_cost_today\n",
    "        \n",
    "        optimal_ev = max(ev_by_day)\n",
    "        net_benefit = optimal_ev - ev_today\n",
    "        \n",
    "        return net_benefit\n",
    "    \n",
    "    def _execute_trade(self, day, inventory, batch_size, reason):\n",
    "        amount = inventory * batch_size\n",
    "        self.last_sale_day = day\n",
    "        return {'action': 'SELL', 'amount': amount, 'reason': reason}\n",
    "    \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.last_sale_day = -self.max_days_without_sale\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## MATCHED PAIR 2: MOVING AVERAGE\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "class MovingAverageStrategy(Strategy):\n",
    "    \"\"\"\n",
    "    MATCHED PAIR BASELINE: Moving Average with Historical Indicators\n",
    "    \n",
    "    CHANGES FROM ORIGINAL:\n",
    "    - Kept: Crossover trigger (daily evaluation already existed)\n",
    "    - Changed: Cooldown to 7 days (was 5)\n",
    "    - Added: RSI_historical, ADX_historical, Std_dev_historical\n",
    "    - Changed: Batch sizing 20-35% based on historical signals\n",
    "    \n",
    "    CONSTRUCTOR: UNCHANGED for backward compatibility\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ma_period=30, batch_fraction=0.25, max_days_without_sale=60):\n",
    "        super().__init__(\"Moving Average\")\n",
    "        self.period = ma_period\n",
    "        self.baseline_batch = batch_fraction\n",
    "        self.max_days_without_sale = max_days_without_sale\n",
    "        self.cooldown_days = 7  # Changed from 5 to 7\n",
    "        self.last_sale_day = -self.max_days_without_sale\n",
    "    \n",
    "    def decide(self, day, inventory, current_price, price_history, predictions=None):\n",
    "        if inventory <= 0:\n",
    "            return {'action': 'HOLD', 'amount': 0, 'reason': 'no_inventory'}\n",
    "        \n",
    "        forced = self._force_liquidation_check(day, inventory)\n",
    "        if forced:\n",
    "            return forced\n",
    "        \n",
    "        days_since_sale = day - self.last_sale_day\n",
    "        \n",
    "        if days_since_sale >= self.max_days_without_sale:\n",
    "            batch_size = self.baseline_batch\n",
    "            return self._execute_trade(day, inventory, batch_size,\n",
    "                                      f'fallback_{days_since_sale}d')\n",
    "        \n",
    "        if len(price_history) < self.period + 1:\n",
    "            return {'action': 'HOLD', 'amount': 0, 'reason': 'insufficient_history'}\n",
    "        \n",
    "        # Crossover detection\n",
    "        recent_prices = price_history['price'].tail(self.period + 1).values\n",
    "        ma_current = np.mean(recent_prices[-self.period:])\n",
    "        ma_prev = np.mean(recent_prices[-(self.period+1):-1])\n",
    "        prev_price = recent_prices[-2]\n",
    "        \n",
    "        crossover = (prev_price <= ma_prev and current_price > ma_current)\n",
    "        can_trade = days_since_sale >= self.cooldown_days\n",
    "        \n",
    "        if not crossover:\n",
    "            return {'action': 'HOLD', 'amount': 0, 'reason': 'no_crossover'}\n",
    "        \n",
    "        if not can_trade:\n",
    "            return {'action': 'HOLD', 'amount': 0, \n",
    "                   'reason': f'cooldown_{self.cooldown_days - days_since_sale}d'}\n",
    "        \n",
    "        # ADDED: Analyze with historical indicators\n",
    "        batch_size, reason = self._analyze_with_historical(current_price, price_history)\n",
    "        \n",
    "        return self._execute_trade(day, inventory, batch_size, reason)\n",
    "    \n",
    "    def _analyze_with_historical(self, current_price, price_history):\n",
    "        \"\"\"NEW: Analyze using historical technical indicators\"\"\"\n",
    "        \n",
    "        prices = price_history['price'].values\n",
    "        rsi = calculate_rsi(prices, period=14)\n",
    "        adx, plus_di, minus_di = calculate_adx(price_history, period=14)\n",
    "        std_dev = calculate_std_dev_historical(prices, period=14)\n",
    "        \n",
    "        batch_size = self.baseline_batch\n",
    "        \n",
    "        if adx > 25 and rsi >= 45 and rsi <= 70:\n",
    "            batch_size = 0.20\n",
    "            reason = f'strong_momentum_rsi{rsi:.0f}_adx{adx:.0f}'\n",
    "        elif rsi > 70 and adx > 25:\n",
    "            batch_size = 0.35\n",
    "            reason = f'overbought_strong_trend_rsi{rsi:.0f}_adx{adx:.0f}'\n",
    "        elif rsi > 70:\n",
    "            batch_size = 0.30\n",
    "            reason = f'overbought_rsi{rsi:.0f}'\n",
    "        elif std_dev > 0.03:\n",
    "            batch_size = 0.30\n",
    "            reason = f'high_volatility_stddev{std_dev:.3f}'\n",
    "        elif adx < 20:\n",
    "            batch_size = 0.25\n",
    "            reason = f'weak_trend_choppy_adx{adx:.0f}'\n",
    "        else:\n",
    "            reason = f'baseline_crossover_rsi{rsi:.0f}_adx{adx:.0f}'\n",
    "        \n",
    "        return batch_size, reason\n",
    "    \n",
    "    def _execute_trade(self, day, inventory, batch_size, reason):\n",
    "        amount = inventory * batch_size\n",
    "        self.last_sale_day = day\n",
    "        return {'action': 'SELL', 'amount': amount, 'reason': reason}\n",
    "    \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.last_sale_day = -self.max_days_without_sale\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "class MovingAveragePredictive(Strategy):\n",
    "    \"\"\"\n",
    "    MATCHED PAIR PREDICTIVE: Moving Average with Historical + Predicted Indicators\n",
    "    \n",
    "    CHANGES FROM ORIGINAL:\n",
    "    - MATCHED: Same crossover trigger as baseline\n",
    "    - MATCHED: Same cooldown (7 days)\n",
    "    - MATCHED: Same historical indicator logic as baseline\n",
    "    - Added: RSI_predicted, ADX_predicted, Std_dev_predictions\n",
    "    - Added: Cost-benefit check\n",
    "    - Changed: Batch adjustments ±10% from baseline based on predictions\n",
    "    \n",
    "    CONSTRUCTOR: Optional cost parameters added for cost-benefit (backward compatible)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ma_period=30, batch_fraction=0.25, max_days_without_sale=60,\n",
    "                 storage_cost_pct_per_day=0.025, transaction_cost_pct=0.25):\n",
    "        super().__init__(\"Moving Average Predictive\")\n",
    "        self.period = ma_period\n",
    "        self.baseline_batch = batch_fraction\n",
    "        self.max_days_without_sale = max_days_without_sale\n",
    "        self.cooldown_days = 7\n",
    "        self.last_sale_day = -self.max_days_without_sale\n",
    "        \n",
    "        # Cost parameters for cost-benefit\n",
    "        self.storage_cost_pct = storage_cost_pct_per_day\n",
    "        self.transaction_cost_pct = transaction_cost_pct\n",
    "    \n",
    "    def decide(self, day, inventory, current_price, price_history, predictions=None):\n",
    "        if inventory <= 0:\n",
    "            return {'action': 'HOLD', 'amount': 0, 'reason': 'no_inventory'}\n",
    "        \n",
    "        forced = self._force_liquidation_check(day, inventory)\n",
    "        if forced:\n",
    "            return forced\n",
    "        \n",
    "        days_since_sale = day - self.last_sale_day\n",
    "        \n",
    "        if days_since_sale >= self.max_days_without_sale:\n",
    "            batch_size = self.baseline_batch\n",
    "            return self._execute_trade(day, inventory, batch_size,\n",
    "                                      f'fallback_{days_since_sale}d')\n",
    "        \n",
    "        if len(price_history) < self.period + 1:\n",
    "            return {'action': 'HOLD', 'amount': 0, 'reason': 'insufficient_history'}\n",
    "        \n",
    "        # IDENTICAL baseline logic as MovingAverageStrategy\n",
    "        recent_prices = price_history['price'].tail(self.period + 1).values\n",
    "        ma_current = np.mean(recent_prices[-self.period:])\n",
    "        ma_prev = np.mean(recent_prices[-(self.period+1):-1])\n",
    "        prev_price = recent_prices[-2]\n",
    "        \n",
    "        crossover = (prev_price <= ma_prev and current_price > ma_current)\n",
    "        can_trade = days_since_sale >= self.cooldown_days\n",
    "        \n",
    "        if not crossover:\n",
    "            return {'action': 'HOLD', 'amount': 0, 'reason': 'no_crossover'}\n",
    "        \n",
    "        if not can_trade:\n",
    "            return {'action': 'HOLD', 'amount': 0, \n",
    "                   'reason': f'cooldown_{self.cooldown_days - days_since_sale}d'}\n",
    "        \n",
    "        # Choose analysis path\n",
    "        if predictions is None or predictions.size == 0:\n",
    "            # No predictions - use baseline historical analysis\n",
    "            batch_size, reason = self._analyze_with_historical(current_price, price_history)\n",
    "        else:\n",
    "            # Full predictive analysis\n",
    "            batch_size, reason = self._analyze_with_predictions(\n",
    "                current_price, price_history, predictions\n",
    "            )\n",
    "        \n",
    "        return self._execute_trade(day, inventory, batch_size, reason)\n",
    "    \n",
    "    def _analyze_with_historical(self, current_price, price_history):\n",
    "        \"\"\"IDENTICAL to MovingAverageStrategy baseline analysis\"\"\"\n",
    "        \n",
    "        prices = price_history['price'].values\n",
    "        rsi = calculate_rsi(prices, period=14)\n",
    "        adx, plus_di, minus_di = calculate_adx(price_history, period=14)\n",
    "        std_dev = calculate_std_dev_historical(prices, period=14)\n",
    "        \n",
    "        batch_size = self.baseline_batch\n",
    "        \n",
    "        if adx > 25 and rsi >= 45 and rsi <= 70:\n",
    "            batch_size = 0.20\n",
    "            reason = f'strong_momentum_rsi{rsi:.0f}_adx{adx:.0f}'\n",
    "        elif rsi > 70 and adx > 25:\n",
    "            batch_size = 0.35\n",
    "            reason = f'overbought_strong_trend_rsi{rsi:.0f}_adx{adx:.0f}'\n",
    "        elif rsi > 70:\n",
    "            batch_size = 0.30\n",
    "            reason = f'overbought_rsi{rsi:.0f}'\n",
    "        elif std_dev > 0.03:\n",
    "            batch_size = 0.30\n",
    "            reason = f'high_volatility_stddev{std_dev:.3f}'\n",
    "        elif adx < 20:\n",
    "            batch_size = 0.25\n",
    "            reason = f'weak_trend_choppy_adx{adx:.0f}'\n",
    "        else:\n",
    "            reason = f'baseline_crossover_rsi{rsi:.0f}_adx{adx:.0f}'\n",
    "        \n",
    "        return batch_size, reason\n",
    "    \n",
    "    def _analyze_with_predictions(self, current_price, price_history, predictions):\n",
    "        \"\"\"ADDED: Full predictive analysis with historical + predicted indicators\"\"\"\n",
    "        \n",
    "        # Start with baseline historical analysis\n",
    "        prices = price_history['price'].values\n",
    "        rsi_hist = calculate_rsi(prices, period=14)\n",
    "        adx_hist, _, _ = calculate_adx(price_history, period=14)\n",
    "        std_dev_hist = calculate_std_dev_historical(prices, period=14)\n",
    "        \n",
    "        # Get baseline batch from historical signals\n",
    "        batch_size = self.baseline_batch\n",
    "        if adx_hist > 25 and rsi_hist >= 45 and rsi_hist <= 70:\n",
    "            batch_size = 0.20\n",
    "        elif rsi_hist > 70 and adx_hist > 25:\n",
    "            batch_size = 0.35\n",
    "        elif rsi_hist > 70:\n",
    "            batch_size = 0.30\n",
    "        elif std_dev_hist > 0.03:\n",
    "            batch_size = 0.30\n",
    "        elif adx_hist < 20:\n",
    "            batch_size = 0.25\n",
    "        \n",
    "        # Calculate predicted indicators\n",
    "        rsi_pred = calculate_rsi_predicted(predictions, period=14)\n",
    "        adx_pred, _, _ = calculate_adx_predicted(predictions)\n",
    "        cv_pred = calculate_prediction_confidence(predictions, horizon_day=13)\n",
    "        \n",
    "        # Calculate cost-benefit\n",
    "        net_benefit = self._calculate_cost_benefit(current_price, predictions)\n",
    "        \n",
    "        # Adjust batch based on predictions (±10% from baseline)\n",
    "        adjustment = 0.0\n",
    "        reasons = []\n",
    "        \n",
    "        if adx_pred > 25 and cv_pred < 0.05 and net_benefit > 100:\n",
    "            adjustment -= 0.10\n",
    "            reasons.append(f'momentum_continues_adx{adx_pred:.0f}_defer')\n",
    "        elif adx_pred < 15 or (adx_hist > adx_pred * 1.2):\n",
    "            adjustment += 0.10\n",
    "            reasons.append(f'momentum_fading_adx_hist{adx_hist:.0f}_pred{adx_pred:.0f}')\n",
    "        elif cv_pred > 0.20 or net_benefit < 0:\n",
    "            adjustment += 0.10\n",
    "            reasons.append(f'low_conf_cv{cv_pred:.2%}_netben${net_benefit:.0f}')\n",
    "        elif rsi_pred > 75:\n",
    "            adjustment += 0.05\n",
    "            reasons.append(f'pred_overbought_rsi{rsi_pred:.0f}')\n",
    "        \n",
    "        batch_size = np.clip(batch_size + adjustment, 0.10, 0.45)\n",
    "        \n",
    "        reason = f'hist_rsi{rsi_hist:.0f}_adx{adx_hist:.0f}_' + '_'.join(reasons)\n",
    "        \n",
    "        return batch_size, reason\n",
    "    \n",
    "    def _calculate_cost_benefit(self, current_price, predictions):\n",
    "        \"\"\"Calculate net benefit of waiting vs selling today\"\"\"\n",
    "        \n",
    "        ev_by_day = []\n",
    "        max_horizon = predictions.shape[1]\n",
    "        \n",
    "        for h in range(max_horizon):\n",
    "            future_price = np.median(predictions[:, h])\n",
    "            days_to_wait = h + 1\n",
    "            storage_cost = current_price * (self.storage_cost_pct / 100) * days_to_wait\n",
    "            transaction_cost = future_price * (self.transaction_cost_pct / 100)\n",
    "            ev = future_price - storage_cost - transaction_cost\n",
    "            ev_by_day.append(ev)\n",
    "        \n",
    "        transaction_cost_today = current_price * (self.transaction_cost_pct / 100)\n",
    "        ev_today = current_price - transaction_cost_today\n",
    "        \n",
    "        optimal_ev = max(ev_by_day)\n",
    "        net_benefit = optimal_ev - ev_today\n",
    "        \n",
    "        return net_benefit\n",
    "    \n",
    "    def _execute_trade(self, day, inventory, batch_size, reason):\n",
    "        amount = inventory * batch_size\n",
    "        self.last_sale_day = day\n",
    "        return {'action': 'SELL', 'amount': amount, 'reason': reason}\n",
    "    \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.last_sale_day = -self.max_days_without_sale\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## STANDALONE PREDICTION STRATEGIES\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "class ConsensusStrategy(Strategy):\n",
    "    \"\"\"\n",
    "    Standalone Prediction Strategy: Consensus with Full Indicators\n",
    "    \n",
    "    CHANGES FROM ORIGINAL:\n",
    "    - Changed: Daily evaluation (was every 14 days)\n",
    "    - Added: RSI_historical, ADX_historical, Std_dev_historical\n",
    "    - Added: RSI_predicted, ADX_predicted, Std_dev_predictions\n",
    "    - Added: Cost-benefit check\n",
    "    - Changed: Batch 5-40% based on all signals\n",
    "    \n",
    "    CONSTRUCTOR: Optional cost parameters added (backward compatible)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, consensus_threshold=0.70, min_return=0.03, evaluation_day=14,\n",
    "                 storage_cost_pct_per_day=0.025, transaction_cost_pct=0.25):\n",
    "        super().__init__(\"Consensus\")\n",
    "        self.consensus_threshold = consensus_threshold\n",
    "        self.min_return = min_return\n",
    "        self.evaluation_day = evaluation_day  # Keep for horizon day selection\n",
    "        self.cooldown_days = 7  # Changed from implicit 14 to explicit 7\n",
    "        self.last_sale_day = -self.cooldown_days\n",
    "        \n",
    "        # Cost parameters\n",
    "        self.storage_cost_pct = storage_cost_pct_per_day\n",
    "        self.transaction_cost_pct = transaction_cost_pct\n",
    "    \n",
    "    def decide(self, day, inventory, current_price, price_history, predictions=None):\n",
    "        if inventory <= 0:\n",
    "            return {'action': 'HOLD', 'amount': 0, 'reason': 'no_inventory'}\n",
    "        \n",
    "        forced = self._force_liquidation_check(day, inventory)\n",
    "        if forced:\n",
    "            return forced\n",
    "        \n",
    "        days_since_sale = day - self.last_sale_day\n",
    "        \n",
    "        # CHANGED: Check cooldown daily (not scheduled every 14 days)\n",
    "        if days_since_sale < self.cooldown_days:\n",
    "            return {'action': 'HOLD', 'amount': 0, \n",
    "                   'reason': f'cooldown_{self.cooldown_days - days_since_sale}d'}\n",
    "        \n",
    "        if predictions is None or predictions.size == 0:\n",
    "            if days_since_sale >= 30:\n",
    "                return self._execute_trade(day, inventory, 0.20, 'no_predictions_fallback')\n",
    "            return {'action': 'HOLD', 'amount': 0, 'reason': 'no_predictions_waiting'}\n",
    "        \n",
    "        # ADDED: Full analysis with all indicators\n",
    "        batch_size, reason = self._analyze_consensus(\n",
    "            current_price, price_history, predictions\n",
    "        )\n",
    "        \n",
    "        if batch_size > 0:\n",
    "            return self._execute_trade(day, inventory, batch_size, reason)\n",
    "        else:\n",
    "            return {'action': 'HOLD', 'amount': 0, 'reason': reason}\n",
    "    \n",
    "    def _analyze_consensus(self, current_price, price_history, predictions):\n",
    "        \"\"\"ENHANCED: Analyze using historical + predicted indicators + consensus\"\"\"\n",
    "        \n",
    "        # Historical indicators\n",
    "        prices = price_history['price'].values\n",
    "        rsi_hist = calculate_rsi(prices, period=14)\n",
    "        adx_hist, _, _ = calculate_adx(price_history, period=14)\n",
    "        \n",
    "        # Predicted indicators\n",
    "        rsi_pred = calculate_rsi_predicted(predictions, period=14)\n",
    "        adx_pred, _, _ = calculate_adx_predicted(predictions)\n",
    "        cv_pred = calculate_prediction_confidence(predictions, horizon_day=min(self.evaluation_day-1, predictions.shape[1]-1))\n",
    "        \n",
    "        # Consensus analysis\n",
    "        eval_day_idx = min(self.evaluation_day, predictions.shape[1]) - 1\n",
    "        day_preds = predictions[:, eval_day_idx]\n",
    "        median_pred = np.median(day_preds)\n",
    "        expected_return = (median_pred - current_price) / current_price\n",
    "        bullish_pct = np.mean(day_preds > current_price)\n",
    "        \n",
    "        # Cost-benefit\n",
    "        net_benefit = self._calculate_cost_benefit(current_price, predictions)\n",
    "        \n",
    "        # Decision matrix\n",
    "        batch_size = 0.0\n",
    "        \n",
    "        if (bullish_pct >= 0.80 and expected_return >= self.min_return and\n",
    "            cv_pred < 0.05 and adx_pred > 25 and net_benefit > 100):\n",
    "            batch_size = 0.05\n",
    "            reason = f'very_strong_consensus_{bullish_pct:.0%}_defer'\n",
    "        elif (bullish_pct >= self.consensus_threshold and expected_return >= self.min_return and\n",
    "              cv_pred < 0.10 and net_benefit > 50):\n",
    "            batch_size = 0.10\n",
    "            reason = f'strong_consensus_{bullish_pct:.0%}_conf{cv_pred:.1%}'\n",
    "        elif bullish_pct >= 0.60 and expected_return >= self.min_return * 0.5:\n",
    "            batch_size = 0.20\n",
    "            reason = f'moderate_consensus_{bullish_pct:.0%}'\n",
    "        elif cv_pred > 0.20 or bullish_pct < 0.55:\n",
    "            batch_size = 0.30\n",
    "            reason = f'weak_consensus_{bullish_pct:.0%}_or_high_unc{cv_pred:.1%}'\n",
    "        elif bullish_pct < 0.40 or expected_return < -self.min_return:\n",
    "            batch_size = 0.40\n",
    "            reason = f'bearish_consensus_{bullish_pct:.0%}_ret{expected_return:.1%}'\n",
    "        else:\n",
    "            batch_size = 0.20\n",
    "            reason = f'mixed_signals_cons{bullish_pct:.0%}_ret{expected_return:.1%}'\n",
    "        \n",
    "        return batch_size, reason\n",
    "    \n",
    "    def _calculate_cost_benefit(self, current_price, predictions):\n",
    "        ev_by_day = []\n",
    "        for h in range(predictions.shape[1]):\n",
    "            future_price = np.median(predictions[:, h])\n",
    "            days_to_wait = h + 1\n",
    "            storage_cost = current_price * (self.storage_cost_pct / 100) * days_to_wait\n",
    "            transaction_cost = future_price * (self.transaction_cost_pct / 100)\n",
    "            ev = future_price - storage_cost - transaction_cost\n",
    "            ev_by_day.append(ev)\n",
    "        \n",
    "        transaction_cost_today = current_price * (self.transaction_cost_pct / 100)\n",
    "        ev_today = current_price - transaction_cost_today\n",
    "        \n",
    "        return max(ev_by_day) - ev_today\n",
    "    \n",
    "    def _execute_trade(self, day, inventory, batch_size, reason):\n",
    "        amount = inventory * batch_size\n",
    "        self.last_sale_day = day\n",
    "        return {'action': 'SELL', 'amount': amount, 'reason': reason}\n",
    "    \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.last_sale_day = -self.cooldown_days\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "class ExpectedValueStrategy(Strategy):\n",
    "    \"\"\"\n",
    "    Standalone Prediction Strategy: Expected Value with Full Indicators\n",
    "    \n",
    "    CHANGES FROM ORIGINAL:\n",
    "    - Changed: Daily evaluation (was every 10 days)\n",
    "    - Added: RSI_historical, ADX_historical, Std_dev_historical\n",
    "    - Added: RSI_predicted, ADX_predicted, Std_dev_predictions\n",
    "    - Kept: Cost-benefit (already existed)\n",
    "    - Changed: Batch 10-35% based on EV and confidence\n",
    "    \n",
    "    CONSTRUCTOR: UNCHANGED (already had cost parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage_cost_pct_per_day, transaction_cost_pct,\n",
    "                 min_ev_improvement=50, baseline_batch=0.15, baseline_frequency=10):\n",
    "        super().__init__(\"Expected Value\")\n",
    "        self.storage_cost_pct_per_day = storage_cost_pct_per_day\n",
    "        self.transaction_cost_pct = transaction_cost_pct\n",
    "        self.min_ev_improvement = min_ev_improvement\n",
    "        self.baseline_batch = baseline_batch\n",
    "        self.baseline_frequency = baseline_frequency  # Keep for reference but not used in scheduling\n",
    "        self.cooldown_days = 7  # Changed from implicit 10 to explicit 7\n",
    "        self.last_sale_day = -self.cooldown_days\n",
    "    \n",
    "    def decide(self, day, inventory, current_price, price_history, predictions=None):\n",
    "        if inventory <= 0:\n",
    "            return {'action': 'HOLD', 'amount': 0, 'reason': 'no_inventory'}\n",
    "        \n",
    "        forced = self._force_liquidation_check(day, inventory)\n",
    "        if forced:\n",
    "            return forced\n",
    "        \n",
    "        days_since_sale = day - self.last_sale_day\n",
    "        \n",
    "        # CHANGED: Check cooldown daily (not scheduled every 10 days)\n",
    "        if days_since_sale < self.cooldown_days:\n",
    "            return {'action': 'HOLD', 'amount': 0, \n",
    "                   'reason': f'cooldown_{self.cooldown_days - days_since_sale}d'}\n",
    "        \n",
    "        if predictions is None or predictions.size == 0:\n",
    "            if days_since_sale >= 30:\n",
    "                return self._execute_trade(day, inventory, self.baseline_batch, 'no_predictions_fallback')\n",
    "            return {'action': 'HOLD', 'amount': 0, 'reason': 'no_predictions_waiting'}\n",
    "        \n",
    "        # ENHANCED: Full EV analysis with all indicators\n",
    "        batch_size, reason = self._analyze_expected_value(\n",
    "            current_price, price_history, predictions\n",
    "        )\n",
    "        \n",
    "        if batch_size > 0:\n",
    "            return self._execute_trade(day, inventory, batch_size, reason)\n",
    "        else:\n",
    "            return {'action': 'HOLD', 'amount': 0, 'reason': reason}\n",
    "    \n",
    "    def _analyze_expected_value(self, current_price, price_history, predictions):\n",
    "        \"\"\"ENHANCED: EV optimization + all indicators\"\"\"\n",
    "        \n",
    "        # Historical indicators\n",
    "        prices = price_history['price'].values\n",
    "        rsi_hist = calculate_rsi(prices, period=14)\n",
    "        adx_hist, _, _ = calculate_adx(price_history, period=14)\n",
    "        \n",
    "        # Predicted indicators\n",
    "        rsi_pred = calculate_rsi_predicted(predictions, period=14)\n",
    "        adx_pred, _, _ = calculate_adx_predicted(predictions)\n",
    "        cv_pred = calculate_prediction_confidence(predictions, horizon_day=13)\n",
    "        \n",
    "        # Find optimal sale day\n",
    "        optimal_day, net_benefit = self._find_optimal_sale_day(current_price, predictions)\n",
    "        \n",
    "        # Decision matrix\n",
    "        batch_size = 0.0\n",
    "        \n",
    "        if optimal_day <= 3 and cv_pred < 0.08 and net_benefit > self.min_ev_improvement:\n",
    "            batch_size = 0.20\n",
    "            reason = f'peak_soon_day{optimal_day}_ev${net_benefit:.0f}_conf{cv_pred:.1%}'\n",
    "        elif optimal_day <= 7 and cv_pred < 0.12 and net_benefit > self.min_ev_improvement:\n",
    "            batch_size = 0.15\n",
    "            reason = f'peak_mid_day{optimal_day}_ev${net_benefit:.0f}'\n",
    "        elif optimal_day > 7 and cv_pred < 0.08 and adx_pred > 25:\n",
    "            batch_size = 0.10\n",
    "            reason = f'peak_late_day{optimal_day}_high_conf_defer'\n",
    "        elif optimal_day > 7:\n",
    "            batch_size = 0.15\n",
    "            reason = f'peak_late_day{optimal_day}_uncertain_cv{cv_pred:.1%}'\n",
    "        elif net_benefit < self.min_ev_improvement:\n",
    "            if cv_pred < 0.08:\n",
    "                batch_size = 0.10\n",
    "                reason = f'no_ev_benefit_high_conf_cv{cv_pred:.1%}'\n",
    "            elif cv_pred < 0.15:\n",
    "                batch_size = 0.15\n",
    "                reason = f'no_ev_benefit_mod_conf_cv{cv_pred:.1%}'\n",
    "            else:\n",
    "                batch_size = 0.35\n",
    "                reason = f'no_ev_benefit_low_conf_cv{cv_pred:.1%}'\n",
    "        else:\n",
    "            batch_size = 0.15\n",
    "            reason = f'baseline_ev${net_benefit:.0f}_day{optimal_day}'\n",
    "        \n",
    "        return batch_size, reason\n",
    "    \n",
    "    def _find_optimal_sale_day(self, current_price, predictions):\n",
    "        ev_by_day = []\n",
    "        for h in range(predictions.shape[1]):\n",
    "            future_price = np.median(predictions[:, h])\n",
    "            days_to_wait = h + 1\n",
    "            storage_cost = current_price * (self.storage_cost_pct_per_day / 100) * days_to_wait\n",
    "            transaction_cost = future_price * (self.transaction_cost_pct / 100)\n",
    "            ev = future_price - storage_cost - transaction_cost\n",
    "            ev_by_day.append(ev)\n",
    "        \n",
    "        transaction_cost_today = current_price * (self.transaction_cost_pct / 100)\n",
    "        ev_today = current_price - transaction_cost_today\n",
    "        \n",
    "        optimal_day = np.argmax(ev_by_day)\n",
    "        net_benefit = ev_by_day[optimal_day] - ev_today\n",
    "        \n",
    "        return optimal_day, net_benefit\n",
    "    \n",
    "    def _execute_trade(self, day, inventory, batch_size, reason):\n",
    "        amount = inventory * batch_size\n",
    "        self.last_sale_day = day\n",
    "        return {'action': 'SELL', 'amount': amount, 'reason': reason}\n",
    "    \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.last_sale_day = -self.cooldown_days\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "class RiskAdjustedStrategy(Strategy):\n",
    "    \"\"\"\n",
    "    Standalone Prediction Strategy: Risk-Adjusted with Full Indicators\n",
    "    \n",
    "    CHANGES FROM ORIGINAL:\n",
    "    - Changed: Daily evaluation (was every 12 days)\n",
    "    - Added: RSI_historical, ADX_historical, Std_dev_historical\n",
    "    - Added: RSI_predicted, ADX_predicted, Std_dev_predictions\n",
    "    - Added: Cost-benefit check\n",
    "    - Changed: Batch 8-40% based on risk/uncertainty\n",
    "    - Std_dev_predictions is PRIMARY driver\n",
    "    \n",
    "    CONSTRUCTOR: Optional cost parameters added (backward compatible)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_return=0.05, max_uncertainty=0.08, \n",
    "                 consensus_threshold=0.65, evaluation_day=14,\n",
    "                 storage_cost_pct_per_day=0.025, transaction_cost_pct=0.25):\n",
    "        super().__init__(\"Risk-Adjusted\")\n",
    "        self.min_return = min_return\n",
    "        self.max_uncertainty = max_uncertainty  # Keep for reference\n",
    "        self.consensus_threshold = consensus_threshold  # Keep for reference\n",
    "        self.evaluation_day = evaluation_day  # Keep for horizon day selection\n",
    "        self.cooldown_days = 7  # Changed from implicit 12 to explicit 7\n",
    "        self.last_sale_day = -self.cooldown_days\n",
    "        \n",
    "        # Cost parameters\n",
    "        self.storage_cost_pct = storage_cost_pct_per_day\n",
    "        self.transaction_cost_pct = transaction_cost_pct\n",
    "    \n",
    "    def decide(self, day, inventory, current_price, price_history, predictions=None):\n",
    "        if inventory <= 0:\n",
    "            return {'action': 'HOLD', 'amount': 0, 'reason': 'no_inventory'}\n",
    "        \n",
    "        forced = self._force_liquidation_check(day, inventory)\n",
    "        if forced:\n",
    "            return forced\n",
    "        \n",
    "        days_since_sale = day - self.last_sale_day\n",
    "        \n",
    "        # CHANGED: Check cooldown daily (not scheduled every 12 days)\n",
    "        if days_since_sale < self.cooldown_days:\n",
    "            return {'action': 'HOLD', 'amount': 0, \n",
    "                   'reason': f'cooldown_{self.cooldown_days - days_since_sale}d'}\n",
    "        \n",
    "        if predictions is None or predictions.size == 0:\n",
    "            if days_since_sale >= 30:\n",
    "                return self._execute_trade(day, inventory, 0.18, 'no_predictions_fallback')\n",
    "            return {'action': 'HOLD', 'amount': 0, 'reason': 'no_predictions_waiting'}\n",
    "        \n",
    "        # ENHANCED: Full risk-adjusted analysis\n",
    "        batch_size, reason = self._analyze_risk_adjusted(\n",
    "            current_price, price_history, predictions\n",
    "        )\n",
    "        \n",
    "        if batch_size > 0:\n",
    "            return self._execute_trade(day, inventory, batch_size, reason)\n",
    "        else:\n",
    "            return {'action': 'HOLD', 'amount': 0, 'reason': reason}\n",
    "    \n",
    "    def _analyze_risk_adjusted(self, current_price, price_history, predictions):\n",
    "        \"\"\"ENHANCED: Analyze risk/return using all indicators\"\"\"\n",
    "        \n",
    "        # Historical indicators\n",
    "        prices = price_history['price'].values\n",
    "        rsi_hist = calculate_rsi(prices, period=14)\n",
    "        adx_hist, _, _ = calculate_adx(price_history, period=14)\n",
    "        \n",
    "        # Predicted indicators (cv_pred is PRIMARY driver)\n",
    "        rsi_pred = calculate_rsi_predicted(predictions, period=14)\n",
    "        adx_pred, _, _ = calculate_adx_predicted(predictions)\n",
    "        cv_pred = calculate_prediction_confidence(predictions, horizon_day=min(self.evaluation_day-1, predictions.shape[1]-1))\n",
    "        \n",
    "        # Expected return\n",
    "        eval_day_idx = min(self.evaluation_day, predictions.shape[1]) - 1\n",
    "        day_preds = predictions[:, eval_day_idx]\n",
    "        median_pred = np.median(day_preds)\n",
    "        expected_return = (median_pred - current_price) / current_price\n",
    "        \n",
    "        # Trajectory\n",
    "        predicted_medians = [np.median(predictions[:, h]) for h in range(predictions.shape[1])]\n",
    "        daily_changes = np.diff(predicted_medians)\n",
    "        pct_positive = np.mean(daily_changes > 0)\n",
    "        \n",
    "        # Cost-benefit\n",
    "        net_benefit = self._calculate_cost_benefit(current_price, predictions)\n",
    "        \n",
    "        # Risk assessment - cv_pred is PRIMARY driver\n",
    "        batch_size = 0.0\n",
    "        \n",
    "        if (cv_pred < 0.05 and adx_pred > 25 and expected_return >= self.min_return and\n",
    "            pct_positive > 0.70 and net_benefit > 100):\n",
    "            batch_size = 0.08\n",
    "            reason = f'very_low_risk_cv{cv_pred:.1%}_adx{adx_pred:.0f}_defer'\n",
    "        elif cv_pred < 0.10 and adx_pred > 20 and expected_return >= self.min_return * 0.67 and net_benefit > 50:\n",
    "            batch_size = 0.12\n",
    "            reason = f'low_risk_cv{cv_pred:.1%}_ret{expected_return:.1%}'\n",
    "        elif cv_pred < 0.15 and expected_return > 0:\n",
    "            batch_size = 0.18\n",
    "            reason = f'medium_risk_cv{cv_pred:.1%}_baseline'\n",
    "        elif cv_pred < 0.25 or adx_pred < 15:\n",
    "            batch_size = 0.25\n",
    "            reason = f'med_high_risk_cv{cv_pred:.1%}_weak_trend'\n",
    "        elif cv_pred >= 0.25 or expected_return < 0:\n",
    "            batch_size = 0.30\n",
    "            reason = f'high_risk_cv{cv_pred:.1%}_reduce_exposure'\n",
    "        else:\n",
    "            batch_size = 0.40\n",
    "            reason = f'very_high_risk_cv{cv_pred:.1%}_exit_fast'\n",
    "        \n",
    "        return batch_size, reason\n",
    "    \n",
    "    def _calculate_cost_benefit(self, current_price, predictions):\n",
    "        ev_by_day = []\n",
    "        for h in range(predictions.shape[1]):\n",
    "            future_price = np.median(predictions[:, h])\n",
    "            days_to_wait = h + 1\n",
    "            storage_cost = current_price * (self.storage_cost_pct / 100) * days_to_wait\n",
    "            transaction_cost = future_price * (self.transaction_cost_pct / 100)\n",
    "            ev = future_price - storage_cost - transaction_cost\n",
    "            ev_by_day.append(ev)\n",
    "        \n",
    "        transaction_cost_today = current_price * (self.transaction_cost_pct / 100)\n",
    "        ev_today = current_price - transaction_cost_today\n",
    "        \n",
    "        return max(ev_by_day) - ev_today\n",
    "    \n",
    "    def _execute_trade(self, day, inventory, batch_size, reason):\n",
    "        amount = inventory * batch_size\n",
    "        self.last_sale_day = day\n",
    "        return {'action': 'SELL', 'amount': amount, 'reason': reason}\n",
    "    \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.last_sale_day = -self.cooldown_days\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## TESTING ALL STRATEGIES\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Test all strategies with sample data\n",
    "test_day = 30\n",
    "test_inventory = 50\n",
    "test_price = 200.0\n",
    "test_predictions = np.random.normal(210, 15, (2000, 14))\n",
    "\n",
    "# Create minimal price history\n",
    "test_price_history = pd.DataFrame({\n",
    "    'date': pd.date_range('2022-01-01', periods=test_day + 1),\n",
    "    'price': np.linspace(195, test_price, test_day + 1)\n",
    "})\n",
    "\n",
    "# Initialize all strategies with realistic parameters\n",
    "all_strategies = [\n",
    "    ImmediateSaleStrategy(min_batch_size=5.0, sale_frequency_days=7),\n",
    "    EqualBatchStrategy(batch_size=0.25, frequency_days=30),\n",
    "    PriceThresholdStrategy(threshold_pct=0.05, batch_fraction=0.25, max_days_without_sale=60),\n",
    "    MovingAverageStrategy(ma_period=30, batch_fraction=0.25, max_days_without_sale=60),\n",
    "    ConsensusStrategy(consensus_threshold=0.70, min_return=0.03, evaluation_day=14),\n",
    "    ExpectedValueStrategy(\n",
    "        storage_cost_pct_per_day=0.025,\n",
    "        transaction_cost_pct=0.25,\n",
    "        min_ev_improvement=50,\n",
    "        baseline_batch=0.15,\n",
    "        baseline_frequency=10\n",
    "    ),\n",
    "    RiskAdjustedStrategy(min_return=0.05, max_uncertainty=0.08, \n",
    "                        consensus_threshold=0.65, evaluation_day=14),\n",
    "    PriceThresholdPredictive(threshold_pct=0.05, batch_fraction=0.25, max_days_without_sale=60),\n",
    "    MovingAveragePredictive(ma_period=30, batch_fraction=0.25, max_days_without_sale=60)\n",
    "]\n",
    "\n",
    "# Test harvest start functionality\n",
    "for strategy in all_strategies:\n",
    "    strategy.set_harvest_start(0)  # Simulate harvest starting at day 0\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TESTING ALL STRATEGIES - VERSION 3.0 (ENHANCED WITH INDICATORS)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTest scenario: Day {test_day}, Inventory {test_inventory}t, Price ${test_price}\")\n",
    "print(f\"Predictions: Mean=${np.mean(test_predictions[:, 13]):.2f}, Std=${np.std(test_predictions[:, 13]):.2f}\\n\")\n",
    "\n",
    "for strategy in all_strategies:\n",
    "    decision = strategy.decide(\n",
    "        day=test_day,\n",
    "        inventory=test_inventory,\n",
    "        current_price=test_price,\n",
    "        price_history=test_price_history,\n",
    "        predictions=test_predictions\n",
    "    )\n",
    "    \n",
    "    action = decision.get('action', 'N/A')\n",
    "    amount = decision.get('amount', 0)\n",
    "    reason = decision.get('reason', 'N/A')\n",
    "    \n",
    "    print(f\"{strategy.name:30s}: {action:4s} {amount:5.1f}t - {reason}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ All 9 strategies implemented and tested successfully!\")\n",
    "print(\"✓ CHANGES FROM ORIGINAL:\")\n",
    "print(\"  - PriceThreshold: Fixed to use 30-day MA (not reference price)\")\n",
    "print(\"  - MovingAverage: Cooldown 7 days (was 5)\")\n",
    "print(\"  - Consensus/ExpectedValue/RiskAdjusted: Daily evaluation (not scheduled)\")\n",
    "print(\"  - All baselines: Added RSI/ADX/Std_dev_historical\")\n",
    "print(\"  - All predictive: Added RSI/ADX/Std_dev_predictions + cost-benefit\")\n",
    "print(\"  - Matched pairs: Identical baseline logic, predictions add overlay only\")\n",
    "print(\"✓ ENHANCEMENTS:\")\n",
    "print(\"  - Technical indicators: RSI, ADX, Standard Deviation\")\n",
    "print(\"  - Cost-benefit analysis for all prediction strategies\")\n",
    "print(\"  - Dynamic batch sizing based on signals (20-40% range)\")\n",
    "print(\"  - Daily signal evaluation (not scheduled checks)\")\n",
    "print(f\"✓ Max holding period: {all_strategies[0].max_holding_days} days (365 days from harvest)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "099c052d-86cb-4b19-97e2-1a19c65b8b6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NOTEBOOK 03: BACKTESTING ENGINE (UPDATED)\n",
    "# ============================================================================\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Backtesting Engine - Updated for Harvest Cycles\n",
    "# MAGIC \n",
    "# MAGIC Now handles:\n",
    "# MAGIC - Gradual inventory accumulation during harvest windows\n",
    "# MAGIC - Multiple harvest cycles across simulation period\n",
    "# MAGIC - 365-day max holding from harvest window start\n",
    "# MAGIC - Force liquidation before new harvest begins\n",
    "# MAGIC - Percentage-based costs (storage and transaction scale with price)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./00_setup_and_config\n",
    "# MAGIC %run ./02_strategy_implementations\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "class BacktestEngine:\n",
    "    \"\"\"\n",
    "    Backtesting engine for commodity trading strategies.\n",
    "    Tracks net earnings from selling commodity inventory with harvest cycle awareness.\n",
    "    Uses percentage-based costs that scale with commodity price.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, prices, prediction_matrices, producer_config):\n",
    "        \"\"\"\n",
    "        Initialize backtest engine with harvest schedule.\n",
    "        \n",
    "        Args:\n",
    "            prices: DataFrame with columns ['date', 'price']\n",
    "            prediction_matrices: dict mapping dates to N×H prediction arrays\n",
    "            producer_config: dict with commodity configuration including harvest_windows\n",
    "        \"\"\"\n",
    "        self.prices = prices.copy().sort_values('date').reset_index(drop=True)\n",
    "        self.prediction_matrices = prediction_matrices\n",
    "        self.config = producer_config\n",
    "        \n",
    "        # Create harvest schedule from config\n",
    "        self.harvest_schedule = self._create_harvest_schedule()\n",
    "        \n",
    "        # Detect prediction structure\n",
    "        if len(prediction_matrices) > 0:\n",
    "            sample_matrix = list(prediction_matrices.values())[0]\n",
    "            self.n_runs = sample_matrix.shape[0]\n",
    "            self.n_horizons = sample_matrix.shape[1]\n",
    "            print(f\"Backtest engine initialized:\")\n",
    "            print(f\"  Commodity: {self.config['commodity']}\")\n",
    "            print(f\"  Price days: {len(self.prices)}\")\n",
    "            print(f\"  Prediction matrices: {len(self.prediction_matrices)}\")\n",
    "            print(f\"  Matrix structure: {self.n_runs} runs × {self.n_horizons} horizons\")\n",
    "            print(f\"  Harvest windows: {self.config['harvest_windows']}\")\n",
    "            print(f\"  Annual harvest volume: {self.config['harvest_volume']} tons\")\n",
    "            print(f\"  Storage cost: {self.config.get('storage_cost_pct_per_day', 'N/A')}% per day\")\n",
    "            print(f\"  Transaction cost: {self.config.get('transaction_cost_pct', 'N/A')}% per sale\")\n",
    "            \n",
    "            # Report harvest schedule stats\n",
    "            harvest_days = sum(1 for d in self.harvest_schedule.values() if d['is_harvest_day'])\n",
    "            harvest_starts = sum(1 for d in self.harvest_schedule.values() if d['is_harvest_window_start'])\n",
    "            print(f\"  Harvest days in simulation: {harvest_days}\")\n",
    "            print(f\"  Harvest cycles in simulation: {harvest_starts}\")\n",
    "    \n",
    "    def _create_harvest_schedule(self):\n",
    "        \"\"\"\n",
    "        Create a harvest schedule showing when inventory accumulates.\n",
    "        Returns dict mapping date -> harvest info\n",
    "        \"\"\"\n",
    "        harvest_schedule = {}\n",
    "        \n",
    "        # Get harvest windows from config (list of (start_month, end_month) tuples)\n",
    "        harvest_windows = self.config['harvest_windows']\n",
    "        annual_volume = self.config['harvest_volume']\n",
    "        \n",
    "        # For each date in prices, determine if it's in a harvest window\n",
    "        for idx, row in self.prices.iterrows():\n",
    "            date = row['date']\n",
    "            month = date.month\n",
    "            year = date.year\n",
    "            \n",
    "            # Check if this date falls in any harvest window\n",
    "            is_harvest = False\n",
    "            for start_month, end_month in harvest_windows:\n",
    "                if start_month <= end_month:\n",
    "                    # Window within same year (e.g., May-September)\n",
    "                    if start_month <= month <= end_month:\n",
    "                        is_harvest = True\n",
    "                        break\n",
    "                else:\n",
    "                    # Window crosses year boundary (e.g., October-December wraps to Jan-Feb)\n",
    "                    if month >= start_month or month <= end_month:\n",
    "                        is_harvest = True\n",
    "                        break\n",
    "            \n",
    "            if is_harvest:\n",
    "                # Calculate which harvest year this belongs to\n",
    "                # For simplicity, use year if window is in same calendar year,\n",
    "                # otherwise use year of the start month\n",
    "                for start_month, end_month in harvest_windows:\n",
    "                    if start_month <= end_month:\n",
    "                        if start_month <= month <= end_month:\n",
    "                            harvest_year = year\n",
    "                            break\n",
    "                    else:\n",
    "                        if month >= start_month:\n",
    "                            harvest_year = year\n",
    "                        else:\n",
    "                            harvest_year = year - 1\n",
    "                        break\n",
    "                \n",
    "                # Determine if this is the first day of the harvest window\n",
    "                is_window_start = False\n",
    "                if idx > 0:\n",
    "                    prev_date = self.prices.loc[idx - 1, 'date']\n",
    "                    prev_month = prev_date.month\n",
    "                    prev_was_harvest = False\n",
    "                    for start_month, end_month in harvest_windows:\n",
    "                        if start_month <= end_month:\n",
    "                            if start_month <= prev_month <= end_month:\n",
    "                                prev_was_harvest = True\n",
    "                                break\n",
    "                        else:\n",
    "                            if prev_month >= start_month or prev_month <= end_month:\n",
    "                                prev_was_harvest = True\n",
    "                                break\n",
    "                    # Window starts if previous day wasn't harvest but today is\n",
    "                    is_window_start = not prev_was_harvest\n",
    "                else:\n",
    "                    # First day of simulation - check if it's start of window\n",
    "                    is_window_start = (month == harvest_windows[0][0])\n",
    "                \n",
    "                # Calculate daily increment\n",
    "                # Distribute annual volume evenly across all harvest days in the year\n",
    "                # Count how many harvest days there are in a typical year\n",
    "                days_in_window = 0\n",
    "                for start_month, end_month in harvest_windows:\n",
    "                    if start_month <= end_month:\n",
    "                        # Simple case: May (5) to Sep (9) = 5 months\n",
    "                        days_in_window += (end_month - start_month + 1) * 30  # Approximate\n",
    "                    else:\n",
    "                        # Cross-year case: Oct (10) to Feb (2) = Oct-Dec + Jan-Feb\n",
    "                        days_in_window += (12 - start_month + 1 + end_month) * 30\n",
    "                \n",
    "                # Daily increment = annual volume / number of days in harvest\n",
    "                daily_increment = annual_volume / days_in_window if days_in_window > 0 else 0\n",
    "                \n",
    "                harvest_schedule[date] = {\n",
    "                    'is_harvest_day': True,\n",
    "                    'is_harvest_window_start': is_window_start,\n",
    "                    'daily_increment': daily_increment,\n",
    "                    'harvest_year': harvest_year\n",
    "                }\n",
    "            else:\n",
    "                harvest_schedule[date] = {\n",
    "                    'is_harvest_day': False,\n",
    "                    'is_harvest_window_start': False,\n",
    "                    'daily_increment': 0.0,\n",
    "                    'harvest_year': None\n",
    "                }\n",
    "        \n",
    "        return harvest_schedule\n",
    "    \n",
    "    def run(self, strategy):\n",
    "        \"\"\"Run backtest for a strategy with harvest cycle tracking\"\"\"\n",
    "        strategy.reset()\n",
    "        \n",
    "        # Start with ZERO inventory - will accumulate during harvest\n",
    "        self.inventory = 0\n",
    "        self.trades = []\n",
    "        self.daily_state = []\n",
    "        self.total_storage_costs = 0\n",
    "        \n",
    "        for idx in range(len(self.prices)):\n",
    "            current_date = self.prices.loc[idx, 'date']\n",
    "            current_price = self.prices.loc[idx, 'price']\n",
    "            \n",
    "            # Get harvest schedule for this date\n",
    "            schedule = self.harvest_schedule.get(current_date, {\n",
    "                'is_harvest_window_start': False,\n",
    "                'is_harvest_day': False,\n",
    "                'daily_increment': 0.0,\n",
    "                'harvest_year': None\n",
    "            })\n",
    "            \n",
    "            # CRITICAL: Before new harvest starts, force-liquidate old inventory\n",
    "            if schedule['is_harvest_window_start'] and self.inventory > 0:\n",
    "                liquidation = strategy.force_liquidate_before_new_harvest(self.inventory)\n",
    "                if liquidation and liquidation['action'] == 'SELL':\n",
    "                    amount = liquidation['amount']\n",
    "                    self._execute_trade(idx, current_date, current_price, amount, \n",
    "                                      liquidation['reason'])\n",
    "            \n",
    "            # Check if harvest window is starting (reset 365-day clock)\n",
    "            if schedule['is_harvest_window_start']:\n",
    "                strategy.set_harvest_start(idx)\n",
    "            \n",
    "            # Add daily harvest increment if in harvest window\n",
    "            if schedule['is_harvest_day']:\n",
    "                daily_increment = schedule['daily_increment']\n",
    "                self.inventory += daily_increment\n",
    "            else:\n",
    "                daily_increment = 0.0\n",
    "            \n",
    "            # Get predictions and price history\n",
    "            predictions = self.prediction_matrices.get(current_date, None)\n",
    "            price_history = self.prices.loc[:idx]\n",
    "            \n",
    "            # Get strategy decision\n",
    "            decision = strategy.decide(idx, self.inventory, current_price, price_history, predictions)\n",
    "            \n",
    "            # Execute trades\n",
    "            if decision['action'] == 'SELL' and decision.get('amount', 0) > 0:\n",
    "                amount = min(decision['amount'], self.inventory)\n",
    "                if amount >= self.config.get('min_inventory_to_trade', 0):\n",
    "                    self._execute_trade(idx, current_date, current_price, amount, decision.get('reason', ''))\n",
    "            \n",
    "            # Accumulate storage costs for remaining inventory (percentage-based)\n",
    "            # Storage cost = inventory × current_price × (storage_cost_pct_per_day / 100)\n",
    "            storage_cost_pct = self.config.get('storage_cost_pct_per_day', \n",
    "                                              self.config.get('storage_cost_per_ton_per_day', 0))\n",
    "            \n",
    "            # Convert price from cents/lb to $/ton (cents/lb × 20 = $/ton)\n",
    "            price_per_ton = current_price * 20\n",
    "            \n",
    "            # Check if using percentage-based or legacy fixed cost\n",
    "            if 'storage_cost_pct_per_day' in self.config:\n",
    "                daily_storage_cost = self.inventory * price_per_ton * (storage_cost_pct / 100)\n",
    "            else:\n",
    "                # Legacy: fixed dollar amount per ton per day\n",
    "                daily_storage_cost = self.inventory * storage_cost_pct\n",
    "            \n",
    "            self.total_storage_costs += daily_storage_cost\n",
    "            \n",
    "            # Track daily state (including harvest activity)\n",
    "            self.daily_state.append({\n",
    "                'date': current_date,\n",
    "                'day': idx,\n",
    "                'price': current_price,\n",
    "                'inventory': self.inventory,\n",
    "                'harvest_added': daily_increment,  # Track daily harvest additions\n",
    "                'is_harvest_window': schedule['is_harvest_day'],  # Track harvest status\n",
    "                'harvest_year': schedule.get('harvest_year'),  # Track which harvest year\n",
    "                'daily_storage_cost': daily_storage_cost,\n",
    "                'cumulative_storage_cost': self.total_storage_costs\n",
    "            })\n",
    "        \n",
    "        # Force liquidation at end if inventory remains\n",
    "        if self.inventory > 0:\n",
    "            final_date = self.prices.iloc[-1]['date']\n",
    "            final_price = self.prices.iloc[-1]['price']\n",
    "            self._execute_trade(len(self.prices)-1, final_date, final_price, \n",
    "                              self.inventory, 'end_of_simulation_forced_liquidation')\n",
    "        \n",
    "        # Calculate summary metrics\n",
    "        total_revenue = sum(t['revenue'] for t in self.trades)\n",
    "        total_transaction_costs = sum(t['transaction_cost'] for t in self.trades)\n",
    "        net_earnings = total_revenue - total_transaction_costs - self.total_storage_costs\n",
    "        \n",
    "        return {\n",
    "            'strategy_name': strategy.name,\n",
    "            'trades': self.trades,\n",
    "            'daily_state': pd.DataFrame(self.daily_state),\n",
    "            'total_revenue': total_revenue,\n",
    "            'total_transaction_costs': total_transaction_costs,\n",
    "            'total_storage_costs': self.total_storage_costs,\n",
    "            'net_earnings': net_earnings,\n",
    "            'harvest_schedule': self.harvest_schedule  # Include for analysis\n",
    "        }\n",
    "    \n",
    "    def _execute_trade(self, day, date, price, amount, reason):\n",
    "        \"\"\"Execute a trade and update state with percentage-based transaction costs\"\"\"\n",
    "        # Convert price from cents/lb to $/ton (cents/lb × 20 = $/ton)\n",
    "        price_per_ton = price * 20\n",
    "        \n",
    "        revenue = amount * price_per_ton\n",
    "        \n",
    "        # Transaction cost = amount × price × (transaction_cost_pct / 100)\n",
    "        transaction_cost_pct = self.config.get('transaction_cost_pct',\n",
    "                                              self.config.get('transaction_cost_per_ton', 0))\n",
    "        \n",
    "        # Check if using percentage-based or legacy fixed cost\n",
    "        if 'transaction_cost_pct' in self.config:\n",
    "            transaction_cost = amount * price_per_ton * (transaction_cost_pct / 100)\n",
    "        else:\n",
    "            # Legacy: fixed dollar amount per ton\n",
    "            transaction_cost = amount * transaction_cost_pct\n",
    "        \n",
    "        net_revenue = revenue - transaction_cost\n",
    "        \n",
    "        self.inventory -= amount\n",
    "        \n",
    "        self.trades.append({\n",
    "            'day': day,\n",
    "            'date': date,\n",
    "            'price': price,\n",
    "            'amount': amount,\n",
    "            'revenue': revenue,\n",
    "            'transaction_cost': transaction_cost,\n",
    "            'net_revenue': net_revenue,\n",
    "            'reason': reason\n",
    "        })\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def calculate_metrics(results):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive performance metrics including risk-adjusted measures.\n",
    "    \"\"\"\n",
    "    \n",
    "    trades = results['trades']\n",
    "    daily_state = results['daily_state']\n",
    "    \n",
    "    # Core financial metrics\n",
    "    total_revenue = results['total_revenue']\n",
    "    total_transaction_costs = results['total_transaction_costs']\n",
    "    total_storage_costs = results['total_storage_costs']\n",
    "    net_earnings = results['net_earnings']\n",
    "    \n",
    "    # Trading pattern metrics\n",
    "    n_trades = len(trades)\n",
    "    \n",
    "    if n_trades > 0:\n",
    "        total_volume = sum(t['amount'] for t in trades)\n",
    "        avg_sale_price = total_revenue / total_volume if total_volume > 0 else 0\n",
    "        \n",
    "        # Days to liquidate\n",
    "        first_trade_day = trades[0]['day']\n",
    "        last_trade_day = trades[-1]['day']\n",
    "        days_to_liquidate = last_trade_day - first_trade_day\n",
    "        \n",
    "        # Average days between trades\n",
    "        if n_trades > 1:\n",
    "            trade_days = [t['day'] for t in trades]\n",
    "            days_between_trades = np.mean(np.diff(trade_days))\n",
    "        else:\n",
    "            days_between_trades = 0\n",
    "        \n",
    "        # First and last sale prices\n",
    "        first_sale_price = trades[0]['price']\n",
    "        last_sale_price = trades[-1]['price']\n",
    "        \n",
    "    else:\n",
    "        avg_sale_price = 0\n",
    "        days_to_liquidate = 0\n",
    "        days_between_trades = 0\n",
    "        first_sale_price = 0\n",
    "        last_sale_price = 0\n",
    "    \n",
    "    # Calculate portfolio values over time for risk metrics\n",
    "    trades_by_day = {t['day']: t for t in trades}\n",
    "    accumulated_net_proceeds = 0\n",
    "    portfolio_values = []\n",
    "    \n",
    "    for idx, row in daily_state.iterrows():\n",
    "        day = row['day']\n",
    "        inventory = row['inventory']\n",
    "        price = row['price']\n",
    "        \n",
    "        # Add net proceeds from any sales today\n",
    "        if day in trades_by_day:\n",
    "            trade = trades_by_day[day]\n",
    "            accumulated_net_proceeds += (trade['revenue'] - trade['transaction_cost'])\n",
    "        \n",
    "        # Subtract daily storage costs\n",
    "        accumulated_net_proceeds -= row['daily_storage_cost']\n",
    "        \n",
    "        # Portfolio = net proceeds + remaining inventory market value\n",
    "        inventory_value = inventory * price\n",
    "        portfolio_value = accumulated_net_proceeds + inventory_value\n",
    "        portfolio_values.append(portfolio_value)\n",
    "    \n",
    "    portfolio_values = np.array(portfolio_values)\n",
    "    \n",
    "    # Calculate risk-return metrics\n",
    "    if len(portfolio_values) > 1:\n",
    "        # Daily changes\n",
    "        daily_changes = np.diff(portfolio_values)\n",
    "        \n",
    "        # Total return (from first to last portfolio value)\n",
    "        initial_value = portfolio_values[0]\n",
    "        final_value = portfolio_values[-1]\n",
    "        total_return = (final_value - initial_value) / abs(initial_value) if initial_value != 0 else 0\n",
    "        \n",
    "        # Annualized return (assuming ~252 trading days per year)\n",
    "        n_days = len(portfolio_values)\n",
    "        annualized_return = (1 + total_return) ** (252 / n_days) - 1 if n_days > 0 else 0\n",
    "        \n",
    "        # Volatility (std of daily changes, annualized)\n",
    "        volatility = np.std(daily_changes) * np.sqrt(252)\n",
    "        \n",
    "        # Sharpe ratio (assuming risk-free rate = 0 for simplicity)\n",
    "        sharpe_ratio = annualized_return / volatility if volatility > 0 else 0\n",
    "    else:\n",
    "        total_return = 0\n",
    "        annualized_return = 0\n",
    "        volatility = 0\n",
    "        sharpe_ratio = 0\n",
    "    \n",
    "    return {\n",
    "        'strategy': results['strategy_name'],\n",
    "        'net_earnings': net_earnings,\n",
    "        'total_revenue': total_revenue,\n",
    "        'total_costs': total_transaction_costs + total_storage_costs,\n",
    "        'transaction_costs': total_transaction_costs,\n",
    "        'storage_costs': total_storage_costs,\n",
    "        'avg_sale_price': avg_sale_price,\n",
    "        'first_sale_price': first_sale_price,\n",
    "        'last_sale_price': last_sale_price,\n",
    "        'n_trades': n_trades,\n",
    "        'days_to_liquidate': days_to_liquidate,\n",
    "        'avg_days_between_trades': days_between_trades,\n",
    "        # Risk-return metrics\n",
    "        'total_return': total_return,\n",
    "        'annualized_return': annualized_return,\n",
    "        'volatility': volatility,\n",
    "        'sharpe_ratio': sharpe_ratio\n",
    "    }\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BACKTESTING ENGINE - MULTI-COMMODITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"✓ BacktestEngine class ready (with harvest cycle support)\")\n",
    "print(\"✓ calculate_metrics function ready\")\n",
    "print(\"\\nEngine features:\")\n",
    "print(\"  - Harvest-aware: tracks inventory accumulation during harvest windows\")\n",
    "print(\"  - Multi-cycle: handles multiple harvest seasons across simulation\")\n",
    "print(\"  - Age tracking: enforces 365-day max holding from harvest start\")\n",
    "print(\"  - Pre-harvest liquidation: forces sale of old inventory before new harvest\")\n",
    "print(\"  - Percentage-based costs: storage and transaction costs scale with price\")\n",
    "print(\"  - Data-driven: adapts to actual prediction matrix structure\")\n",
    "print(\"  - Tracks: net earnings, trades, daily inventory state, harvest events\")\n",
    "print(\"  - Handles: transaction costs, storage costs, forced liquidation\")\n",
    "print(\"\\nNEW: Inventory starts at zero and accumulates during harvest windows\")\n",
    "print(\"NEW: Each harvest window resets the 365-day holding period\")\n",
    "print(\"NEW: Old inventory is liquidated before new harvest begins\")\n",
    "print(\"NEW: Costs are percentage-based and scale automatically with commodity value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42411200-0eb4-4a18-b0f5-3caf694644f4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763067348448}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NOTEBOOK 04: RUN COMPARATIVE ANALYSIS (UPDATED - MULTI-MODEL)\n",
    "# ============================================================================\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Run Comparative Analysis - All Commodities and Model Versions\n",
    "# MAGIC \n",
    "# MAGIC Runs backtest analysis for all configured commodities and model versions sequentially.\n",
    "# MAGIC Generates commodity-specific results and cross-commodity comparisons.\n",
    "# MAGIC \n",
    "# MAGIC **Tests 9 Strategies:**\n",
    "# MAGIC - 4 Baselines: Immediate Sale, Equal Batch, Price Threshold, Moving Average\n",
    "# MAGIC - 5 Prediction-based: Consensus, Expected Value, Risk-Adjusted, Price Threshold Predictive, MA Predictive\n",
    "# MAGIC \n",
    "# MAGIC **Updates:**\n",
    "# MAGIC - Added multi-model support (loops over all model versions)\n",
    "# MAGIC - Tracks model_version in all results\n",
    "# MAGIC - Added Total Revenue (Without Costs) visualization\n",
    "# MAGIC - Enhanced scenario tracking for Risk-Adjusted strategy\n",
    "# MAGIC - Includes 2 NEW A/B test strategies (predictive versions of baseline strategies)\n",
    "# MAGIC - All existing functionality preserved\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./00_setup_and_config\n",
    "# MAGIC %run ./02_strategy_implementations\n",
    "# MAGIC %run ./03_backtesting_engine\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Run Analysis for All Commodities and Model Versions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Store results for all commodities and models\n",
    "all_commodity_results = {}\n",
    "\n",
    "# Loop through each commodity\n",
    "for CURRENT_COMMODITY in COMMODITY_CONFIGS.keys():\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"STARTING ANALYSIS FOR: {CURRENT_COMMODITY.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Get commodity-specific configuration\n",
    "    commodity_config = COMMODITY_CONFIGS[CURRENT_COMMODITY]\n",
    "    \n",
    "    # Initialize storage for this commodity\n",
    "    all_commodity_results[CURRENT_COMMODITY] = {}\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Discover all model versions for this commodity\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(f\"\\nDiscovering model versions...\")\n",
    "    \n",
    "    # Check synthetic predictions\n",
    "    synthetic_versions = []\n",
    "    try:\n",
    "        DATA_PATHS = get_data_paths(CURRENT_COMMODITY)\n",
    "        synthetic_df = spark.table(DATA_PATHS['predictions']).select(\"model_version\").distinct()\n",
    "        synthetic_versions = [row.model_version for row in synthetic_df.collect()]\n",
    "        print(f\"  Synthetic models: {synthetic_versions}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  No synthetic predictions found: {e}\")\n",
    "    \n",
    "    # Check real predictions\n",
    "    real_versions = []\n",
    "    try:\n",
    "        real_versions = get_model_versions(CURRENT_COMMODITY)\n",
    "        print(f\"  Real models: {real_versions}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  No real predictions found: {e}\")\n",
    "    \n",
    "    # Combine all model versions\n",
    "    all_model_versions = list(set(synthetic_versions + real_versions))\n",
    "    \n",
    "    if len(all_model_versions) == 0:\n",
    "        print(f\"\\n⚠️  No model versions found for {CURRENT_COMMODITY}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n✓ Found {len(all_model_versions)} model versions for {CURRENT_COMMODITY}\")\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Process each model version\n",
    "    # --------------------------------------------------------------------------\n",
    "    for model_idx, MODEL_VERSION in enumerate(all_model_versions, 1):\n",
    "        print(\"\\n\" + \"#\" * 80)\n",
    "        print(f\"# MODEL {model_idx}/{len(all_model_versions)}: {CURRENT_COMMODITY.upper()} - {MODEL_VERSION}\")\n",
    "        print(\"#\" * 80)\n",
    "        \n",
    "        MODEL_DATA_PATHS = get_data_paths(CURRENT_COMMODITY, MODEL_VERSION)\n",
    "        \n",
    "        # Determine source type\n",
    "        source_type = \"SYNTHETIC\" if MODEL_VERSION.startswith('synthetic_') else \"REAL\"\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Load prepared data for this model version\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nLoading prepared data for {CURRENT_COMMODITY.upper()} - {MODEL_VERSION}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load prices (same for all models of this commodity)\n",
    "            prices = spark.table(get_data_paths(CURRENT_COMMODITY)['prices_prepared']).toPandas()\n",
    "            prices['date'] = pd.to_datetime(prices['date'])\n",
    "            \n",
    "            print(f\"✓ Loaded {len(prices)} days of prices\")\n",
    "            \n",
    "            # Load prediction matrices for this model version\n",
    "            # Determine path based on model version type\n",
    "            if MODEL_VERSION.startswith('synthetic_'):\n",
    "                matrices_path = MODEL_DATA_PATHS['prediction_matrices']\n",
    "            else:\n",
    "                matrices_path = MODEL_DATA_PATHS['prediction_matrices_real']\n",
    "            \n",
    "            with open(matrices_path, 'rb') as f:\n",
    "                prediction_matrices = pickle.load(f)\n",
    "            \n",
    "            print(f\"✓ Loaded {len(prediction_matrices)} prediction matrices ({source_type})\")\n",
    "            \n",
    "            # Inspect prediction structure\n",
    "            if len(prediction_matrices) > 0:\n",
    "                sample_matrix = list(prediction_matrices.values())[0]\n",
    "                print(f\"✓ Prediction structure: {sample_matrix.shape[0]} runs × {sample_matrix.shape[1]} horizons\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading data for {MODEL_VERSION}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Date alignment diagnostic (optional - can comment out after verification)\n",
    "        # ----------------------------------------------------------------------\n",
    "        if False:  # Set to True to enable diagnostic\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"DATE ALIGNMENT DIAGNOSTIC\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            pred_keys = list(prediction_matrices.keys())\n",
    "            price_dates = prices['date'].tolist()\n",
    "            pred_set = set(pred_keys)\n",
    "            price_set = set(price_dates)\n",
    "            overlap = pred_set.intersection(price_set)\n",
    "            \n",
    "            print(f\"\\nOverlap Analysis:\")\n",
    "            print(f\"  Total prediction keys: {len(pred_set)}\")\n",
    "            print(f\"  Total price dates: {len(price_set)}\")\n",
    "            print(f\"  Matching dates: {len(overlap)}\")\n",
    "            print(f\"  Match rate: {100*len(overlap)/len(pred_set) if len(pred_set) > 0 else 0:.1f}%\")\n",
    "            \n",
    "            if len(overlap) == 0:\n",
    "                print(\"\\n❌ WARNING: NO MATCHING DATES!\")\n",
    "            else:\n",
    "                print(\"\\n✓ SUCCESS: Dates are aligned!\")\n",
    "            \n",
    "            print(\"=\" * 80)\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Initialize strategies\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nInitializing strategies...\")\n",
    "        \n",
    "        baselines = [\n",
    "            ImmediateSaleStrategy(),\n",
    "            EqualBatchStrategy(**BASELINE_PARAMS['equal_batch']),\n",
    "            PriceThresholdStrategy(\n",
    "                threshold_pct=BASELINE_PARAMS['price_threshold']['threshold_pct']\n",
    "            ),\n",
    "            MovingAverageStrategy(\n",
    "                ma_period=BASELINE_PARAMS['moving_average']['ma_period']\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        prediction_strategies = [\n",
    "            ConsensusStrategy(**PREDICTION_PARAMS['consensus']),\n",
    "            ExpectedValueStrategy(\n",
    "                storage_cost_pct_per_day=commodity_config['storage_cost_pct_per_day'],\n",
    "                transaction_cost_pct=commodity_config['transaction_cost_pct'],\n",
    "                **PREDICTION_PARAMS['expected_value']\n",
    "            ),\n",
    "            RiskAdjustedStrategy(**PREDICTION_PARAMS['risk_adjusted']),\n",
    "            # NEW A/B Test Strategies\n",
    "            PriceThresholdPredictive(\n",
    "                threshold_pct=BASELINE_PARAMS['price_threshold']['threshold_pct'],\n",
    "                batch_fraction=0.25,\n",
    "                max_days_without_sale=60\n",
    "            ),\n",
    "            MovingAveragePredictive(\n",
    "                ma_period=BASELINE_PARAMS['moving_average']['ma_period'],\n",
    "                batch_fraction=0.25,\n",
    "                max_days_without_sale=60\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        all_strategies = baselines + prediction_strategies\n",
    "        \n",
    "        print(f\"✓ {len(baselines)} baseline strategies\")\n",
    "        print(f\"✓ {len(prediction_strategies)} prediction-based strategies (3 enhanced + 2 A/B test)\")\n",
    "        print(f\"Total: {len(all_strategies)} strategies to test\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Run backtests\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"RUNNING BACKTESTS - {CURRENT_COMMODITY.upper()} - {MODEL_VERSION}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        engine = BacktestEngine(prices, prediction_matrices, commodity_config)\n",
    "        \n",
    "        results_dict = {}\n",
    "        metrics_list = []\n",
    "        \n",
    "        for i, strategy in enumerate(all_strategies, 1):\n",
    "            print(f\"\\n[{i}/{len(all_strategies)}] Running: {strategy.name}...\")\n",
    "            results = engine.run(strategy)\n",
    "            metrics = calculate_metrics(results)\n",
    "            results_dict[strategy.name] = results\n",
    "            metrics_list.append(metrics)\n",
    "            \n",
    "            print(f\"  Total Revenue:  ${metrics['total_revenue']:,.2f}\")\n",
    "            print(f\"  Net Earnings:   ${metrics['net_earnings']:,.2f}\")\n",
    "            print(f\"  Avg Sale Price: ${metrics['avg_sale_price']:.2f}\")\n",
    "            print(f\"  Total Costs:    ${metrics['total_costs']:,.2f}\") \n",
    "            print(f\"  Trades:         {metrics['n_trades']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"BACKTESTS COMPLETE\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Analyze Risk-Adjusted Strategy Scenarios\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(\"\\nAnalyzing Risk-Adjusted Strategy Scenarios...\")\n",
    "        \n",
    "        if 'Risk-Adjusted' in results_dict:\n",
    "            risk_adj_results = results_dict['Risk-Adjusted']\n",
    "            trades = risk_adj_results['trades']\n",
    "            \n",
    "            # DEBUG: Print actual trade reasons\n",
    "            print(\"\\nDEBUG - All Risk-Adjusted Trade Reasons:\")\n",
    "            reason_counts = {}\n",
    "            for trade in trades:\n",
    "                reason = trade.get('reason', 'UNKNOWN')\n",
    "                reason_counts[reason] = reason_counts.get(reason, 0) + 1\n",
    "            \n",
    "            for reason, count in sorted(reason_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"  {count:2d}× {reason}\")\n",
    "            print()\n",
    "            \n",
    "            # Extract scenario counts from trade reasons\n",
    "            scenario_counts = {\n",
    "                'very_low_risk_8pct': 0,\n",
    "                'low_risk_12pct': 0,\n",
    "                'medium_risk_18pct': 0,\n",
    "                'med_high_risk_25pct': 0,\n",
    "                'high_risk_30pct': 0,\n",
    "                'very_high_risk_40pct': 0,\n",
    "                'other': 0\n",
    "            }\n",
    "            \n",
    "            for trade in trades:\n",
    "                reason = trade.get('reason', '')\n",
    "                if 'very_low_risk' in reason or ('cv' in reason and 'defer' in reason):\n",
    "                    scenario_counts['very_low_risk_8pct'] += 1\n",
    "                elif 'low_risk' in reason and 'ret' in reason:\n",
    "                    scenario_counts['low_risk_12pct'] += 1\n",
    "                elif 'medium_risk' in reason and 'baseline' in reason:\n",
    "                    scenario_counts['medium_risk_18pct'] += 1\n",
    "                elif 'med_high_risk' in reason or 'weak_trend' in reason:\n",
    "                    scenario_counts['med_high_risk_25pct'] += 1\n",
    "                elif 'high_risk' in reason and 'reduce_exposure' in reason:\n",
    "                    scenario_counts['high_risk_30pct'] += 1\n",
    "                elif 'very_high_risk' in reason or 'exit_fast' in reason:\n",
    "                    scenario_counts['very_high_risk_40pct'] += 1\n",
    "                else:\n",
    "                    scenario_counts['other'] += 1\n",
    "            \n",
    "            print(\"Risk-Adjusted Strategy - Scenario Distribution:\")\n",
    "            print(f\"  Very Low Risk (8% batch):       {scenario_counts['very_low_risk_8pct']} trades\")\n",
    "            print(f\"  Low Risk (12% batch):           {scenario_counts['low_risk_12pct']} trades\")\n",
    "            print(f\"  Medium Risk (18% batch):        {scenario_counts['medium_risk_18pct']} trades\")\n",
    "            print(f\"  Med-High Risk (25% batch):      {scenario_counts['med_high_risk_25pct']} trades\")\n",
    "            print(f\"  High Risk (30% batch):          {scenario_counts['high_risk_30pct']} trades\")\n",
    "            print(f\"  Very High Risk (40% batch):     {scenario_counts['very_high_risk_40pct']} trades\")\n",
    "            print(f\"  Other (deadline/fallback):      {scenario_counts['other']} trades\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Create results table\n",
    "        # ----------------------------------------------------------------------\n",
    "        results_df = pd.DataFrame(metrics_list)\n",
    "        baseline_names = [s.name for s in baselines]\n",
    "        results_df['type'] = results_df['strategy'].apply(\n",
    "            lambda x: 'Baseline' if x in baseline_names else 'Prediction'\n",
    "        )\n",
    "        results_df['commodity'] = CURRENT_COMMODITY\n",
    "        results_df['model_version'] = MODEL_VERSION\n",
    "        \n",
    "        # Sort by net earnings\n",
    "        results_df = results_df.sort_values('net_earnings', ascending=False)\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Identify best performers\n",
    "        # ----------------------------------------------------------------------\n",
    "        best_baseline = results_df[results_df['type'] == 'Baseline'].iloc[0]\n",
    "        best_prediction = results_df[results_df['type'] == 'Prediction'].iloc[0]\n",
    "        best_overall = results_df.iloc[0]\n",
    "        \n",
    "        # Calculate advantage\n",
    "        earnings_diff = best_prediction['net_earnings'] - best_baseline['net_earnings']\n",
    "        pct_diff = (earnings_diff / abs(best_baseline['net_earnings'])) * 100 if best_baseline['net_earnings'] != 0 else 0\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"RESULTS SUMMARY - {CURRENT_COMMODITY.upper()} - {MODEL_VERSION}\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\n🏆 Best Overall: {best_overall['strategy']}\")\n",
    "        print(f\"   Net Earnings: ${best_overall['net_earnings']:,.2f}\")\n",
    "        print(f\"\\n📊 Best Baseline: {best_baseline['strategy']}\")\n",
    "        print(f\"   Net Earnings: ${best_baseline['net_earnings']:,.2f}\")\n",
    "        print(f\"\\n🎯 Best Prediction: {best_prediction['strategy']}\")\n",
    "        print(f\"   Net Earnings: ${best_prediction['net_earnings']:,.2f}\")\n",
    "        print(f\"\\n📈 Prediction Advantage: ${earnings_diff:+,.2f} ({pct_diff:+.1f}%)\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Save results\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(\"\\nSaving results...\")\n",
    "        \n",
    "        # Save to Delta\n",
    "        spark.createDataFrame(results_df).write.format(\"delta\").mode(\"overwrite\") \\\n",
    "            .saveAsTable(MODEL_DATA_PATHS['results'])\n",
    "        print(f\"  ✓ Saved to Delta: {MODEL_DATA_PATHS['results']}\")\n",
    "        \n",
    "        # Save detailed results\n",
    "        with open(MODEL_DATA_PATHS['results_detailed'], 'wb') as f:\n",
    "            pickle.dump(results_dict, f)\n",
    "        print(f\"  ✓ Saved: {MODEL_DATA_PATHS['results_detailed']}\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Generate visualizations\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(\"\\nGenerating visualizations...\")\n",
    "        \n",
    "        # Net earnings chart\n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "        baseline_data = results_df[results_df['type'] == 'Baseline'].sort_values('net_earnings', ascending=True)\n",
    "        prediction_data = results_df[results_df['type'] == 'Prediction'].sort_values('net_earnings', ascending=True)\n",
    "        \n",
    "        y_baseline = range(len(baseline_data))\n",
    "        y_prediction = range(len(baseline_data), len(baseline_data) + len(prediction_data))\n",
    "        \n",
    "        ax.barh(y_baseline, baseline_data['net_earnings'], color='steelblue', alpha=0.7, label='Baseline')\n",
    "        ax.barh(y_prediction, prediction_data['net_earnings'], color='orangered', alpha=0.7, label='Prediction')\n",
    "        \n",
    "        all_strategies_sorted = pd.concat([baseline_data, prediction_data])\n",
    "        ax.set_yticks(range(len(all_strategies_sorted)))\n",
    "        ax.set_yticklabels(all_strategies_sorted['strategy'])\n",
    "        ax.set_xlabel('Net Earnings ($)', fontsize=12)\n",
    "        ax.set_title(f'Net Earnings by Strategy - {CURRENT_COMMODITY.upper()} - {MODEL_VERSION}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        for i, (idx, row) in enumerate(all_strategies_sorted.iterrows()):\n",
    "            ax.text(row['net_earnings'], i, f\"  ${row['net_earnings']:,.0f}\", va='center', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        earnings_chart_path = f\"{VOLUME_PATH}/net_earnings_{CURRENT_COMMODITY}_{MODEL_VERSION}.png\"\n",
    "        plt.savefig(earnings_chart_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"  ✓ Saved: {earnings_chart_path}\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Generate Trading Timeline Visualization\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(\"\\nGenerating trading timeline visualization...\")\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(16, 8))\n",
    "        \n",
    "        # Plot price history as background\n",
    "        ax.plot(prices['date'], prices['price'], color='gray', linewidth=1.5,\n",
    "                alpha=0.5, label='Price History', zorder=1)\n",
    "        \n",
    "        # Color map for strategies\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(all_strategies)))\n",
    "        strategy_colors = dict(zip([s.name for s in all_strategies], colors))\n",
    "        \n",
    "        # Plot trades for each strategy\n",
    "        for name, results in results_dict.items():\n",
    "            trades = results['trades']\n",
    "            if len(trades) > 0:\n",
    "                trade_dates = [t['date'] for t in trades]\n",
    "                trade_prices = [t['price'] for t in trades]\n",
    "                trade_amounts = [t['amount'] for t in trades]\n",
    "                \n",
    "                # Marker size proportional to trade amount\n",
    "                sizes = [amt * 10 for amt in trade_amounts]\n",
    "                \n",
    "                ax.scatter(trade_dates, trade_prices, s=sizes, alpha=0.6,\n",
    "                          color=strategy_colors[name], label=name,\n",
    "                          edgecolors='black', linewidth=0.5, zorder=2)\n",
    "        \n",
    "        ax.set_xlabel('Date', fontsize=12)\n",
    "        ax.set_ylabel('Price ($/ton)', fontsize=12)\n",
    "        ax.set_title(f'Trading Timeline - {CURRENT_COMMODITY.upper()} - {MODEL_VERSION}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        timeline_path = f\"{VOLUME_PATH}/trading_timeline_{CURRENT_COMMODITY}_{MODEL_VERSION}.png\"\n",
    "        plt.savefig(timeline_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"  ✓ Saved: {timeline_path}\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Total Revenue Without Costs Visualization\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nGenerating total revenue (without costs) chart...\")\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(16, 8))\n",
    "        \n",
    "        # Calculate and plot cumulative revenue for each strategy (no costs deducted)\n",
    "        for name, results in results_dict.items():\n",
    "            daily_state = results['daily_state']\n",
    "            trades_by_day = {t['day']: t for t in results['trades']}\n",
    "            \n",
    "            # Build up cumulative revenue day by day\n",
    "            cumulative_revenue = []\n",
    "            dates = []\n",
    "            running_revenue = 0\n",
    "            \n",
    "            for idx, row in daily_state.iterrows():\n",
    "                day = row['day']\n",
    "                date = row['date']\n",
    "                \n",
    "                # Add any sales revenue from today (no costs subtracted)\n",
    "                if day in trades_by_day:\n",
    "                    trade = trades_by_day[day]\n",
    "                    running_revenue += trade['revenue']\n",
    "                \n",
    "                cumulative_revenue.append(running_revenue)\n",
    "                dates.append(date)\n",
    "            \n",
    "            # Plot full time series\n",
    "            is_pred = name not in [s.name for s in baselines]\n",
    "            ax.plot(dates, cumulative_revenue, label=name,\n",
    "                   linestyle='-' if is_pred else '--',\n",
    "                   linewidth=2 if is_pred else 1.5,\n",
    "                   alpha=0.9 if is_pred else 0.6)\n",
    "        \n",
    "        ax.set_xlabel('Date', fontsize=12)\n",
    "        ax.set_ylabel('Cumulative Total Revenue ($)', fontsize=12)\n",
    "        ax.set_title(f'Cumulative Total Revenue Over Time (Without Costs) - {CURRENT_COMMODITY.upper()} - {MODEL_VERSION}',\n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "        \n",
    "        # Format x-axis to show dates nicely\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        revenue_only_path = f'{VOLUME_PATH}/total_revenue_no_costs_{CURRENT_COMMODITY}_{MODEL_VERSION}.png'\n",
    "        plt.savefig(revenue_only_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"  ✓ Saved: {revenue_only_path}\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Cumulative Net Revenue Visualization\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nGenerating cumulative net revenue chart...\")\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(16, 8))\n",
    "        \n",
    "        # Calculate and plot cumulative net revenue for each strategy\n",
    "        for name, results in results_dict.items():\n",
    "            daily_state = results['daily_state']\n",
    "            trades_by_day = {t['day']: t for t in results['trades']}\n",
    "            \n",
    "            # Build up cumulative values day by day\n",
    "            cumulative_net_revenue = []\n",
    "            dates = []\n",
    "            running_revenue = 0\n",
    "            running_transaction_costs = 0\n",
    "            running_storage_costs = 0\n",
    "            \n",
    "            for idx, row in daily_state.iterrows():\n",
    "                day = row['day']\n",
    "                date = row['date']\n",
    "                \n",
    "                # Add any sales revenue/costs from today\n",
    "                if day in trades_by_day:\n",
    "                    trade = trades_by_day[day]\n",
    "                    running_revenue += trade['revenue']\n",
    "                    running_transaction_costs += trade['transaction_cost']\n",
    "                \n",
    "                # Add today's storage cost\n",
    "                running_storage_costs += row['daily_storage_cost']\n",
    "                \n",
    "                # Net earnings to date\n",
    "                net_revenue = running_revenue - running_transaction_costs - running_storage_costs\n",
    "                cumulative_net_revenue.append(net_revenue)\n",
    "                dates.append(date)\n",
    "            \n",
    "            # Plot full time series\n",
    "            is_pred = name not in [s.name for s in baselines]\n",
    "            ax.plot(dates, cumulative_net_revenue, label=name,\n",
    "                   linestyle='-' if is_pred else '--',\n",
    "                   linewidth=2 if is_pred else 1.5,\n",
    "                   alpha=0.9 if is_pred else 0.6)\n",
    "        \n",
    "        ax.set_xlabel('Date', fontsize=12)\n",
    "        ax.set_ylabel('Cumulative Net Revenue ($)', fontsize=12)\n",
    "        ax.set_title(f'Cumulative Net Revenue Over Time - {CURRENT_COMMODITY.upper()} - {MODEL_VERSION}',\n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "        \n",
    "        # Format x-axis\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        cumulative_path = f'{VOLUME_PATH}/cumulative_returns_{CURRENT_COMMODITY}_{MODEL_VERSION}.png'\n",
    "        plt.savefig(cumulative_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"  ✓ Saved: {cumulative_path}\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Inventory Drawdown Visualization\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nGenerating inventory drawdown chart...\")\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(16, 8))\n",
    "        \n",
    "        # Plot inventory levels over time for each strategy\n",
    "        for name, results in results_dict.items():\n",
    "            daily_state = results['daily_state']\n",
    "            \n",
    "            dates = daily_state['date'].tolist()\n",
    "            inventory = daily_state['inventory'].tolist()\n",
    "            \n",
    "            # Plot\n",
    "            is_pred = name not in [s.name for s in baselines]\n",
    "            ax.plot(dates, inventory, label=name,\n",
    "                   linestyle='-' if is_pred else '--',\n",
    "                   linewidth=2 if is_pred else 1.5,\n",
    "                   alpha=0.9 if is_pred else 0.6)\n",
    "        \n",
    "        ax.set_xlabel('Date', fontsize=12)\n",
    "        ax.set_ylabel('Inventory (tons)', fontsize=12)\n",
    "        ax.set_title(f'Inventory Drawdown Over Time - {CURRENT_COMMODITY.upper()} - {MODEL_VERSION}',\n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "        \n",
    "        # Format x-axis\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        drawdown_path = f'{VOLUME_PATH}/inventory_drawdown_{CURRENT_COMMODITY}_{MODEL_VERSION}.png'\n",
    "        plt.savefig(drawdown_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"  ✓ Saved: {drawdown_path}\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Forced Liquidation Analysis\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nAnalyzing forced liquidations...\")\n",
    "        \n",
    "        if 'Risk-Adjusted' in results_dict:\n",
    "            forced = [t for t in results_dict['Risk-Adjusted']['trades']\n",
    "                     if 'liquidate' in t['reason'].lower() or 'forced' in t['reason'].lower()]\n",
    "            \n",
    "            if len(forced) > 0:\n",
    "                print(f\"\\nForced liquidation events for Risk-Adjusted strategy:\")\n",
    "                for t in forced:\n",
    "                    print(f\"  {str(t['date'])[:10]}: {t['amount']:6.2f} tons - {t['reason']}\")\n",
    "                \n",
    "                total = sum(t['amount'] for t in forced)\n",
    "                print(f\"\\nTotal: {total:.2f} tons across {len(forced)} events\")\n",
    "                print(f\"Average: {total/len(forced):.2f} tons per liquidation\")\n",
    "                \n",
    "                # Calculate % of total harvest\n",
    "                total_harvest = commodity_config['harvest_volume']\n",
    "                pct_liquidated = (total / total_harvest) * 100\n",
    "                print(f\"% of total harvest: {pct_liquidated:.1f}%\")\n",
    "            else:\n",
    "                print(\"No forced liquidations detected\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Store results for cross-model/commodity comparison\n",
    "        # ----------------------------------------------------------------------\n",
    "        all_commodity_results[CURRENT_COMMODITY][MODEL_VERSION] = {\n",
    "            'commodity': CURRENT_COMMODITY,\n",
    "            'model_version': MODEL_VERSION,\n",
    "            'results_df': results_df,\n",
    "            'results_dict': results_dict,\n",
    "            'best_baseline': best_baseline,\n",
    "            'best_prediction': best_prediction,\n",
    "            'best_overall': best_overall,\n",
    "            'earnings_diff': earnings_diff,\n",
    "            'pct_diff': pct_diff,\n",
    "            'prices': prices,\n",
    "            'config': commodity_config\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n✓ Analysis complete for {CURRENT_COMMODITY.upper()} - {MODEL_VERSION}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Cross-Model and Cross-Commodity Comparison\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL COMMODITY ANALYSES COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CROSS-MODEL AND CROSS-COMMODITY COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_data = []\n",
    "for commodity_name, model_data in all_commodity_results.items():\n",
    "    for model_version, results in model_data.items():\n",
    "        # Get the full results dataframe with all strategies\n",
    "        results_df = results['results_df'].copy()\n",
    "        \n",
    "        # Add commodity and model info to each row\n",
    "        results_df['Commodity'] = commodity_name.upper()\n",
    "        results_df['Model Version'] = model_version\n",
    "        \n",
    "        comparison_data.append(results_df)\n",
    "\n",
    "# Concatenate all results\n",
    "comparison_df = pd.concat(comparison_data, ignore_index=True)\n",
    "\n",
    "# Reorder columns for better readability\n",
    "column_order = ['Commodity', 'Model Version', 'strategy', 'type', 'net_earnings', \n",
    "                'total_revenue', 'total_costs', 'avg_sale_price', 'n_trades']\n",
    "# Use only columns that exist\n",
    "existing_cols = [col for col in column_order if col in comparison_df.columns]\n",
    "other_cols = [col for col in comparison_df.columns if col not in column_order]\n",
    "comparison_df = comparison_df[existing_cols + other_cols]\n",
    "\n",
    "# Sort by commodity, model, then net earnings\n",
    "comparison_df = comparison_df.sort_values(['Commodity', 'Model Version', 'net_earnings'], \n",
    "                                          ascending=[True, True, False])\n",
    "\n",
    "print(\"\\n📊 Summary by Commodity and Model:\")\n",
    "display(comparison_df)\n",
    "\n",
    "# Save comparison\n",
    "comparison_path = f\"{VOLUME_PATH}/cross_model_commodity_summary.csv\"\n",
    "comparison_df.to_csv(comparison_path, index=False)\n",
    "print(f\"\\n✓ Saved: {comparison_path}\")\n",
    "\n",
    "# Determine which commodity/model combination benefits most from predictions\n",
    "best_combo = comparison_df.loc[comparison_df['Prediction Advantage ($)'].idxmax()]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n💡 Best commodity/model for prediction-based strategies:\")\n",
    "print(f\"   {best_combo['Commodity']} - {best_combo['Model Version']}\")\n",
    "print(f\"   Advantage: ${best_combo['Prediction Advantage ($)']:,.2f}\")\n",
    "print(f\"   ({best_combo['Prediction Advantage (%)']:.1f}% improvement)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Generate Cross-Model/Commodity Visualizations\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\nGenerating cross-model/commodity comparison charts...\")\n",
    "\n",
    "# Separate synthetic and real models for visualization\n",
    "synthetic_df = comparison_df[comparison_df['Model Version'].str.startswith('synthetic_')]\n",
    "real_df = comparison_df[~comparison_df['Model Version'].str.startswith('synthetic_')]\n",
    "\n",
    "# Chart 1: Prediction Advantage by Model and Commodity\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "commodities = comparison_df['Commodity'].unique()\n",
    "models = comparison_df['Model Version'].unique()\n",
    "\n",
    "x = np.arange(len(commodities))\n",
    "width = 0.8 / len(models)\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model_data = comparison_df[comparison_df['Model Version'] == model]\n",
    "    advantages = [model_data[model_data['Commodity'] == c]['Prediction Advantage ($)'].values[0] \n",
    "                  if len(model_data[model_data['Commodity'] == c]) > 0 else 0\n",
    "                  for c in commodities]\n",
    "    \n",
    "    ax.bar(x + i * width, advantages, width, label=model, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Commodity', fontsize=12)\n",
    "ax.set_ylabel('Prediction Advantage ($)', fontsize=12)\n",
    "ax.set_title('Prediction Strategy Advantage by Model and Commodity', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width * (len(models) - 1) / 2)\n",
    "ax.set_xticklabels(commodities)\n",
    "ax.legend(loc='best', fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "advantage_path = f\"{VOLUME_PATH}/cross_model_commodity_advantage.png\"\n",
    "plt.savefig(advantage_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {advantage_path}\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Chart 2: Best Strategy Earnings by Model and Commodity\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model_data = comparison_df[comparison_df['Model Version'] == model]\n",
    "    baseline_earnings = [model_data[model_data['Commodity'] == c]['Best Baseline Earnings'].values[0]\n",
    "                         if len(model_data[model_data['Commodity'] == c]) > 0 else 0\n",
    "                         for c in commodities]\n",
    "    prediction_earnings = [model_data[model_data['Commodity'] == c]['Best Prediction Earnings'].values[0]\n",
    "                          if len(model_data[model_data['Commodity'] == c]) > 0 else 0\n",
    "                          for c in commodities]\n",
    "    \n",
    "    x_offset = x + i * width\n",
    "    ax.bar(x_offset - width/4, baseline_earnings, width/2, \n",
    "           label=f'{model} - Baseline' if i == 0 else '', \n",
    "           color='steelblue', alpha=0.7)\n",
    "    ax.bar(x_offset + width/4, prediction_earnings, width/2, \n",
    "           label=f'{model} - Prediction' if i == 0 else '', \n",
    "           color='orangered', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Commodity', fontsize=12)\n",
    "ax.set_ylabel('Net Earnings ($)', fontsize=12)\n",
    "ax.set_title('Best Strategy Earnings by Model and Commodity', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width * (len(models) - 1) / 2)\n",
    "ax.set_xticklabels(commodities)\n",
    "ax.legend(loc='best', fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "earnings_path = f\"{VOLUME_PATH}/cross_model_commodity_earnings.png\"\n",
    "plt.savefig(earnings_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {earnings_path}\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MULTI-COMMODITY/MULTI-MODEL COMPARATIVE ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nResults saved to: {VOLUME_PATH}\")\n",
    "print(f\"\\nCommodities analyzed: {', '.join([c.upper() for c in COMMODITY_CONFIGS.keys()])}\")\n",
    "print(f\"Total commodity-model combinations: {len(comparison_df)}\")\n",
    "print(f\"Strategies tested per combination: {len(all_strategies)}\")\n",
    "print(f\"  - 4 Baseline strategies\")\n",
    "print(f\"  - 5 Prediction-based strategies (3 enhanced + 2 A/B test)\")\n",
    "print(f\"\\n✓ All analyses complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e5d3dcd-c9cd-4529-a2ca-4b6ae5e9d109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NOTEBOOK 05: STATISTICAL VALIDATION (MULTI-MODEL)\n",
    "# ============================================================================\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Statistical Validation - All Commodities and Model Versions\n",
    "# MAGIC \n",
    "# MAGIC Performs statistical tests comparing prediction vs baseline strategies.\n",
    "# MAGIC Runs for all configured commodities and model versions.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./00_setup_and_config\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pickle\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Process All Commodities and Model Versions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Loop through all commodities\n",
    "for CURRENT_COMMODITY in COMMODITY_CONFIGS.keys():\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"STATISTICAL VALIDATION: {CURRENT_COMMODITY.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Discover all model versions for this commodity\n",
    "    print(f\"\\nDiscovering model versions...\")\n",
    "    \n",
    "    synthetic_versions = []\n",
    "    try:\n",
    "        DATA_PATHS = get_data_paths(CURRENT_COMMODITY)\n",
    "        synthetic_df = spark.table(DATA_PATHS['predictions']).select(\"model_version\").distinct()\n",
    "        synthetic_versions = [row.model_version for row in synthetic_df.collect()]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    real_versions = []\n",
    "    try:\n",
    "        real_versions = get_model_versions(CURRENT_COMMODITY)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    all_model_versions = list(set(synthetic_versions + real_versions))\n",
    "    \n",
    "    if len(all_model_versions) == 0:\n",
    "        print(f\"⚠️  No model versions found for {CURRENT_COMMODITY}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"✓ Found {len(all_model_versions)} model versions\")\n",
    "    \n",
    "    # Loop through each model version\n",
    "    for MODEL_VERSION in all_model_versions:\n",
    "        print(f\"\\n{'-' * 80}\")\n",
    "        print(f\"MODEL: {MODEL_VERSION}\")\n",
    "        print(f\"{'-' * 80}\")\n",
    "        \n",
    "        MODEL_DATA_PATHS = get_data_paths(CURRENT_COMMODITY, MODEL_VERSION)\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Load Results\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nLoading results...\")\n",
    "        \n",
    "        try:\n",
    "            with open(MODEL_DATA_PATHS['results_detailed'], 'rb') as f:\n",
    "                results_dict = pickle.load(f)\n",
    "            \n",
    "            results_df = spark.table(MODEL_DATA_PATHS['results']).toPandas()\n",
    "            \n",
    "            print(f\"✓ Loaded results for {len(results_dict)} strategies\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Could not load results: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Identify Best Baseline\n",
    "        # ----------------------------------------------------------------------\n",
    "        baseline_names = ['Immediate Sale', 'Equal Batches', 'Price Threshold', 'Moving Average']\n",
    "        baseline_results = results_df[results_df['strategy'].isin(baseline_names)]\n",
    "        \n",
    "        if len(baseline_results) == 0:\n",
    "            print(\"⚠️  No baseline results found\")\n",
    "            continue\n",
    "        \n",
    "        best_baseline = baseline_results.sort_values('net_earnings', ascending=False).iloc[0]['strategy']\n",
    "        \n",
    "        print(\"\\nBaseline Comparison:\")\n",
    "        print(f\"  Best baseline: {best_baseline}\")\n",
    "        print(f\"  Net Earnings: ${baseline_results.loc[baseline_results['strategy'] == best_baseline, 'net_earnings'].iloc[0]:,.2f}\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Extract Time Series Data\n",
    "        # ----------------------------------------------------------------------\n",
    "        def get_daily_portfolio_values(results, initial_price):\n",
    "            \"\"\"Calculate daily portfolio value = remaining inventory value + accumulated net proceeds\"\"\"\n",
    "            daily_state = results['daily_state']\n",
    "            trades_by_day = {t['day']: t for t in results['trades']}\n",
    "            \n",
    "            accumulated_net_proceeds = 0\n",
    "            portfolio_values = []\n",
    "            \n",
    "            for idx, row in daily_state.iterrows():\n",
    "                day = row['day']\n",
    "                inventory = row['inventory']\n",
    "                price = row['price']\n",
    "                \n",
    "                if day in trades_by_day:\n",
    "                    trade = trades_by_day[day]\n",
    "                    accumulated_net_proceeds += trade['net_revenue']\n",
    "                \n",
    "                inventory_value = inventory * price\n",
    "                portfolio_value = accumulated_net_proceeds + inventory_value\n",
    "                portfolio_values.append(portfolio_value)\n",
    "            \n",
    "            return np.array(portfolio_values)\n",
    "        \n",
    "        print(\"\\nExtracting time series data...\")\n",
    "        initial_price = results_dict[list(results_dict.keys())[0]]['daily_state']['price'].iloc[0]\n",
    "        \n",
    "        portfolio_values_dict = {}\n",
    "        for name, results in results_dict.items():\n",
    "            pv = get_daily_portfolio_values(results, initial_price)\n",
    "            portfolio_values_dict[name] = pv\n",
    "        \n",
    "        print(f\"✓ Extracted time series for {len(portfolio_values_dict)} strategies\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Statistical Tests - Predictions vs Best Baseline\n",
    "        # ----------------------------------------------------------------------\n",
    "        def get_daily_changes(portfolio_values):\n",
    "            \"\"\"Get daily changes in portfolio value\"\"\"\n",
    "            changes = np.diff(portfolio_values)\n",
    "            return changes\n",
    "        \n",
    "        print(\"\\nRunning statistical tests...\")\n",
    "        \n",
    "        baseline_pv = portfolio_values_dict[best_baseline]\n",
    "        baseline_changes = get_daily_changes(baseline_pv)\n",
    "        \n",
    "        prediction_names = ['Consensus', 'Expected Value', 'Risk-Adjusted']\n",
    "        comparison_results = []\n",
    "        \n",
    "        for pred_strat in prediction_names:\n",
    "            if pred_strat in portfolio_values_dict:\n",
    "                pred_pv = portfolio_values_dict[pred_strat]\n",
    "                pred_changes = get_daily_changes(pred_pv)\n",
    "                \n",
    "                # Align lengths\n",
    "                min_len = min(len(baseline_changes), len(pred_changes))\n",
    "                baseline_changes_aligned = baseline_changes[:min_len]\n",
    "                pred_changes_aligned = pred_changes[:min_len]\n",
    "                \n",
    "                # Paired t-test on daily changes\n",
    "                diff = pred_changes_aligned - baseline_changes_aligned\n",
    "                t_stat, p_value = stats.ttest_rel(pred_changes_aligned, baseline_changes_aligned)\n",
    "                \n",
    "                # Effect size (Cohen's d)\n",
    "                cohens_d = np.mean(diff) / np.std(diff) if np.std(diff) > 0 else 0\n",
    "                \n",
    "                # Confidence interval\n",
    "                ci = stats.t.interval(0.95, len(diff)-1, loc=np.mean(diff), scale=stats.sem(diff))\n",
    "                \n",
    "                # Mean difference per day (in dollars)\n",
    "                mean_daily_diff = np.mean(diff)\n",
    "                \n",
    "                # Final earnings difference\n",
    "                pred_earnings = results_df[results_df['strategy'] == pred_strat]['net_earnings'].iloc[0]\n",
    "                baseline_earnings = results_df[results_df['strategy'] == best_baseline]['net_earnings'].iloc[0]\n",
    "                earnings_diff = pred_earnings - baseline_earnings\n",
    "                \n",
    "                comparison_results.append({\n",
    "                    'strategy': pred_strat,\n",
    "                    'baseline': best_baseline,\n",
    "                    't_statistic': t_stat,\n",
    "                    'p_value': p_value,\n",
    "                    'significant': p_value < 0.05,\n",
    "                    'cohens_d': cohens_d,\n",
    "                    'mean_daily_diff': mean_daily_diff,\n",
    "                    'ci_lower': ci[0],\n",
    "                    'ci_upper': ci[1],\n",
    "                    'total_earnings_diff': earnings_diff\n",
    "                })\n",
    "                \n",
    "                print(f\"\\n  {pred_strat} vs {best_baseline}:\")\n",
    "                print(f\"    Earnings diff: ${earnings_diff:+,.2f}\")\n",
    "                print(f\"    p-value: {p_value:.4f} {'***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else 'ns'}\")\n",
    "                print(f\"    Significant: {'YES' if p_value < 0.05 else 'NO'}\")\n",
    "                print(f\"    Cohen's d: {cohens_d:+.3f}\")\n",
    "        \n",
    "        if len(comparison_results) > 0:\n",
    "            comparison_df = pd.DataFrame(comparison_results)\n",
    "        else:\n",
    "            comparison_df = pd.DataFrame()\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Bootstrap Confidence Intervals\n",
    "        # ----------------------------------------------------------------------\n",
    "        def bootstrap_earnings(portfolio_values, n_boot=1000):\n",
    "            \"\"\"Bootstrap confidence intervals for final net earnings\"\"\"\n",
    "            daily_changes = get_daily_changes(portfolio_values)\n",
    "            initial_value = portfolio_values[0]\n",
    "            \n",
    "            final_values = []\n",
    "            \n",
    "            for _ in range(n_boot):\n",
    "                resampled_changes = np.random.choice(daily_changes, size=len(daily_changes), replace=True)\n",
    "                final_value = initial_value + np.sum(resampled_changes)\n",
    "                final_values.append(final_value)\n",
    "            \n",
    "            final_values = np.array(final_values)\n",
    "            \n",
    "            return {\n",
    "                'mean': np.mean(final_values),\n",
    "                'median': np.median(final_values),\n",
    "                'std': np.std(final_values),\n",
    "                'ci_lower': np.percentile(final_values, 2.5),\n",
    "                'ci_upper': np.percentile(final_values, 97.5)\n",
    "            }\n",
    "        \n",
    "        print(\"\\nBootstrapping confidence intervals...\")\n",
    "        \n",
    "        bootstrap_results = {}\n",
    "        strategies_to_test = prediction_names + [best_baseline]\n",
    "        \n",
    "        for name in strategies_to_test:\n",
    "            if name in portfolio_values_dict:\n",
    "                bootstrap_results[name] = bootstrap_earnings(\n",
    "                    portfolio_values_dict[name], \n",
    "                    n_boot=ANALYSIS_CONFIG['bootstrap_iterations']\n",
    "                )\n",
    "        \n",
    "        # Create bootstrap summary table\n",
    "        bootstrap_df = pd.DataFrame(bootstrap_results).T\n",
    "        bootstrap_df = bootstrap_df.reset_index().rename(columns={'index': 'strategy'})\n",
    "        bootstrap_df = bootstrap_df.sort_values('mean', ascending=False)\n",
    "        \n",
    "        print(f\"✓ Bootstrap complete for {len(bootstrap_results)} strategies\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Summary Statistics\n",
    "        # ----------------------------------------------------------------------\n",
    "        summary_stats = []\n",
    "        for name in [best_baseline] + prediction_names:\n",
    "            if name in results_df['strategy'].values:\n",
    "                row = results_df[results_df['strategy'] == name].iloc[0]\n",
    "                stats_dict = {\n",
    "                    'strategy': name,\n",
    "                    'type': row['type'],\n",
    "                    'net_earnings': row['net_earnings'],\n",
    "                    'avg_sale_price': row['avg_sale_price'],\n",
    "                    'total_costs': row['total_costs'],\n",
    "                    'n_trades': row['n_trades']\n",
    "                }\n",
    "                \n",
    "                if name in bootstrap_results:\n",
    "                    stats_dict['earnings_ci_width'] = bootstrap_results[name]['ci_upper'] - bootstrap_results[name]['ci_lower']\n",
    "                \n",
    "                summary_stats.append(stats_dict)\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_stats)\n",
    "        summary_df = summary_df.sort_values('net_earnings', ascending=False)\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Save Results\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(\"\\nSaving results...\")\n",
    "        \n",
    "        stat_results = {\n",
    "            'commodity': CURRENT_COMMODITY,\n",
    "            'model_version': MODEL_VERSION,\n",
    "            'comparisons': comparison_df if len(comparison_results) > 0 else None,\n",
    "            'bootstrap': bootstrap_results,\n",
    "            'bootstrap_summary': bootstrap_df,\n",
    "            'summary_stats': summary_df,\n",
    "            'best_baseline': best_baseline\n",
    "        }\n",
    "        \n",
    "        with open(MODEL_DATA_PATHS['statistical_results'], 'wb') as f:\n",
    "            pickle.dump(stat_results, f)\n",
    "        print(f\"  ✓ Saved: {MODEL_DATA_PATHS['statistical_results']}\")\n",
    "        \n",
    "        # Save as CSV for easy viewing\n",
    "        if len(comparison_results) > 0:\n",
    "            comparison_df.to_csv(MODEL_DATA_PATHS['statistical_comparisons'], index=False)\n",
    "            print(f\"  ✓ Saved: {MODEL_DATA_PATHS['statistical_comparisons']}\")\n",
    "        \n",
    "        bootstrap_df.to_csv(MODEL_DATA_PATHS['bootstrap_summary'], index=False)\n",
    "        summary_df.to_csv(MODEL_DATA_PATHS['summary_stats'], index=False)\n",
    "        print(f\"  ✓ Saved: {MODEL_DATA_PATHS['bootstrap_summary']}\")\n",
    "        print(f\"  ✓ Saved: {MODEL_DATA_PATHS['summary_stats']}\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Print Key Findings\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"✓ STATISTICAL VALIDATION COMPLETE - {CURRENT_COMMODITY.upper()} - {MODEL_VERSION}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if len(comparison_results) > 0:\n",
    "            print(f\"\\nKey findings:\")\n",
    "            print(f\"  Best baseline: {best_baseline}\")\n",
    "            \n",
    "            best_pred_result = comparison_df.loc[comparison_df['total_earnings_diff'].idxmax()]\n",
    "            print(f\"  Best prediction strategy: {best_pred_result['strategy']}\")\n",
    "            print(f\"  Earnings advantage: ${best_pred_result['total_earnings_diff']:+,.2f}\")\n",
    "            print(f\"  Statistical significance: {'YES (p < 0.05)' if best_pred_result['significant'] else 'NO (p >= 0.05)'}\")\n",
    "            print(f\"  Effect size: {best_pred_result['cohens_d']:.3f}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL STATISTICAL VALIDATIONS COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Commodities analyzed: {', '.join([c.upper() for c in COMMODITY_CONFIGS.keys()])}\")\n",
    "print(\"\\n✓ Statistical validation complete for all commodities and models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7587df95-e796-4f2e-997c-5dcb7a5ceb65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NOTEBOOK 06: FEATURE IMPORTANCE ANALYSIS (MULTI-MODEL)\n",
    "# ============================================================================\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Feature Importance Analysis - All Commodities and Model Versions\n",
    "# MAGIC \n",
    "# MAGIC Analyzes which prediction features are most important for forecasting returns.\n",
    "# MAGIC Runs for all configured commodities and model versions.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./00_setup_and_config\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Feature Extraction and Analysis\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def extract_features(predictions, current_price, eval_day=14):\n",
    "    \"\"\"\n",
    "    Extract features from prediction ensemble for a given evaluation day.\n",
    "    \n",
    "    Args:\n",
    "        predictions: N×H matrix of predictions (N runs × H horizons)\n",
    "        current_price: Current market price\n",
    "        eval_day: Which day ahead to evaluate (1-14)\n",
    "    \n",
    "    Returns:\n",
    "        dict of features or None if invalid\n",
    "    \"\"\"\n",
    "    if predictions is None or len(predictions) == 0:\n",
    "        return None\n",
    "    \n",
    "    day_preds = predictions[:, eval_day - 1]\n",
    "    \n",
    "    return {\n",
    "        'directional_consensus': np.mean(day_preds > current_price),\n",
    "        'expected_return': (np.median(day_preds) - current_price) / current_price,\n",
    "        'uncertainty': (np.percentile(day_preds, 75) - np.percentile(day_preds, 25)) / np.median(day_preds),\n",
    "        'skewness': float(pd.Series(day_preds).skew()),\n",
    "        'prediction_range': (np.max(day_preds) - np.min(day_preds)) / current_price,\n",
    "        'downside_risk': (np.percentile(day_preds, 10) - current_price) / current_price\n",
    "    }\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Loop through all commodities\n",
    "for CURRENT_COMMODITY in COMMODITY_CONFIGS.keys():\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"FEATURE IMPORTANCE ANALYSIS: {CURRENT_COMMODITY.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Discover all model versions for this commodity\n",
    "    print(f\"\\nDiscovering model versions...\")\n",
    "    \n",
    "    synthetic_versions = []\n",
    "    try:\n",
    "        DATA_PATHS = get_data_paths(CURRENT_COMMODITY)\n",
    "        synthetic_df = spark.table(DATA_PATHS['predictions']).select(\"model_version\").distinct()\n",
    "        synthetic_versions = [row.model_version for row in synthetic_df.collect()]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    real_versions = []\n",
    "    try:\n",
    "        real_versions = get_model_versions(CURRENT_COMMODITY)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    all_model_versions = list(set(synthetic_versions + real_versions))\n",
    "    \n",
    "    if len(all_model_versions) == 0:\n",
    "        print(f\"⚠️  No model versions found for {CURRENT_COMMODITY}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"✓ Found {len(all_model_versions)} model versions\")\n",
    "    \n",
    "    # Loop through each model version\n",
    "    for MODEL_VERSION in all_model_versions:\n",
    "        print(f\"\\n{'-' * 80}\")\n",
    "        print(f\"MODEL: {MODEL_VERSION}\")\n",
    "        print(f\"{'-' * 80}\")\n",
    "        \n",
    "        MODEL_DATA_PATHS = get_data_paths(CURRENT_COMMODITY, MODEL_VERSION)\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Load Data\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nLoading prepared data...\")\n",
    "        \n",
    "        try:\n",
    "            prices = spark.table(get_data_paths(CURRENT_COMMODITY)['prices_prepared']).toPandas()\n",
    "            prices['date'] = pd.to_datetime(prices['date']).dt.normalize()\n",
    "            \n",
    "            # Load prediction matrices for this model version\n",
    "            if MODEL_VERSION.startswith('synthetic_'):\n",
    "                matrices_path = MODEL_DATA_PATHS['prediction_matrices']\n",
    "            else:\n",
    "                matrices_path = MODEL_DATA_PATHS['prediction_matrices_real']\n",
    "            \n",
    "            with open(matrices_path, 'rb') as f:\n",
    "                prediction_matrices = pickle.load(f)\n",
    "            \n",
    "            # Normalize prediction matrix keys\n",
    "            prediction_matrices_normalized = {\n",
    "                pd.Timestamp(k).normalize(): v \n",
    "                for k, v in prediction_matrices.items()\n",
    "            }\n",
    "            prediction_matrices = prediction_matrices_normalized\n",
    "            \n",
    "            print(f\"✓ Loaded {len(prices)} days of prices\")\n",
    "            print(f\"✓ Loaded {len(prediction_matrices)} prediction matrices\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Could not load data: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Build Feature Dataset\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nExtracting features...\")\n",
    "        \n",
    "        eval_day = ANALYSIS_CONFIG['forecast_horizon']\n",
    "        feature_data = []\n",
    "        \n",
    "        for i in range(len(prices) - eval_day):\n",
    "            current_date = prices.loc[i, 'date']\n",
    "            current_price = prices.loc[i, 'price']\n",
    "            \n",
    "            if current_date not in prediction_matrices:\n",
    "                continue\n",
    "            \n",
    "            predictions = prediction_matrices[current_date]\n",
    "            features = extract_features(predictions, current_price, eval_day)\n",
    "            \n",
    "            if features is None:\n",
    "                continue\n",
    "            \n",
    "            future_price = prices.loc[i + eval_day, 'price']\n",
    "            actual_return = (future_price - current_price) / current_price\n",
    "            \n",
    "            feature_data.append({\n",
    "                'date': current_date,\n",
    "                'current_price': current_price,\n",
    "                'actual_return': actual_return,\n",
    "                **features\n",
    "            })\n",
    "        \n",
    "        if len(feature_data) == 0:\n",
    "            print(\"⚠️  No features extracted\")\n",
    "            continue\n",
    "        \n",
    "        feature_df = pd.DataFrame(feature_data)\n",
    "        print(f\"✓ Extracted features for {len(feature_df)} days\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Train Random Forest Model\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nTraining Random Forest model...\")\n",
    "        \n",
    "        feature_cols = ['directional_consensus', 'expected_return', 'uncertainty', \n",
    "                       'skewness', 'prediction_range', 'downside_risk']\n",
    "        \n",
    "        X = feature_df[feature_cols]\n",
    "        y = feature_df['actual_return']\n",
    "        \n",
    "        # Train model\n",
    "        rf_model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=ANALYSIS_CONFIG['random_seed'],\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        rf_model.fit(X, y)\n",
    "        \n",
    "        # Cross-validation score\n",
    "        cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='r2')\n",
    "        \n",
    "        print(f\"✓ Model trained\")\n",
    "        print(f\"  R² score: {rf_model.score(X, y):.3f}\")\n",
    "        print(f\"  CV R² score: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Feature Importance\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nAnalyzing feature importance...\")\n",
    "        \n",
    "        importances = rf_model.feature_importances_\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nFeature Importances:\")\n",
    "        for _, row in importance_df.iterrows():\n",
    "            print(f\"  {row['feature']:25s}: {row['importance']:.3f}\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Visualization\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nGenerating visualization...\")\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        ax.barh(importance_df['feature'], importance_df['importance'], color='steelblue', alpha=0.7)\n",
    "        ax.set_xlabel('Importance', fontsize=12)\n",
    "        ax.set_title(f'Feature Importance - {CURRENT_COMMODITY.upper()} - {MODEL_VERSION}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        viz_path = f'{VOLUME_PATH}/feature_importance_{CURRENT_COMMODITY}_{MODEL_VERSION}.png'\n",
    "        plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✓ Saved: {viz_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Save Results\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nSaving results...\")\n",
    "        \n",
    "        feature_analysis = {\n",
    "            'commodity': CURRENT_COMMODITY,\n",
    "            'model_version': MODEL_VERSION,\n",
    "            'feature_importance': importance_df,\n",
    "            'model': rf_model,\n",
    "            'feature_data': feature_df,\n",
    "            'cv_scores': cv_scores,\n",
    "            'r2_score': rf_model.score(X, y)\n",
    "        }\n",
    "        \n",
    "        with open(MODEL_DATA_PATHS['feature_analysis'], 'wb') as f:\n",
    "            pickle.dump(feature_analysis, f)\n",
    "        \n",
    "        print(f\"✓ Saved: {MODEL_DATA_PATHS['feature_analysis']}\")\n",
    "        \n",
    "        # Save importance as CSV\n",
    "        importance_csv = f'{VOLUME_PATH}/feature_importance_{CURRENT_COMMODITY}_{MODEL_VERSION}.csv'\n",
    "        importance_df.to_csv(importance_csv, index=False)\n",
    "        print(f\"✓ Saved: {importance_csv}\")\n",
    "        \n",
    "        print(f\"\\n✓ Feature analysis complete for {MODEL_VERSION}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"✓ {CURRENT_COMMODITY.upper()} COMPLETE\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL FEATURE IMPORTANCE ANALYSES COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Commodities analyzed: {', '.join([c.upper() for c in COMMODITY_CONFIGS.keys()])}\")\n",
    "print(\"\\n✓ Feature importance analysis complete for all commodities and models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bbe4cc1-4591-4fd9-9d5a-350d28ec5fa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NOTEBOOK 07: SENSITIVITY ANALYSIS (MULTI-MODEL)\n",
    "# ============================================================================\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Sensitivity Analysis - All Commodities and Model Versions\n",
    "# MAGIC \n",
    "# MAGIC Tests how robust strategies are to parameter and cost changes.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./00_setup_and_config\n",
    "# MAGIC %run ./02_strategy_implementations\n",
    "# MAGIC %run ./03_backtesting_engine\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Sensitivity Analysis Functions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def run_sensitivity_consensus(prices, prediction_matrices, commodity_config):\n",
    "    \"\"\"Test sensitivity of Consensus strategy to parameter changes.\"\"\"\n",
    "    engine = BacktestEngine(prices, prediction_matrices, commodity_config)\n",
    "    results = []\n",
    "    \n",
    "    # Test different consensus thresholds and minimum returns\n",
    "    for cons_thresh in [0.60, 0.65, 0.70, 0.75, 0.80]:\n",
    "        for min_ret in [0.02, 0.03, 0.04, 0.05, 0.06]:\n",
    "            strategy = ConsensusStrategy(\n",
    "                consensus_threshold=cons_thresh, \n",
    "                min_return=min_ret, \n",
    "                evaluation_day=ANALYSIS_CONFIG['forecast_horizon']\n",
    "            )\n",
    "            backtest_result = engine.run(strategy)\n",
    "            metrics = calculate_metrics(backtest_result)\n",
    "            results.append({\n",
    "                'consensus_threshold': cons_thresh, \n",
    "                'min_return': min_ret, \n",
    "                **metrics\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def run_cost_sensitivity(prices, prediction_matrices, commodity_config, cost_type='transaction'):\n",
    "    \"\"\"Test sensitivity to transaction or storage cost changes (percentage-based).\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for multiplier in [0.5, 0.75, 1.0, 1.5, 2.0]:\n",
    "        config = commodity_config.copy()\n",
    "        \n",
    "        if cost_type == 'transaction':\n",
    "            config['transaction_cost_pct'] = commodity_config['transaction_cost_pct'] * multiplier\n",
    "        elif cost_type == 'storage':\n",
    "            config['storage_cost_pct_per_day'] = commodity_config['storage_cost_pct_per_day'] * multiplier\n",
    "        \n",
    "        engine = BacktestEngine(prices, prediction_matrices, config)\n",
    "        \n",
    "        # Run prediction strategy\n",
    "        pred_strategy = RiskAdjustedStrategy(**PREDICTION_PARAMS['risk_adjusted'])\n",
    "        pred_result = engine.run(pred_strategy)\n",
    "        pred_metrics = calculate_metrics(pred_result)\n",
    "        \n",
    "        # Run baseline strategy\n",
    "        baseline_strategy = MovingAverageStrategy(ma_period=BASELINE_PARAMS['moving_average']['ma_period'])\n",
    "        baseline_result = engine.run(baseline_strategy)\n",
    "        baseline_metrics = calculate_metrics(baseline_result)\n",
    "        \n",
    "        results.append({\n",
    "            'cost_multiplier': multiplier,\n",
    "            'prediction_earnings': pred_metrics['net_earnings'],\n",
    "            'baseline_earnings': baseline_metrics['net_earnings'],\n",
    "            'advantage': pred_metrics['net_earnings'] - baseline_metrics['net_earnings']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Run Sensitivity Analysis for All Commodities and Models\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Loop through all commodities\n",
    "for CURRENT_COMMODITY in COMMODITY_CONFIGS.keys():\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"SENSITIVITY ANALYSIS: {CURRENT_COMMODITY.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    CURRENT_CONFIG = COMMODITY_CONFIGS[CURRENT_COMMODITY]\n",
    "    \n",
    "    # Discover all model versions for this commodity\n",
    "    print(f\"\\nDiscovering model versions...\")\n",
    "    \n",
    "    synthetic_versions = []\n",
    "    try:\n",
    "        DATA_PATHS = get_data_paths(CURRENT_COMMODITY)\n",
    "        synthetic_df = spark.table(DATA_PATHS['predictions']).select(\"model_version\").distinct()\n",
    "        synthetic_versions = [row.model_version for row in synthetic_df.collect()]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    real_versions = []\n",
    "    try:\n",
    "        real_versions = get_model_versions(CURRENT_COMMODITY)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    all_model_versions = list(set(synthetic_versions + real_versions))\n",
    "    \n",
    "    if len(all_model_versions) == 0:\n",
    "        print(f\"⚠️  No model versions found for {CURRENT_COMMODITY}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"✓ Found {len(all_model_versions)} model versions\")\n",
    "    \n",
    "    # Loop through each model version\n",
    "    for MODEL_VERSION in all_model_versions:\n",
    "        print(f\"\\n{'-' * 80}\")\n",
    "        print(f\"MODEL: {MODEL_VERSION}\")\n",
    "        print(f\"{'-' * 80}\")\n",
    "        \n",
    "        MODEL_DATA_PATHS = get_data_paths(CURRENT_COMMODITY, MODEL_VERSION)\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Load Data\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nLoading prepared data...\")\n",
    "        \n",
    "        try:\n",
    "            prices = spark.table(get_data_paths(CURRENT_COMMODITY)['prices_prepared']).toPandas()\n",
    "            prices['date'] = pd.to_datetime(prices['date'])\n",
    "            \n",
    "            # Load prediction matrices for this model version\n",
    "            if MODEL_VERSION.startswith('synthetic_'):\n",
    "                matrices_path = MODEL_DATA_PATHS['prediction_matrices']\n",
    "            else:\n",
    "                matrices_path = MODEL_DATA_PATHS['prediction_matrices_real']\n",
    "            \n",
    "            with open(matrices_path, 'rb') as f:\n",
    "                prediction_matrices = pickle.load(f)\n",
    "            \n",
    "            print(f\"✓ Loaded {len(prices)} days of prices\")\n",
    "            print(f\"✓ Loaded {len(prediction_matrices)} prediction matrices\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Could not load data: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Consensus Strategy Parameter Sensitivity\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nRunning Consensus strategy parameter sensitivity...\")\n",
    "        \n",
    "        consensus_sensitivity = run_sensitivity_consensus(prices, prediction_matrices, CURRENT_CONFIG)\n",
    "        \n",
    "        print(f\"✓ Tested {len(consensus_sensitivity)} parameter combinations\")\n",
    "        \n",
    "        # Find optimal parameters\n",
    "        best_params = consensus_sensitivity.loc[consensus_sensitivity['net_earnings'].idxmax()]\n",
    "        print(f\"\\nOptimal parameters:\")\n",
    "        print(f\"  Consensus threshold: {best_params['consensus_threshold']:.2f}\")\n",
    "        print(f\"  Minimum return: {best_params['min_return']:.2%}\")\n",
    "        print(f\"  Net earnings: ${best_params['net_earnings']:,.2f}\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Transaction Cost Sensitivity\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nRunning transaction cost sensitivity...\")\n",
    "        \n",
    "        transaction_sensitivity = run_cost_sensitivity(prices, prediction_matrices, CURRENT_CONFIG, cost_type='transaction')\n",
    "        \n",
    "        print(f\"✓ Tested {len(transaction_sensitivity)} cost scenarios\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Storage Cost Sensitivity\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nRunning storage cost sensitivity...\")\n",
    "        \n",
    "        storage_sensitivity = run_cost_sensitivity(prices, prediction_matrices, CURRENT_CONFIG, cost_type='storage')\n",
    "        \n",
    "        print(f\"✓ Tested {len(storage_sensitivity)} cost scenarios\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Visualizations\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nGenerating visualizations...\")\n",
    "        \n",
    "        # 1. Consensus parameter heatmap\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        pivot_data = consensus_sensitivity.pivot(\n",
    "            index='min_return', \n",
    "            columns='consensus_threshold', \n",
    "            values='net_earnings'\n",
    "        )\n",
    "        \n",
    "        sns.heatmap(pivot_data, annot=True, fmt='.0f', cmap='RdYlGn', ax=ax, \n",
    "                   cbar_kws={'label': 'Net Earnings ($)'})\n",
    "        \n",
    "        ax.set_title(f'Consensus Strategy Parameter Sensitivity\\n{CURRENT_COMMODITY.upper()} - {MODEL_VERSION}', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Consensus Threshold', fontsize=12)\n",
    "        ax.set_ylabel('Minimum Return', fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        heatmap_path = f'{VOLUME_PATH}/consensus_sensitivity_{CURRENT_COMMODITY}_{MODEL_VERSION}.png'\n",
    "        plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✓ Saved: {heatmap_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Transaction cost sensitivity\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        ax.plot(transaction_sensitivity['cost_multiplier'], \n",
    "               transaction_sensitivity['prediction_earnings'], \n",
    "               marker='o', linewidth=2, label='Prediction Strategy', color='orangered')\n",
    "        ax.plot(transaction_sensitivity['cost_multiplier'], \n",
    "               transaction_sensitivity['baseline_earnings'], \n",
    "               marker='s', linewidth=2, label='Baseline Strategy', color='steelblue')\n",
    "        \n",
    "        ax.set_xlabel('Transaction Cost Multiplier', fontsize=12)\n",
    "        ax.set_ylabel('Net Earnings ($)', fontsize=12)\n",
    "        ax.set_title(f'Transaction Cost Sensitivity\\n{CURRENT_COMMODITY.upper()} - {MODEL_VERSION}', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.axvline(x=1.0, color='black', linestyle='--', linewidth=1, alpha=0.5, label='Baseline Cost')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        trans_path = f'{VOLUME_PATH}/transaction_sensitivity_{CURRENT_COMMODITY}_{MODEL_VERSION}.png'\n",
    "        plt.savefig(trans_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✓ Saved: {trans_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # 3. Storage cost sensitivity\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        ax.plot(storage_sensitivity['cost_multiplier'], \n",
    "               storage_sensitivity['prediction_earnings'], \n",
    "               marker='o', linewidth=2, label='Prediction Strategy', color='orangered')\n",
    "        ax.plot(storage_sensitivity['cost_multiplier'], \n",
    "               storage_sensitivity['baseline_earnings'], \n",
    "               marker='s', linewidth=2, label='Baseline Strategy', color='steelblue')\n",
    "        \n",
    "        ax.set_xlabel('Storage Cost Multiplier', fontsize=12)\n",
    "        ax.set_ylabel('Net Earnings ($)', fontsize=12)\n",
    "        ax.set_title(f'Storage Cost Sensitivity\\n{CURRENT_COMMODITY.upper()} - {MODEL_VERSION}', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.axvline(x=1.0, color='black', linestyle='--', linewidth=1, alpha=0.5, label='Baseline Cost')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        storage_path = f'{VOLUME_PATH}/storage_sensitivity_{CURRENT_COMMODITY}_{MODEL_VERSION}.png'\n",
    "        plt.savefig(storage_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✓ Saved: {storage_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Save Results\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nSaving results...\")\n",
    "        \n",
    "        sensitivity_results = {\n",
    "            'commodity': CURRENT_COMMODITY,\n",
    "            'model_version': MODEL_VERSION,\n",
    "            'consensus_sensitivity': consensus_sensitivity,\n",
    "            'transaction_sensitivity': transaction_sensitivity,\n",
    "            'storage_sensitivity': storage_sensitivity,\n",
    "            'optimal_consensus_params': {\n",
    "                'consensus_threshold': best_params['consensus_threshold'],\n",
    "                'min_return': best_params['min_return'],\n",
    "                'net_earnings': best_params['net_earnings']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(MODEL_DATA_PATHS['sensitivity_results'], 'wb') as f:\n",
    "            pickle.dump(sensitivity_results, f)\n",
    "        \n",
    "        print(f\"✓ Saved: {MODEL_DATA_PATHS['sensitivity_results']}\")\n",
    "        \n",
    "        # Save as CSVs\n",
    "        consensus_csv = f'{VOLUME_PATH}/consensus_sensitivity_{CURRENT_COMMODITY}_{MODEL_VERSION}.csv'\n",
    "        consensus_sensitivity.to_csv(consensus_csv, index=False)\n",
    "        print(f\"✓ Saved: {consensus_csv}\")\n",
    "        \n",
    "        trans_csv = f'{VOLUME_PATH}/transaction_sensitivity_{CURRENT_COMMODITY}_{MODEL_VERSION}.csv'\n",
    "        transaction_sensitivity.to_csv(trans_csv, index=False)\n",
    "        print(f\"✓ Saved: {trans_csv}\")\n",
    "        \n",
    "        storage_csv = f'{VOLUME_PATH}/storage_sensitivity_{CURRENT_COMMODITY}_{MODEL_VERSION}.csv'\n",
    "        storage_sensitivity.to_csv(storage_csv, index=False)\n",
    "        print(f\"✓ Saved: {storage_csv}\")\n",
    "        \n",
    "        print(f\"\\n✓ Sensitivity analysis complete for {MODEL_VERSION}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"✓ {CURRENT_COMMODITY.upper()} COMPLETE\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL SENSITIVITY ANALYSES COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Commodities analyzed: {', '.join([c.upper() for c in COMMODITY_CONFIGS.keys()])}\")\n",
    "print(\"\\n✓ Sensitivity analysis complete for all commodities and models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "add34aa1-26aa-4bf2-96d1-12b61b03674b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NOTEBOOK 08: VISUALIZATION AND REPORTING (MULTI-MODEL)\n",
    "# ============================================================================\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Final Report and Dashboard - All Commodities and Model Versions\n",
    "# MAGIC \n",
    "# MAGIC Creates comprehensive reports and visualizations for each commodity and model version\n",
    "# MAGIC plus cross-model and cross-commodity comparisons.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./00_setup_and_config\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Generate Individual Commodity and Model Reports\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Store summaries for cross-commodity and cross-model comparison\n",
    "all_summaries = []\n",
    "\n",
    "# Loop through all commodities\n",
    "for CURRENT_COMMODITY in COMMODITY_CONFIGS.keys():\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"GENERATING REPORTS: {CURRENT_COMMODITY.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Discover all model versions for this commodity\n",
    "    print(f\"\\nDiscovering model versions...\")\n",
    "    \n",
    "    synthetic_versions = []\n",
    "    try:\n",
    "        DATA_PATHS = get_data_paths(CURRENT_COMMODITY)\n",
    "        synthetic_df = spark.table(DATA_PATHS['predictions']).select(\"model_version\").distinct()\n",
    "        synthetic_versions = [row.model_version for row in synthetic_df.collect()]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    real_versions = []\n",
    "    try:\n",
    "        real_versions = get_model_versions(CURRENT_COMMODITY)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    all_model_versions = list(set(synthetic_versions + real_versions))\n",
    "    \n",
    "    if len(all_model_versions) == 0:\n",
    "        print(f\"⚠️  No model versions found for {CURRENT_COMMODITY}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"✓ Found {len(all_model_versions)} model versions\")\n",
    "    \n",
    "    # Loop through each model version\n",
    "    for MODEL_VERSION in all_model_versions:\n",
    "        print(f\"\\n{'-' * 80}\")\n",
    "        print(f\"MODEL: {MODEL_VERSION}\")\n",
    "        print(f\"{'-' * 80}\")\n",
    "        \n",
    "        MODEL_DATA_PATHS = get_data_paths(CURRENT_COMMODITY, MODEL_VERSION)\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Load All Results\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nLoading results...\")\n",
    "        \n",
    "        try:\n",
    "            with open(MODEL_DATA_PATHS['results_detailed'], 'rb') as f:\n",
    "                results_detailed = pickle.load(f)\n",
    "            \n",
    "            with open(MODEL_DATA_PATHS['statistical_results'], 'rb') as f:\n",
    "                stat_results = pickle.load(f)\n",
    "            \n",
    "            with open(MODEL_DATA_PATHS['feature_analysis'], 'rb') as f:\n",
    "                feature_results = pickle.load(f)\n",
    "            \n",
    "            with open(MODEL_DATA_PATHS['sensitivity_results'], 'rb') as f:\n",
    "                sensitivity_results = pickle.load(f)\n",
    "            \n",
    "            results_df = spark.table(MODEL_DATA_PATHS['results']).toPandas()\n",
    "            \n",
    "            print(f\"✓ Loaded all analysis results\")\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"⚠️  Missing file for {CURRENT_COMMODITY} - {MODEL_VERSION}\")\n",
    "            print(f\"   {e}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Error loading data: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Executive Summary\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nGenerating executive summary...\")\n",
    "        \n",
    "        # Best performers\n",
    "        best_overall = results_df.loc[results_df['net_earnings'].idxmax()]\n",
    "        \n",
    "        baseline_results = results_df[results_df['type'] == 'baseline']\n",
    "        prediction_results = results_df[results_df['type'] == 'prediction']\n",
    "        \n",
    "        best_baseline = baseline_results.loc[baseline_results['net_earnings'].idxmax()] if len(baseline_results) > 0 else None\n",
    "        best_prediction = prediction_results.loc[prediction_results['net_earnings'].idxmax()] if len(prediction_results) > 0 else None\n",
    "        \n",
    "        # Calculate advantage\n",
    "        if best_baseline is not None and best_prediction is not None:\n",
    "            earnings_diff = best_prediction['net_earnings'] - best_baseline['net_earnings']\n",
    "            pct_diff = (earnings_diff / abs(best_baseline['net_earnings'])) * 100 if best_baseline['net_earnings'] != 0 else 0\n",
    "        else:\n",
    "            earnings_diff = 0\n",
    "            pct_diff = 0\n",
    "        \n",
    "        # Summary statistics\n",
    "        summary = {\n",
    "            'commodity': CURRENT_COMMODITY,\n",
    "            'model_version': MODEL_VERSION,\n",
    "            'source_type': 'SYNTHETIC' if MODEL_VERSION.startswith('synthetic_') else 'REAL',\n",
    "            'best_overall_strategy': best_overall['strategy'],\n",
    "            'best_overall_earnings': best_overall['net_earnings'],\n",
    "            'best_baseline_strategy': best_baseline['strategy'] if best_baseline is not None else None,\n",
    "            'best_baseline_earnings': best_baseline['net_earnings'] if best_baseline is not None else None,\n",
    "            'best_prediction_strategy': best_prediction['strategy'] if best_prediction is not None else None,\n",
    "            'best_prediction_earnings': best_prediction['net_earnings'] if best_prediction is not None else None,\n",
    "            'prediction_advantage_dollars': earnings_diff,\n",
    "            'prediction_advantage_pct': pct_diff,\n",
    "            'n_strategies_tested': len(results_df),\n",
    "            'statistical_significance': stat_results.get('comparisons', pd.DataFrame()).get('significant', pd.Series()).any() if 'comparisons' in stat_results else False\n",
    "        }\n",
    "        \n",
    "        all_summaries.append(summary)\n",
    "        \n",
    "        print(f\"✓ Executive summary created\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Create Summary Report\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nCreating summary report...\")\n",
    "        \n",
    "        report_lines = []\n",
    "        report_lines.append(\"=\" * 80)\n",
    "        report_lines.append(f\"TRADING STRATEGY ANALYSIS REPORT\")\n",
    "        report_lines.append(f\"Commodity: {CURRENT_COMMODITY.upper()}\")\n",
    "        report_lines.append(f\"Model: {MODEL_VERSION}\")\n",
    "        report_lines.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        report_lines.append(\"=\" * 80)\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        report_lines.append(\"EXECUTIVE SUMMARY\")\n",
    "        report_lines.append(\"-\" * 80)\n",
    "        report_lines.append(f\"Best Overall Strategy: {best_overall['strategy']}\")\n",
    "        report_lines.append(f\"  Net Earnings: ${best_overall['net_earnings']:,.2f}\")\n",
    "        report_lines.append(f\"  Total Revenue: ${best_overall['total_revenue']:,.2f}\")\n",
    "        report_lines.append(f\"  Total Costs: ${best_overall['total_costs']:,.2f}\")\n",
    "        report_lines.append(f\"  Number of Trades: {best_overall['n_trades']}\")\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        if best_baseline is not None:\n",
    "            report_lines.append(f\"Best Baseline Strategy: {best_baseline['strategy']}\")\n",
    "            report_lines.append(f\"  Net Earnings: ${best_baseline['net_earnings']:,.2f}\")\n",
    "            report_lines.append(\"\")\n",
    "        \n",
    "        if best_prediction is not None:\n",
    "            report_lines.append(f\"Best Prediction Strategy: {best_prediction['strategy']}\")\n",
    "            report_lines.append(f\"  Net Earnings: ${best_prediction['net_earnings']:,.2f}\")\n",
    "            report_lines.append(f\"  Advantage over Baseline: ${earnings_diff:+,.2f} ({pct_diff:+.1f}%)\")\n",
    "            report_lines.append(\"\")\n",
    "        \n",
    "        report_lines.append(\"STRATEGY COMPARISON\")\n",
    "        report_lines.append(\"-\" * 80)\n",
    "        for _, row in results_df.sort_values('net_earnings', ascending=False).iterrows():\n",
    "            report_lines.append(f\"{row['strategy']:30s} {row['type']:10s} ${row['net_earnings']:>12,.2f}\")\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        if 'comparisons' in stat_results and stat_results['comparisons'] is not None and len(stat_results['comparisons']) > 0:\n",
    "            report_lines.append(\"STATISTICAL SIGNIFICANCE\")\n",
    "            report_lines.append(\"-\" * 80)\n",
    "            for _, row in stat_results['comparisons'].iterrows():\n",
    "                sig_marker = \"***\" if row['p_value'] < 0.001 else \"**\" if row['p_value'] < 0.01 else \"*\" if row['p_value'] < 0.05 else \"ns\"\n",
    "                report_lines.append(f\"{row['strategy']:30s} p={row['p_value']:.4f} {sig_marker:3s} d={row['cohens_d']:+.3f}\")\n",
    "            report_lines.append(\"\")\n",
    "        \n",
    "        if 'feature_importance' in feature_results:\n",
    "            report_lines.append(\"FEATURE IMPORTANCE\")\n",
    "            report_lines.append(\"-\" * 80)\n",
    "            for _, row in feature_results['feature_importance'].iterrows():\n",
    "                report_lines.append(f\"{row['feature']:30s} {row['importance']:.3f}\")\n",
    "            report_lines.append(\"\")\n",
    "        \n",
    "        report_lines.append(\"=\" * 80)\n",
    "        \n",
    "        report_text = \"\\n\".join(report_lines)\n",
    "        \n",
    "        # Save report\n",
    "        report_path = f'{VOLUME_PATH}/report_{CURRENT_COMMODITY}_{MODEL_VERSION}.txt'\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(report_text)\n",
    "        \n",
    "        print(f\"✓ Saved: {report_path}\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Create Dashboard Visualization\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nCreating dashboard visualization...\")\n",
    "        \n",
    "        fig = plt.figure(figsize=(20, 12))\n",
    "        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # 1. Strategy Earnings Comparison\n",
    "        ax1 = fig.add_subplot(gs[0, :2])\n",
    "        results_sorted = results_df.sort_values('net_earnings', ascending=False)\n",
    "        colors = ['orangered' if t == 'prediction' else 'steelblue' for t in results_sorted['type']]\n",
    "        ax1.barh(results_sorted['strategy'], results_sorted['net_earnings'], color=colors, alpha=0.7)\n",
    "        ax1.set_xlabel('Net Earnings ($)')\n",
    "        ax1.set_title(f'Strategy Performance - {CURRENT_COMMODITY.upper()} - {MODEL_VERSION}', fontweight='bold')\n",
    "        ax1.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # 2. Feature Importance\n",
    "        if 'feature_importance' in feature_results:\n",
    "            ax2 = fig.add_subplot(gs[0, 2])\n",
    "            feat_imp = feature_results['feature_importance'].head(6)\n",
    "            ax2.barh(feat_imp['feature'], feat_imp['importance'], color='forestgreen', alpha=0.7)\n",
    "            ax2.set_xlabel('Importance')\n",
    "            ax2.set_title('Top Features', fontweight='bold')\n",
    "            ax2.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # 3. Transaction Cost Sensitivity\n",
    "        if 'transaction_sensitivity' in sensitivity_results:\n",
    "            ax3 = fig.add_subplot(gs[1, 0])\n",
    "            trans_sens = sensitivity_results['transaction_sensitivity']\n",
    "            ax3.plot(trans_sens['cost_multiplier'], trans_sens['prediction_earnings'], \n",
    "                    marker='o', label='Prediction', color='orangered')\n",
    "            ax3.plot(trans_sens['cost_multiplier'], trans_sens['baseline_earnings'], \n",
    "                    marker='s', label='Baseline', color='steelblue')\n",
    "            ax3.set_xlabel('Cost Multiplier')\n",
    "            ax3.set_ylabel('Net Earnings ($)')\n",
    "            ax3.set_title('Transaction Cost Sensitivity', fontweight='bold')\n",
    "            ax3.legend()\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "            ax3.axvline(x=1.0, color='black', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # 4. Storage Cost Sensitivity\n",
    "        if 'storage_sensitivity' in sensitivity_results:\n",
    "            ax4 = fig.add_subplot(gs[1, 1])\n",
    "            stor_sens = sensitivity_results['storage_sensitivity']\n",
    "            ax4.plot(stor_sens['cost_multiplier'], stor_sens['prediction_earnings'], \n",
    "                    marker='o', label='Prediction', color='orangered')\n",
    "            ax4.plot(stor_sens['cost_multiplier'], stor_sens['baseline_earnings'], \n",
    "                    marker='s', label='Baseline', color='steelblue')\n",
    "            ax4.set_xlabel('Cost Multiplier')\n",
    "            ax4.set_ylabel('Net Earnings ($)')\n",
    "            ax4.set_title('Storage Cost Sensitivity', fontweight='bold')\n",
    "            ax4.legend()\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "            ax4.axvline(x=1.0, color='black', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # 5. Statistical Comparison\n",
    "        if 'comparisons' in stat_results and stat_results['comparisons'] is not None and len(stat_results['comparisons']) > 0:\n",
    "            ax5 = fig.add_subplot(gs[1, 2])\n",
    "            comp = stat_results['comparisons']\n",
    "            colors_sig = ['green' if s else 'gray' for s in comp['significant']]\n",
    "            ax5.barh(comp['strategy'], comp['total_earnings_diff'], color=colors_sig, alpha=0.7)\n",
    "            ax5.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "            ax5.set_xlabel('Earnings Advantage ($)')\n",
    "            ax5.set_title('vs Best Baseline', fontweight='bold')\n",
    "            ax5.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # 6. Summary Stats Table\n",
    "        ax6 = fig.add_subplot(gs[2, :])\n",
    "        ax6.axis('off')\n",
    "        \n",
    "        summary_text = f\"\"\"\n",
    "        SUMMARY STATISTICS\n",
    "        \n",
    "        Best Overall: {best_overall['strategy']} (${best_overall['net_earnings']:,.2f})\n",
    "        Best Baseline: {best_baseline['strategy'] if best_baseline is not None else 'N/A'}\n",
    "        Best Prediction: {best_prediction['strategy'] if best_prediction is not None else 'N/A'}\n",
    "        \n",
    "        Prediction Advantage: ${earnings_diff:+,.2f} ({pct_diff:+.1f}%)\n",
    "        Statistical Significance: {'YES' if summary['statistical_significance'] else 'NO'}\n",
    "        \"\"\"\n",
    "        \n",
    "        ax6.text(0.1, 0.5, summary_text, fontsize=12, verticalalignment='center', \n",
    "                family='monospace')\n",
    "        \n",
    "        plt.suptitle(f'Trading Strategy Analysis Dashboard\\n{CURRENT_COMMODITY.upper()} - {MODEL_VERSION}', \n",
    "                    fontsize=16, fontweight='bold', y=0.98)\n",
    "        \n",
    "        dashboard_path = f'{VOLUME_PATH}/dashboard_{CURRENT_COMMODITY}_{MODEL_VERSION}.png'\n",
    "        plt.savefig(dashboard_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✓ Saved: {dashboard_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"\\n✓ Report complete for {MODEL_VERSION}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"✓ {CURRENT_COMMODITY.upper()} COMPLETE\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Cross-Model and Cross-Commodity Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATING CROSS-MODEL AND CROSS-COMMODITY SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(all_summaries) > 0:\n",
    "    summary_df = pd.DataFrame(all_summaries)\n",
    "    \n",
    "    print(f\"\\nTotal combinations analyzed: {len(summary_df)}\")\n",
    "    print(f\"  Commodities: {summary_df['commodity'].nunique()}\")\n",
    "    print(f\"  Model versions: {summary_df['model_version'].nunique()}\")\n",
    "    \n",
    "    # Save summary\n",
    "    summary_csv = f'{VOLUME_PATH}/all_models_summary.csv'\n",
    "    summary_df.to_csv(summary_csv, index=False)\n",
    "    print(f\"\\n✓ Saved: {summary_csv}\")\n",
    "    \n",
    "    # Display key findings\n",
    "    print(\"\\nTOP PERFORMERS:\")\n",
    "    print(\"-\" * 80)\n",
    "    top_10 = summary_df.nlargest(10, 'best_prediction_earnings')\n",
    "    for _, row in top_10.iterrows():\n",
    "        print(f\"{row['commodity']:10s} {row['model_version']:25s} ${row['best_prediction_earnings']:>12,.2f}  ({row['prediction_advantage_pct']:+6.1f}%)\")\n",
    "    \n",
    "    print(\"\\n✓ All reports and visualizations complete\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No summaries generated\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VISUALIZATION AND REPORTING COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAll reports and dashboards saved to: {VOLUME_PATH}\")\n",
    "print(\"\\n✓ Block 08 complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6443ca90-ccd2-484a-9ffa-f7153d71f026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NOTEBOOK 09: THREE-SCENARIO FOCUSED ANALYSIS (MULTI-COMMODITY)\n",
    "# ============================================================================\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Block 09: Moving Average vs Predictions vs Immediate Sale Analysis\n",
    "# MAGIC \n",
    "# MAGIC This analysis focuses on three key scenarios for each commodity:\n",
    "# MAGIC 1. Moving Average Baseline (no predictions)\n",
    "# MAGIC 2. Moving Average with Predictions\n",
    "# MAGIC 3. Immediate Sale (simple equal batches)\n",
    "# MAGIC \n",
    "# MAGIC Key Question: **Do predictions add value to the moving average strategy?**\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BLOCK 09: THREE-SCENARIO COMPARATIVE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Configuration\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Commodities to analyze\n",
    "COMMODITIES = ['coffee', 'sugar']\n",
    "BASE_PATH = '/Volumes/commodity/silver/trading_agent_volume'\n",
    "\n",
    "# Store results across all commodities\n",
    "all_commodity_summaries = []\n",
    "\n",
    "# Color scheme for consistency\n",
    "COLORS = {'Immediate Sale': '#3498db', 'MA Baseline': '#e74c3c', 'MA + Predictions': '#2ecc71'}\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Analysis Functions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def bootstrap_metric(strategy_name, detailed_results, metric='net_earnings', n_bootstrap=1000, seed=42):\n",
    "    \"\"\"\n",
    "    Calculate bootstrap confidence intervals for a strategy's performance metric.\n",
    "    \n",
    "    This resamples trades with replacement to estimate the distribution of outcomes.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Get the detailed results for this strategy\n",
    "    # detailed_results is a dict with strategy names as keys\n",
    "    if strategy_name not in detailed_results:\n",
    "        return None, None, None\n",
    "    \n",
    "    strat_data = detailed_results[strategy_name]\n",
    "    \n",
    "    if 'trades' not in strat_data or len(strat_data['trades']) == 0:\n",
    "        return None, None, None\n",
    "    \n",
    "    trades_df = pd.DataFrame(strat_data['trades'])\n",
    "    \n",
    "    if len(trades_df) == 0:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Bootstrap resampling\n",
    "    bootstrap_samples = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        # Resample trades with replacement\n",
    "        sample_trades = trades_df.sample(n=len(trades_df), replace=True)\n",
    "        \n",
    "        # Calculate metric for this bootstrap sample\n",
    "        if metric == 'net_earnings':\n",
    "            # Recalculate net earnings from the resampled trades\n",
    "            # Trade structure: 'revenue', 'transaction_cost', 'net_revenue'\n",
    "            # Note: storage costs are tracked separately in the simulation\n",
    "            gross_revenue = sample_trades['revenue'].sum()\n",
    "            transaction_costs = sample_trades['transaction_cost'].sum()\n",
    "            # We can't recalculate storage costs from trades alone, so use net_revenue\n",
    "            sample_metric = sample_trades['net_revenue'].sum()\n",
    "        elif metric == 'total_revenue':\n",
    "            # Total revenue before any costs\n",
    "            sample_metric = sample_trades['revenue'].sum()\n",
    "        elif metric == 'avg_sale_price':\n",
    "            # Trade structure: 'price' (in cents/lb), 'amount' (in tons)\n",
    "            sample_metric = (sample_trades['amount'] * sample_trades['price']).sum() / sample_trades['amount'].sum()\n",
    "        else:\n",
    "            sample_metric = sample_trades[metric].mean()\n",
    "        \n",
    "        bootstrap_samples.append(sample_metric)\n",
    "    \n",
    "    # Calculate confidence intervals\n",
    "    ci_lower = np.percentile(bootstrap_samples, 2.5)\n",
    "    ci_upper = np.percentile(bootstrap_samples, 97.5)\n",
    "    mean_estimate = np.mean(bootstrap_samples)\n",
    "    \n",
    "    return mean_estimate, ci_lower, ci_upper\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Loop Through Each Commodity\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "for COMMODITY in COMMODITIES:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ANALYZING: {COMMODITY.upper()}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # ====================================================================\n",
    "        # 1. Load Results from Prior Runs\n",
    "        # ====================================================================\n",
    "        print(f\"\\n📊 Loading results for {COMMODITY.upper()}...\")\n",
    "        \n",
    "        # Load main results\n",
    "        results_path = f'{BASE_PATH}/results_{COMMODITY}.csv'\n",
    "        results_df = pd.read_csv(results_path)\n",
    "        \n",
    "        # Load detailed results (contains transaction-level data)\n",
    "        detailed_path = f'{BASE_PATH}/results_detailed_{COMMODITY}.pkl'\n",
    "        with open(detailed_path, 'rb') as f:\n",
    "            detailed_results = pickle.load(f)\n",
    "        \n",
    "        print(f\"✓ Loaded {len(results_df)} strategy results\")\n",
    "        print(f\"✓ Loaded detailed results for {len(detailed_results)} strategies\")\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"⚠️  Could not load results for {COMMODITY}: {e}\")\n",
    "        print(f\"   Skipping {COMMODITY}...\")\n",
    "        continue\n",
    "    \n",
    "    # ====================================================================\n",
    "    # 2. Extract the Three Scenarios\n",
    "    # ====================================================================\n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(\"EXTRACTING THREE SCENARIOS\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    \n",
    "    # First, show all available strategies\n",
    "    print(f\"\\nAvailable strategies in {COMMODITY} results:\")\n",
    "    for i, strat in enumerate(results_df['strategy'].unique(), 1):\n",
    "        earnings = results_df[results_df['strategy'] == strat]['net_earnings'].values[0]\n",
    "        print(f\"  {i}. {strat} (${earnings:,.0f})\")\n",
    "    \n",
    "    # Define the three scenarios with EXACT strategy names from the notebook\n",
    "    # Based on notebook code:\n",
    "    # - ImmediateSaleStrategy() → \"Immediate Sale\"\n",
    "    # - MovingAverageStrategy() → \"Moving Average\"\n",
    "    # - MovingAveragePredictive() → \"Moving Average Predictive\"\n",
    "    \n",
    "    SCENARIO_NAMES = {\n",
    "        'Immediate Sale': 'Immediate Sale',\n",
    "        'MA Baseline': 'Moving Average',\n",
    "        'MA + Predictions': 'Moving Average Predictive'\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nSearching for three key scenarios...\")\n",
    "    \n",
    "    # Extract relevant strategies\n",
    "    three_scenarios = []\n",
    "    \n",
    "    for label, exact_name in SCENARIO_NAMES.items():\n",
    "        # Find exact match\n",
    "        match = results_df[results_df['strategy'] == exact_name]\n",
    "        \n",
    "        if len(match) > 0:\n",
    "            best = match.iloc[0]\n",
    "            three_scenarios.append({\n",
    "                'label': label,\n",
    "                'strategy_name': best['strategy'],\n",
    "                'data': best.to_dict()\n",
    "            })\n",
    "            print(f\"✓ Found {label}: {best['strategy']}\")\n",
    "        else:\n",
    "            print(f\"⚠️  No match found for {label} (expected: '{exact_name}')\")\n",
    "    \n",
    "    if len(three_scenarios) < 3:\n",
    "        print(f\"\\n⚠️  Only found {len(three_scenarios)}/3 scenarios for {COMMODITY}\")\n",
    "        print(f\"   Available strategies don't match expected patterns.\")\n",
    "        print(f\"   Please check strategy names above and update SCENARIO_PATTERNS.\")\n",
    "        print(f\"   Skipping {COMMODITY}...\")\n",
    "        continue\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    comparison_df = pd.DataFrame([s['data'] for s in three_scenarios])\n",
    "    comparison_df['label'] = [s['label'] for s in three_scenarios]\n",
    "    \n",
    "    print(f\"\\n✓ Extracted {len(comparison_df)} scenarios for analysis\")\n",
    "    \n",
    "    # ====================================================================\n",
    "    # 3. Statistical Testing: Do Predictions Add Value?\n",
    "    # ====================================================================\n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(\"STATISTICAL ANALYSIS: PREDICTIONS VALUE-ADD\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    \n",
    "    # Extract MA baseline and MA prediction results\n",
    "    ma_baseline = comparison_df[comparison_df['label'] == 'MA Baseline'].iloc[0]\n",
    "    ma_prediction = comparison_df[comparison_df['label'] == 'MA + Predictions'].iloc[0]\n",
    "    immediate_sale = comparison_df[comparison_df['label'] == 'Immediate Sale'].iloc[0]\n",
    "    \n",
    "    # Calculate key differences\n",
    "    earnings_diff = ma_prediction['net_earnings'] - ma_baseline['net_earnings']\n",
    "    earnings_pct = (earnings_diff / ma_baseline['net_earnings']) * 100\n",
    "    \n",
    "    revenue_diff = ma_prediction['total_revenue'] - ma_baseline['total_revenue']\n",
    "    revenue_pct = (revenue_diff / ma_baseline['total_revenue']) * 100\n",
    "    \n",
    "    price_diff = ma_prediction['avg_sale_price'] - ma_baseline['avg_sale_price']\n",
    "    price_pct = (price_diff / ma_baseline['avg_sale_price']) * 100\n",
    "    \n",
    "    print(f\"\\n1. NET EARNINGS COMPARISON (After Costs)\")\n",
    "    print(f\"   MA Baseline:      ${ma_baseline['net_earnings']:,.2f}\")\n",
    "    print(f\"   MA + Predictions: ${ma_prediction['net_earnings']:,.2f}\")\n",
    "    print(f\"   Difference:       ${earnings_diff:,.2f} ({earnings_pct:+.2f}%)\")\n",
    "    print(f\"   Status:           {'✓ PREDICTIONS ADD VALUE' if earnings_diff > 0 else '✗ PREDICTIONS REDUCE VALUE'}\")\n",
    "    \n",
    "    print(f\"\\n2. TOTAL REVENUE COMPARISON (Before Costs)\")\n",
    "    print(f\"   MA Baseline:      ${ma_baseline['total_revenue']:,.2f}\")\n",
    "    print(f\"   MA + Predictions: ${ma_prediction['total_revenue']:,.2f}\")\n",
    "    print(f\"   Difference:       ${revenue_diff:,.2f} ({revenue_pct:+.2f}%)\")\n",
    "    print(f\"   Status:           {'✓ PREDICTIONS ADD VALUE' if revenue_diff > 0 else '✗ PREDICTIONS REDUCE VALUE'}\")\n",
    "    \n",
    "    # Identify if there's a disconnect\n",
    "    if (earnings_diff > 0 and revenue_diff < 0) or (earnings_diff < 0 and revenue_diff > 0):\n",
    "        print(f\"\\n   ⚠️  IMPORTANT: Net and gross revenue show OPPOSITE results!\")\n",
    "        print(f\"      This suggests cost assumptions are driving the difference.\")\n",
    "    \n",
    "    print(f\"\\n3. AVERAGE PRICE COMPARISON\")\n",
    "    print(f\"   MA Baseline:      ${ma_baseline['avg_sale_price']:.2f}\")\n",
    "    print(f\"   MA + Predictions: ${ma_prediction['avg_sale_price']:.2f}\")\n",
    "    print(f\"   Difference:       ${price_diff:.2f} ({price_pct:+.2f}%)\")\n",
    "    \n",
    "    print(f\"\\n4. COST ANALYSIS\")\n",
    "    print(f\"   MA Baseline:\")\n",
    "    print(f\"      Storage costs:     ${ma_baseline['storage_costs']:,.2f}\")\n",
    "    print(f\"      Transaction costs: ${ma_baseline['transaction_costs']:,.2f}\")\n",
    "    print(f\"      Total costs:       ${ma_baseline['total_costs']:,.2f}\")\n",
    "    print(f\"   MA + Predictions:\")\n",
    "    print(f\"      Storage costs:     ${ma_prediction['storage_costs']:,.2f}\")\n",
    "    print(f\"      Transaction costs: ${ma_prediction['transaction_costs']:,.2f}\")\n",
    "    print(f\"      Total costs:       ${ma_prediction['total_costs']:,.2f}\")\n",
    "    print(f\"   Cost difference:      ${ma_prediction['total_costs'] - ma_baseline['total_costs']:+,.2f}\")\n",
    "    \n",
    "    print(f\"\\n5. TRADING BEHAVIOR\")\n",
    "    print(f\"   MA Baseline trades:      {ma_baseline['n_trades']:.0f}\")\n",
    "    print(f\"   MA + Predictions trades: {ma_prediction['n_trades']:.0f}\")\n",
    "    print(f\"   Immediate Sale trades:       {immediate_sale['n_trades']:.0f}\")\n",
    "    \n",
    "    # Calculate metrics vs batch sale baseline\n",
    "    immediate_sale_earnings_diff = ma_prediction['net_earnings'] - immediate_sale['net_earnings']\n",
    "    immediate_sale_earnings_pct = (immediate_sale_earnings_diff / immediate_sale['net_earnings']) * 100\n",
    "    \n",
    "    print(f\"\\n6. COMPARISON TO IMMEDIATE SALE (Simplest Strategy)\")\n",
    "    print(f\"   Immediate Sale:       ${immediate_sale['net_earnings']:,.2f}\")\n",
    "    print(f\"   MA Baseline:      ${ma_baseline['net_earnings']:,.2f} ({(ma_baseline['net_earnings']/immediate_sale['net_earnings']-1)*100:+.1f}%)\")\n",
    "    print(f\"   MA + Predictions: ${ma_prediction['net_earnings']:,.2f} ({batch_earnings_pct:+.1f}%)\")\n",
    "    \n",
    "    # ====================================================================\n",
    "    # 4. Bootstrap Confidence Intervals\n",
    "    # ====================================================================\n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(\"BOOTSTRAP CONFIDENCE INTERVALS (95%)\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    print(\"Resampling trades 1,000 times to assess outcome uncertainty...\\n\")\n",
    "    \n",
    "    # Calculate bootstrap CIs for all three scenarios (both net earnings and total revenue)\n",
    "    ci_results = {}\n",
    "    for scenario in three_scenarios:\n",
    "        label = scenario['label']\n",
    "        strategy_name = scenario['strategy_name']\n",
    "        \n",
    "        # Net earnings\n",
    "        mean_earnings, ci_low_earnings, ci_high_earnings = bootstrap_metric(strategy_name, detailed_results, 'net_earnings')\n",
    "        \n",
    "        # Total revenue\n",
    "        mean_revenue, ci_low_revenue, ci_high_revenue = bootstrap_metric(strategy_name, detailed_results, 'total_revenue')\n",
    "        \n",
    "        if mean_earnings is not None:\n",
    "            ci_results[label] = {\n",
    "                'mean_earnings': mean_earnings,\n",
    "                'ci_low_earnings': ci_low_earnings,\n",
    "                'ci_high_earnings': ci_high_earnings,\n",
    "                'ci_width_earnings': ci_high_earnings - ci_low_earnings,\n",
    "                'mean_revenue': mean_revenue,\n",
    "                'ci_low_revenue': ci_low_revenue,\n",
    "                'ci_high_revenue': ci_high_revenue,\n",
    "                'ci_width_revenue': ci_high_revenue - ci_low_revenue\n",
    "            }\n",
    "            \n",
    "            print(f\"{label}:\")\n",
    "            print(f\"  Net Earnings:  ${mean_earnings:,.2f}  [${ci_low_earnings:,.2f}, ${ci_high_earnings:,.2f}]\")\n",
    "            print(f\"  Total Revenue: ${mean_revenue:,.2f}  [${ci_low_revenue:,.2f}, ${ci_high_revenue:,.2f}]\")\n",
    "            print(f\"  CI Width: ${ci_high_earnings - ci_low_earnings:,.2f} (net) / ${ci_high_revenue - ci_low_revenue:,.2f} (gross)\\n\")\n",
    "    \n",
    "    # Check if CIs overlap\n",
    "    ci_overlap_net = None\n",
    "    ci_overlap_revenue = None\n",
    "    if 'MA Baseline' in ci_results and 'MA + Predictions' in ci_results:\n",
    "        baseline_ci = ci_results['MA Baseline']\n",
    "        prediction_ci = ci_results['MA + Predictions']\n",
    "        \n",
    "        # Check net earnings overlap\n",
    "        ci_overlap_net = not (baseline_ci['ci_high_earnings'] < prediction_ci['ci_low_earnings'] or \n",
    "                              prediction_ci['ci_high_earnings'] < baseline_ci['ci_low_earnings'])\n",
    "        \n",
    "        # Check total revenue overlap\n",
    "        ci_overlap_revenue = not (baseline_ci['ci_high_revenue'] < prediction_ci['ci_low_revenue'] or \n",
    "                                  prediction_ci['ci_high_revenue'] < baseline_ci['ci_low_revenue'])\n",
    "        \n",
    "        print(\"CONFIDENCE INTERVAL OVERLAP ANALYSIS:\")\n",
    "        print(\"\\n  Net Earnings:\")\n",
    "        if ci_overlap_net:\n",
    "            print(\"     ⚠️  Confidence intervals OVERLAP\")\n",
    "            print(\"     → Difference may not be statistically significant\")\n",
    "        else:\n",
    "            print(\"     ✓ Confidence intervals DO NOT overlap\")\n",
    "            print(\"     → Difference is likely statistically significant\")\n",
    "        \n",
    "        print(\"\\n  Total Revenue:\")\n",
    "        if ci_overlap_revenue:\n",
    "            print(\"     ⚠️  Confidence intervals OVERLAP\")\n",
    "            print(\"     → Difference may not be statistically significant\")\n",
    "        else:\n",
    "            print(\"     ✓ Confidence intervals DO NOT overlap\")\n",
    "            print(\"     → Difference is likely statistically significant\")\n",
    "    \n",
    "    # ====================================================================\n",
    "    # 5. Trade-by-Trade Analysis\n",
    "    # ====================================================================\n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(\"TRADE-BY-TRADE COMPARISON\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    \n",
    "    # Extract trade details for MA baseline and MA prediction\n",
    "    trades_analysis = {}\n",
    "    \n",
    "    for scenario in three_scenarios:\n",
    "        label = scenario['label']\n",
    "        strategy_name = scenario['strategy_name']\n",
    "        \n",
    "        # Get from detailed results dictionary\n",
    "        if strategy_name in detailed_results:\n",
    "            result = detailed_results[strategy_name]\n",
    "            if 'trades' in result and len(result['trades']) > 0:\n",
    "                trades_df = pd.DataFrame(result['trades'])\n",
    "                trades_analysis[label] = trades_df\n",
    "    \n",
    "    # Compare trade distributions\n",
    "    t_stat, p_value, cohens_d = None, None, None\n",
    "    if 'MA Baseline' in trades_analysis and 'MA + Predictions' in trades_analysis:\n",
    "        baseline_trades = trades_analysis['MA Baseline']\n",
    "        prediction_trades = trades_analysis['MA + Predictions']\n",
    "        \n",
    "        # Calculate days held if we have the data\n",
    "        if 'day' in baseline_trades.columns:\n",
    "            # Estimate days held from cumulative days (rough approximation)\n",
    "            baseline_trades['days_held'] = baseline_trades['day'].diff().fillna(0)\n",
    "            prediction_trades['days_held'] = prediction_trades['day'].diff().fillna(0)\n",
    "        \n",
    "        print(f\"\\nTRADE STATISTICS:\")\n",
    "        print(f\"\\nMA Baseline:\")\n",
    "        print(f\"  Total trades: {len(baseline_trades)}\")\n",
    "        print(f\"  Avg sale price: ${baseline_trades['price'].mean():.2f}\")\n",
    "        print(f\"  Std sale price: ${baseline_trades['price'].std():.2f}\")\n",
    "        print(f\"  Avg quantity: {baseline_trades['amount'].mean():.2f} tons\")\n",
    "        if 'days_held' in baseline_trades.columns:\n",
    "            print(f\"  Avg days between trades: {baseline_trades['days_held'].mean():.1f}\")\n",
    "        \n",
    "        print(f\"\\nMA + Predictions:\")\n",
    "        print(f\"  Total trades: {len(prediction_trades)}\")\n",
    "        print(f\"  Avg sale price: ${prediction_trades['price'].mean():.2f}\")\n",
    "        print(f\"  Std sale price: ${prediction_trades['price'].std():.2f}\")\n",
    "        print(f\"  Avg quantity: {prediction_trades['amount'].mean():.2f} tons\")\n",
    "        if 'days_held' in prediction_trades.columns:\n",
    "            print(f\"  Avg days between trades: {prediction_trades['days_held'].mean():.1f}\")\n",
    "        \n",
    "        # Statistical test on sale prices\n",
    "        t_stat, p_value = stats.ttest_ind(prediction_trades['price'], \n",
    "                                           baseline_trades['price'])\n",
    "        \n",
    "        print(f\"\\nT-TEST ON SALE PRICES:\")\n",
    "        print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "        print(f\"  p-value: {p_value:.4f}\")\n",
    "        print(f\"  Result: {'✓ Significantly different' if p_value < 0.05 else '✗ Not significantly different'} (α=0.05)\")\n",
    "        \n",
    "        # Effect size (Cohen's d)\n",
    "        pooled_std = np.sqrt((baseline_trades['price'].std()**2 + \n",
    "                             prediction_trades['price'].std()**2) / 2)\n",
    "        cohens_d = (prediction_trades['price'].mean() - \n",
    "                    baseline_trades['price'].mean()) / pooled_std\n",
    "        \n",
    "        print(f\"  Cohen's d: {cohens_d:.4f}\")\n",
    "        \n",
    "        # Interpret effect size\n",
    "        if abs(cohens_d) < 0.2:\n",
    "            effect = \"negligible\"\n",
    "        elif abs(cohens_d) < 0.5:\n",
    "            effect = \"small\"\n",
    "        elif abs(cohens_d) < 0.8:\n",
    "            effect = \"medium\"\n",
    "        else:\n",
    "            effect = \"large\"\n",
    "        \n",
    "        print(f\"  Effect size: {effect}\")\n",
    "    \n",
    "    # ====================================================================\n",
    "    # 6. Visualization: Comprehensive Dashboard\n",
    "    # ====================================================================\n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(\"GENERATING VISUALIZATIONS\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    \n",
    "    # Create comprehensive figure\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    fig.suptitle(f'{COMMODITY.title()} - Three-Scenario Trading Strategy Comparison', \n",
    "                 fontsize=20, fontweight='bold', y=0.995)\n",
    "    \n",
    "    # ====================================================================\n",
    "    # PLOT 1: Earnings Comparison\n",
    "    # ====================================================================\n",
    "    ax1 = plt.subplot(2, 4, 1)\n",
    "    \n",
    "    scenarios_sorted = comparison_df.sort_values('net_earnings', ascending=True)\n",
    "    bars = ax1.barh(range(len(scenarios_sorted)), scenarios_sorted['net_earnings'], \n",
    "                    color=[COLORS[label] for label in scenarios_sorted['label']])\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (idx, row) in enumerate(scenarios_sorted.iterrows()):\n",
    "        ax1.text(row['net_earnings'], i, f\" ${row['net_earnings']:,.0f}\", \n",
    "                va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax1.set_yticks(range(len(scenarios_sorted)))\n",
    "    ax1.set_yticklabels(scenarios_sorted['label'], fontsize=11)\n",
    "    ax1.set_xlabel('Net Earnings ($)', fontsize=11, fontweight='bold')\n",
    "    ax1.set_title('Net Earnings Comparison', fontsize=13, fontweight='bold', pad=10)\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # ====================================================================\n",
    "    # PLOT 2: Net Earnings vs Total Revenue Comparison\n",
    "    # ====================================================================\n",
    "    ax2 = plt.subplot(2, 4, 2)\n",
    "    \n",
    "    x = np.arange(len(comparison_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax2.bar(x - width/2, comparison_df['net_earnings'], width,\n",
    "                    label='Net Earnings', color='steelblue', alpha=0.8)\n",
    "    bars2 = ax2.bar(x + width/2, comparison_df['total_revenue'], width,\n",
    "                    label='Total Revenue', color='orange', alpha=0.8)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (idx, row) in enumerate(comparison_df.iterrows()):\n",
    "        ax2.text(i - width/2, row['net_earnings'], f\"${row['net_earnings']/1000:.0f}k\",\n",
    "                ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "        ax2.text(i + width/2, row['total_revenue'], f\"${row['total_revenue']/1000:.0f}k\",\n",
    "                ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "    \n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(comparison_df['label'], fontsize=11, rotation=15, ha='right')\n",
    "    ax2.set_ylabel('Dollar Amount ($)', fontsize=11, fontweight='bold')\n",
    "    ax2.set_title('Net vs Gross Revenue', fontsize=13, fontweight='bold', pad=10)\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # ====================================================================\n",
    "    # PLOT 3: Average Sale Price\n",
    "    # ====================================================================\n",
    "    ax3 = plt.subplot(2, 4, 3)\n",
    "    \n",
    "    bars = ax3.bar(range(len(comparison_df)), comparison_df['avg_sale_price'],\n",
    "                   color=[COLORS[label] for label in comparison_df['label']])\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (idx, row) in enumerate(comparison_df.iterrows()):\n",
    "        ax3.text(i, row['avg_sale_price'], f\"${row['avg_sale_price']:.0f}\", \n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax3.set_xticks(range(len(comparison_df)))\n",
    "    ax3.set_xticklabels(comparison_df['label'], fontsize=11, rotation=15, ha='right')\n",
    "    ax3.set_ylabel('Average Sale Price ($)', fontsize=11, fontweight='bold')\n",
    "    ax3.set_title('Average Sale Price', fontsize=13, fontweight='bold', pad=10)\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # ====================================================================\n",
    "    # PLOT 4: Number of Trades\n",
    "    # ====================================================================\n",
    "    ax4 = plt.subplot(2, 4, 4)\n",
    "    \n",
    "    bars = ax4.bar(range(len(comparison_df)), comparison_df['n_trades'],\n",
    "                   color=[COLORS[label] for label in comparison_df['label']])\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (idx, row) in enumerate(comparison_df.iterrows()):\n",
    "        ax4.text(i, row['n_trades'], f\"{row['n_trades']:.0f}\", \n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax4.set_xticks(range(len(comparison_df)))\n",
    "    ax4.set_xticklabels(comparison_df['label'], fontsize=11, rotation=15, ha='right')\n",
    "    ax4.set_ylabel('Number of Trades', fontsize=11, fontweight='bold')\n",
    "    ax4.set_title('Trading Activity', fontsize=13, fontweight='bold', pad=10)\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # ====================================================================\n",
    "    # PLOT 5: Sale Price Distribution (if trade data available)\n",
    "    # ====================================================================\n",
    "    ax5 = plt.subplot(2, 4, 5)\n",
    "    \n",
    "    if len(trades_analysis) > 0:\n",
    "        for label, trades_df in trades_analysis.items():\n",
    "            if len(trades_df) > 0:\n",
    "                ax5.hist(trades_df['price'], bins=20, alpha=0.6, \n",
    "                        label=label, color=COLORS[label], edgecolor='black')\n",
    "        \n",
    "        ax5.set_xlabel('Sale Price (cents/lb)', fontsize=11, fontweight='bold')\n",
    "        ax5.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "        ax5.set_title('Sale Price Distribution', fontsize=13, fontweight='bold', pad=10)\n",
    "        ax5.legend(fontsize=10)\n",
    "        ax5.grid(axis='y', alpha=0.3)\n",
    "    else:\n",
    "        ax5.text(0.5, 0.5, 'Trade-level data\\nnot available', \n",
    "                ha='center', va='center', transform=ax5.transAxes, fontsize=12)\n",
    "        ax5.set_title('Sale Price Distribution', fontsize=13, fontweight='bold', pad=10)\n",
    "    \n",
    "    # ====================================================================\n",
    "    # PLOT 6: Days Between Trades Distribution\n",
    "    # ====================================================================\n",
    "    ax6 = plt.subplot(2, 4, 6)\n",
    "    \n",
    "    if len(trades_analysis) > 0:\n",
    "        has_data = False\n",
    "        for label, trades_df in trades_analysis.items():\n",
    "            if len(trades_df) > 0 and 'day' in trades_df.columns:\n",
    "                # Calculate days between consecutive trades\n",
    "                days_between = trades_df['day'].diff().dropna()\n",
    "                if len(days_between) > 0:\n",
    "                    ax6.hist(days_between, bins=20, alpha=0.6, \n",
    "                            label=label, color=COLORS[label], edgecolor='black')\n",
    "                    has_data = True\n",
    "        \n",
    "        if has_data:\n",
    "            ax6.set_xlabel('Days Between Trades', fontsize=11, fontweight='bold')\n",
    "            ax6.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "            ax6.set_title('Trading Frequency Distribution', fontsize=13, fontweight='bold', pad=10)\n",
    "            ax6.legend(fontsize=10)\n",
    "            ax6.grid(axis='y', alpha=0.3)\n",
    "        else:\n",
    "            ax6.text(0.5, 0.5, 'Trade timing data\\nnot available', \n",
    "                    ha='center', va='center', transform=ax6.transAxes, fontsize=12)\n",
    "            ax6.set_title('Trading Frequency Distribution', fontsize=13, fontweight='bold', pad=10)\n",
    "    else:\n",
    "        ax6.text(0.5, 0.5, 'Trade-level data\\nnot available', \n",
    "                ha='center', va='center', transform=ax6.transAxes, fontsize=12)\n",
    "        ax6.set_title('Trading Frequency Distribution', fontsize=13, fontweight='bold', pad=10)\n",
    "    \n",
    "    # ====================================================================\n",
    "    # PLOT 7: Cumulative Earnings (if trade data available)\n",
    "    # ====================================================================\n",
    "    ax7 = plt.subplot(2, 4, 7)\n",
    "    \n",
    "    if len(trades_analysis) > 0:\n",
    "        for label, trades_df in trades_analysis.items():\n",
    "            if len(trades_df) > 0 and 'date' in trades_df.columns:\n",
    "                # Sort by date and calculate cumulative net revenue\n",
    "                trades_sorted = trades_df.sort_values('date').copy()\n",
    "                trades_sorted['cumulative_net'] = trades_sorted['net_revenue'].cumsum()\n",
    "                \n",
    "                ax7.plot(range(len(trades_sorted)), trades_sorted['cumulative_net'], \n",
    "                        marker='o', markersize=4, label=label, color=COLORS[label], linewidth=2)\n",
    "        \n",
    "        ax7.set_xlabel('Trade Number', fontsize=11, fontweight='bold')\n",
    "        ax7.set_ylabel('Cumulative Net Earnings ($)', fontsize=11, fontweight='bold')\n",
    "        ax7.set_title('Cumulative Earnings Over Time', fontsize=13, fontweight='bold', pad=10)\n",
    "        ax7.legend(fontsize=10)\n",
    "        ax7.grid(alpha=0.3)\n",
    "    else:\n",
    "        ax7.text(0.5, 0.5, 'Trade-level data\\nnot available', \n",
    "                ha='center', va='center', transform=ax7.transAxes, fontsize=12)\n",
    "        ax7.set_title('Cumulative Earnings Over Time', fontsize=13, fontweight='bold', pad=10)\n",
    "    \n",
    "    # ====================================================================\n",
    "    # PLOT 8: Summary Statistics Table\n",
    "    # ====================================================================\n",
    "    ax8 = plt.subplot(2, 4, 8)\n",
    "    ax8.axis('off')\n",
    "    \n",
    "    # Prepare summary data\n",
    "    summary_data = []\n",
    "    for _, row in comparison_df.iterrows():\n",
    "        summary_data.append([\n",
    "            row['label'],\n",
    "            f\"${row['total_revenue']:,.0f}\",\n",
    "            f\"${row['net_earnings']:,.0f}\",\n",
    "            f\"${row['avg_sale_price']:.0f}\",\n",
    "            f\"{row['n_trades']:.0f}\",\n",
    "            f\"${row['storage_costs']:,.0f}\"\n",
    "        ])\n",
    "    \n",
    "    # Create table\n",
    "    table = ax8.table(cellText=summary_data,\n",
    "                     colLabels=['Strategy', 'Gross\\nRevenue', 'Net\\nEarnings', \n",
    "                               'Avg\\nPrice', 'Trades', 'Storage\\nCosts'],\n",
    "                     cellLoc='center',\n",
    "                     loc='center',\n",
    "                     colWidths=[0.20, 0.18, 0.18, 0.12, 0.10, 0.18])\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1, 2.5)\n",
    "    \n",
    "    # Style the header\n",
    "    for i in range(6):\n",
    "        cell = table[(0, i)]\n",
    "        cell.set_facecolor('#2c3e50')\n",
    "        cell.set_text_props(weight='bold', color='white', fontsize=9)\n",
    "    \n",
    "    # Style the rows\n",
    "    for i in range(1, len(summary_data) + 1):\n",
    "        for j in range(6):\n",
    "            cell = table[(i, j)]\n",
    "            if i % 2 == 0:\n",
    "                cell.set_facecolor('#ecf0f1')\n",
    "            # Highlight best values\n",
    "            if j == 1:  # Gross revenue column\n",
    "                if summary_data[i-1][1] == max([row[1] for row in summary_data]):\n",
    "                    cell.set_facecolor('#d5f4e6')\n",
    "                    cell.set_text_props(weight='bold')\n",
    "            elif j == 2:  # Net earnings column\n",
    "                if summary_data[i-1][2] == max([row[2] for row in summary_data]):\n",
    "                    cell.set_facecolor('#d5f4e6')\n",
    "                    cell.set_text_props(weight='bold')\n",
    "    \n",
    "    ax8.set_title('Summary Statistics', fontsize=13, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "    \n",
    "    # Save figure\n",
    "    output_path = f'{BASE_PATH}/three_scenario_analysis_{COMMODITY}.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"\\n✓ Saved comprehensive dashboard: {output_path}\")\n",
    "    \n",
    "    # Display in notebook\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # ====================================================================\n",
    "    # 7. Detailed Statistical Report\n",
    "    # ====================================================================\n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(\"FINAL STATISTICAL REPORT\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    \n",
    "    print(f\"\\nCOMMODITY: {COMMODITY.upper()}\")\n",
    "    print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(\"SCENARIO PERFORMANCE SUMMARY\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    \n",
    "    for _, row in comparison_df.sort_values('net_earnings', ascending=False).iterrows():\n",
    "        print(f\"\\n{row['label']}:\")\n",
    "        print(f\"  Strategy: {row['strategy']}\")\n",
    "        print(f\"  Net Earnings: ${row['net_earnings']:,.2f}\")\n",
    "        print(f\"  Average Sale Price: ${row['avg_sale_price']:.2f}\")\n",
    "        print(f\"  Number of Trades: {row['n_trades']:.0f}\")\n",
    "        print(f\"  Total Storage Costs: ${row['storage_costs']:,.0f}\")\n",
    "        print(f\"  Total Transaction Costs: ${row['transaction_costs']:,.0f}\")\n",
    "    \n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(\"KEY FINDINGS: DO PREDICTIONS ADD VALUE?\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    \n",
    "    # Calculate key metrics\n",
    "    ma_baseline_earnings = ma_baseline['net_earnings']\n",
    "    ma_prediction_earnings = ma_prediction['net_earnings']\n",
    "    earnings_advantage = ma_prediction_earnings - ma_baseline_earnings\n",
    "    pct_advantage = (earnings_advantage / ma_baseline_earnings) * 100\n",
    "    \n",
    "    ma_baseline_revenue = ma_baseline['total_revenue']\n",
    "    ma_prediction_revenue = ma_prediction['total_revenue']\n",
    "    revenue_advantage = ma_prediction_revenue - ma_baseline_revenue\n",
    "    revenue_pct_advantage = (revenue_advantage / ma_baseline_revenue) * 100\n",
    "    \n",
    "    print(f\"\\n1. NET EARNINGS IMPROVEMENT (After Costs)\")\n",
    "    print(f\"   Moving Average Baseline: ${ma_baseline_earnings:,.2f}\")\n",
    "    print(f\"   Moving Average + Predictions: ${ma_prediction_earnings:,.2f}\")\n",
    "    print(f\"   Absolute Difference: ${earnings_advantage:,.2f}\")\n",
    "    print(f\"   Percentage Improvement: {pct_advantage:+.2f}%\")\n",
    "    \n",
    "    if earnings_advantage > 0:\n",
    "        print(f\"   ✓ VERDICT: Predictions ADD value on net earnings\")\n",
    "        print(f\"     Predictions improve net earnings by {pct_advantage:.1f}%\")\n",
    "    else:\n",
    "        print(f\"   ✗ VERDICT: Predictions DO NOT add value on net earnings\")\n",
    "        print(f\"     Predictions reduce net earnings by {abs(pct_advantage):.1f}%\")\n",
    "    \n",
    "    print(f\"\\n2. TOTAL REVENUE IMPROVEMENT (Before Costs)\")\n",
    "    print(f\"   Moving Average Baseline: ${ma_baseline_revenue:,.2f}\")\n",
    "    print(f\"   Moving Average + Predictions: ${ma_prediction_revenue:,.2f}\")\n",
    "    print(f\"   Absolute Difference: ${revenue_advantage:,.2f}\")\n",
    "    print(f\"   Percentage Improvement: {revenue_pct_advantage:+.2f}%\")\n",
    "    \n",
    "    if revenue_advantage > 0:\n",
    "        print(f\"   ✓ VERDICT: Predictions ADD value on total revenue\")\n",
    "        print(f\"     Predictions improve total revenue by {revenue_pct_advantage:.1f}%\")\n",
    "    else:\n",
    "        print(f\"   ✗ VERDICT: Predictions DO NOT add value on total revenue\")\n",
    "        print(f\"     Predictions reduce total revenue by {abs(revenue_pct_advantage):.1f}%\")\n",
    "    \n",
    "    # Check for disconnect between metrics\n",
    "    if (earnings_advantage > 0 and revenue_advantage < 0) or (earnings_advantage < 0 and revenue_advantage > 0):\n",
    "        print(f\"\\n   ⚠️  CRITICAL INSIGHT: Net and gross show OPPOSITE results!\")\n",
    "        print(f\"      This means cost assumptions are driving the difference.\")\n",
    "        if revenue_advantage > 0 and earnings_advantage < 0:\n",
    "            print(f\"      → Predictions generate MORE revenue but HIGHER costs\")\n",
    "            cost_diff = ma_prediction['total_costs'] - ma_baseline['total_costs']\n",
    "            print(f\"      → Extra costs: ${cost_diff:,.2f} (storage: ${ma_prediction['storage_costs'] - ma_baseline['storage_costs']:,.2f})\")\n",
    "        elif revenue_advantage < 0 and earnings_advantage > 0:\n",
    "            print(f\"      → Predictions generate LESS revenue but LOWER costs\")\n",
    "            cost_diff = ma_baseline['total_costs'] - ma_prediction['total_costs']\n",
    "            print(f\"      → Cost savings: ${cost_diff:,.2f}\")\n",
    "    \n",
    "    print(f\"\\n3. STATISTICAL SIGNIFICANCE\")\n",
    "    if 'MA Baseline' in ci_results and 'MA + Predictions' in ci_results:\n",
    "        baseline_ci = ci_results['MA Baseline']\n",
    "        prediction_ci = ci_results['MA + Predictions']\n",
    "        \n",
    "        print(f\"   Net Earnings:\")\n",
    "        if ci_overlap_net:\n",
    "            print(f\"      ⚠️  95% Confidence intervals overlap\")\n",
    "            print(f\"         Baseline CI: [${baseline_ci['ci_low_earnings']:,.0f}, ${baseline_ci['ci_high_earnings']:,.0f}]\")\n",
    "            print(f\"         Prediction CI: [${prediction_ci['ci_low_earnings']:,.0f}, ${prediction_ci['ci_high_earnings']:,.0f}]\")\n",
    "            print(f\"         → Difference may not be statistically significant\")\n",
    "        else:\n",
    "            print(f\"      ✓ 95% Confidence intervals DO NOT overlap\")\n",
    "            print(f\"         Baseline CI: [${baseline_ci['ci_low_earnings']:,.0f}, ${baseline_ci['ci_high_earnings']:,.0f}]\")\n",
    "            print(f\"         Prediction CI: [${prediction_ci['ci_low_earnings']:,.0f}, ${prediction_ci['ci_high_earnings']:,.0f}]\")\n",
    "            print(f\"         → Difference is statistically significant\")\n",
    "        \n",
    "        print(f\"\\n   Total Revenue:\")\n",
    "        if ci_overlap_revenue:\n",
    "            print(f\"      ⚠️  95% Confidence intervals overlap\")\n",
    "            print(f\"         Baseline CI: [${baseline_ci['ci_low_revenue']:,.0f}, ${baseline_ci['ci_high_revenue']:,.0f}]\")\n",
    "            print(f\"         Prediction CI: [${prediction_ci['ci_low_revenue']:,.0f}, ${prediction_ci['ci_high_revenue']:,.0f}]\")\n",
    "            print(f\"         → Difference may not be statistically significant\")\n",
    "        else:\n",
    "            print(f\"      ✓ 95% Confidence intervals DO NOT overlap\")\n",
    "            print(f\"         Baseline CI: [${baseline_ci['ci_low_revenue']:,.0f}, ${baseline_ci['ci_high_revenue']:,.0f}]\")\n",
    "            print(f\"         Prediction CI: [${prediction_ci['ci_low_revenue']:,.0f}, ${prediction_ci['ci_high_revenue']:,.0f}]\")\n",
    "            print(f\"         → Difference is statistically significant\")\n",
    "    \n",
    "    print(f\"\\n4. TRADING BEHAVIOR\")\n",
    "    print(f\"   MA Baseline: {ma_baseline['n_trades']:.0f} trades\")\n",
    "    print(f\"   MA + Predictions: {ma_prediction['n_trades']:.0f} trades\")\n",
    "    trade_diff = ma_prediction['n_trades'] - ma_baseline['n_trades']\n",
    "    print(f\"   Difference: {trade_diff:+.0f} trades\")\n",
    "    \n",
    "    if trade_diff > 0:\n",
    "        print(f\"   → Predictions lead to MORE trading activity\")\n",
    "    elif trade_diff < 0:\n",
    "        print(f\"   → Predictions lead to LESS trading activity\")\n",
    "    else:\n",
    "        print(f\"   → No change in trading frequency\")\n",
    "    \n",
    "    print(f\"\\n5. COMPARISON TO IMMEDIATE SALE\")\n",
    "    immediate_sale_earnings = immediate_sale['net_earnings']\n",
    "    immediate_sale_revenue = immediate_sale['total_revenue']\n",
    "    ma_baseline_vs_imm_net = ((ma_baseline_earnings / immediate_sale_earnings) - 1) * 100\n",
    "    ma_prediction_vs_imm_net = ((ma_prediction_earnings / immediate_sale_earnings) - 1) * 100\n",
    "    ma_baseline_vs_imm_rev = ((ma_baseline_revenue / immediate_sale_revenue) - 1) * 100\n",
    "    ma_prediction_vs_imm_rev = ((ma_prediction_revenue / immediate_sale_revenue) - 1) * 100\n",
    "    \n",
    "    print(f\"   Net Earnings:\")\n",
    "    print(f\"      Immediate Sale: ${immediate_sale_earnings:,.2f}\")\n",
    "    print(f\"      MA Baseline: {ma_baseline_vs_imm_net:+.1f}%\")\n",
    "    print(f\"      MA + Predictions: {ma_prediction_vs_imm_net:+.1f}%\")\n",
    "    \n",
    "    print(f\"   Total Revenue:\")\n",
    "    print(f\"      Immediate Sale: ${immediate_sale_revenue:,.2f}\")\n",
    "    print(f\"      MA Baseline: {ma_baseline_vs_imm_rev:+.1f}%\")\n",
    "    print(f\"      MA + Predictions: {ma_prediction_vs_imm_rev:+.1f}%\")\n",
    "    \n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(\"RECOMMENDATION\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    \n",
    "    # Generate recommendation based on all analysis\n",
    "    # Consider both net earnings and total revenue\n",
    "    if (earnings_advantage > 0 and pct_advantage > 5) or (revenue_advantage > 0 and revenue_pct_advantage > 5):\n",
    "        if not ci_overlap_net:\n",
    "            recommendation = \"STRONG: Use predictions\"\n",
    "            print(\"\\n✓ STRONG RECOMMENDATION: Use predictions\")\n",
    "            print(\"  Rationale:\")\n",
    "            if earnings_advantage > 0 and pct_advantage > 5:\n",
    "                print(f\"  • Predictions significantly improve net earnings ({pct_advantage:.1f}%)\")\n",
    "            if revenue_advantage > 0 and revenue_pct_advantage > 5:\n",
    "                print(f\"  • Predictions significantly improve total revenue ({revenue_pct_advantage:.1f}%)\")\n",
    "            print(\"  • Difference is statistically significant\")\n",
    "            print(\"  • Added complexity is justified by performance gain\")\n",
    "        else:\n",
    "            recommendation = \"WEAK: Use predictions\"\n",
    "            print(\"\\n⚠️  WEAK RECOMMENDATION: Use predictions\")\n",
    "            print(\"  Rationale:\")\n",
    "            if earnings_advantage > 0:\n",
    "                print(f\"  • Predictions show improved net earnings ({pct_advantage:.1f}%)\")\n",
    "            if revenue_advantage > 0:\n",
    "                print(f\"  • Predictions show improved total revenue ({revenue_pct_advantage:.1f}%)\")\n",
    "            print(\"  • However, confidence intervals overlap\")\n",
    "            print(\"  • Consider gathering more data for conclusive results\")\n",
    "    elif (earnings_advantage > 0 and pct_advantage > 0) or (revenue_advantage > 0 and revenue_pct_advantage > 0):\n",
    "        recommendation = \"CONDITIONAL: Consider predictions\"\n",
    "        print(\"\\n⚠️  CONDITIONAL RECOMMENDATION: Consider predictions\")\n",
    "        print(\"  Rationale:\")\n",
    "        if earnings_advantage > 0:\n",
    "            print(f\"  • Predictions show modest improvement in net earnings ({pct_advantage:.1f}%)\")\n",
    "        if revenue_advantage > 0:\n",
    "            print(f\"  • Predictions show modest improvement in total revenue ({revenue_pct_advantage:.1f}%)\")\n",
    "        if earnings_advantage < 0 and revenue_advantage > 0:\n",
    "            print(f\"  • ⚠️  Net earnings DECREASE but total revenue INCREASES\")\n",
    "            print(f\"  • This suggests cost assumptions may be too conservative\")\n",
    "        print(\"  • Benefit may not justify added complexity\")\n",
    "        print(\"  • Stick with baseline MA unless marginal gains matter\")\n",
    "    else:\n",
    "        recommendation = \"DO NOT use predictions\"\n",
    "        print(\"\\n✗ RECOMMENDATION: Do NOT use predictions\")\n",
    "        print(\"  Rationale:\")\n",
    "        print(\"  • Predictions reduce earnings\")\n",
    "        print(\"  • Baseline moving average performs better\")\n",
    "        print(\"  • Added complexity is not justified\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ANALYSIS COMPLETE - {COMMODITY.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # ====================================================================\n",
    "    # 8. Save Analysis Results\n",
    "    # ====================================================================\n",
    "    \n",
    "    # Save the three-scenario comparison\n",
    "    comparison_output = {\n",
    "        'commodity': COMMODITY,\n",
    "        'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'scenarios': comparison_df.to_dict('records'),\n",
    "        'ma_baseline_earnings': ma_baseline_earnings,\n",
    "        'ma_prediction_earnings': ma_prediction_earnings,\n",
    "        'earnings_advantage': earnings_advantage,\n",
    "        'pct_advantage': pct_advantage,\n",
    "        'ma_baseline_revenue': ma_baseline_revenue,\n",
    "        'ma_prediction_revenue': ma_prediction_revenue,\n",
    "        'revenue_advantage': revenue_advantage,\n",
    "        'revenue_pct_advantage': revenue_pct_advantage,\n",
    "        'confidence_intervals': ci_results if len(ci_results) > 0 else None,\n",
    "        'ci_overlap_net': ci_overlap_net,\n",
    "        'ci_overlap_revenue': ci_overlap_revenue,\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'cohens_d': cohens_d,\n",
    "        'recommendation': recommendation\n",
    "    }\n",
    "    \n",
    "    # Save as pickle\n",
    "    pkl_path = f'{BASE_PATH}/three_scenario_analysis_{COMMODITY}.pkl'\n",
    "    with open(pkl_path, 'wb') as f:\n",
    "        pickle.dump(comparison_output, f)\n",
    "    print(f\"✓ Saved analysis results: {pkl_path}\")\n",
    "    \n",
    "    # Save as CSV for easy viewing\n",
    "    csv_path = f'{BASE_PATH}/three_scenario_comparison_{COMMODITY}.csv'\n",
    "    comparison_df.to_csv(csv_path, index=False)\n",
    "    print(f\"✓ Saved comparison table: {csv_path}\")\n",
    "    \n",
    "    # Save detailed report as text\n",
    "    report_path = f'{BASE_PATH}/three_scenario_report_{COMMODITY}.txt'\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"THREE-SCENARIO TRADING STRATEGY ANALYSIS\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        f.write(f\"Commodity: {COMMODITY.upper()}\\n\")\n",
    "        f.write(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        f.write(\"SCENARIOS ANALYZED:\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        for _, row in comparison_df.iterrows():\n",
    "            f.write(f\"\\n{row['label']}:\\n\")\n",
    "            f.write(f\"  Net Earnings: ${row['net_earnings']:,.2f}\\n\")\n",
    "            f.write(f\"  Avg Sale Price: ${row['avg_sale_price']:.2f}\\n\")\n",
    "            f.write(f\"  Trades: {row['n_trades']:.0f}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        f.write(\"KEY FINDING: DO PREDICTIONS ADD VALUE?\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        f.write(f\"Earnings Advantage: ${earnings_advantage:,.2f} ({pct_advantage:+.2f}%)\\n\")\n",
    "        \n",
    "        if earnings_advantage > 0:\n",
    "            f.write(\"\\n✓ VERDICT: Predictions ADD value\\n\")\n",
    "        else:\n",
    "            f.write(\"\\n✗ VERDICT: Predictions DO NOT add value\\n\")\n",
    "        \n",
    "        f.write(f\"\\nRecommendation: {recommendation}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    print(f\"✓ Saved detailed report: {report_path}\")\n",
    "    \n",
    "    # Add to cross-commodity summary\n",
    "    all_commodity_summaries.append({\n",
    "        'commodity': COMMODITY,\n",
    "        'ma_baseline_earnings': ma_baseline_earnings,\n",
    "        'ma_prediction_earnings': ma_prediction_earnings,\n",
    "        'immediate_sale_earnings': immediate_sale_earnings,\n",
    "        'earnings_advantage': earnings_advantage,\n",
    "        'pct_advantage': pct_advantage,\n",
    "        'ma_baseline_revenue': ma_baseline_revenue,\n",
    "        'ma_prediction_revenue': ma_prediction_revenue,\n",
    "        'immediate_sale_revenue': immediate_sale_revenue,\n",
    "        'revenue_advantage': revenue_advantage,\n",
    "        'revenue_pct_advantage': revenue_pct_advantage,\n",
    "        'ci_overlap_net': ci_overlap_net,\n",
    "        'ci_overlap_revenue': ci_overlap_revenue,\n",
    "        'p_value': p_value,\n",
    "        'cohens_d': cohens_d,\n",
    "        'recommendation': recommendation\n",
    "    })\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Cross-Commodity Comparison\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "if len(all_commodity_summaries) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CROSS-COMMODITY COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    cross_df = pd.DataFrame(all_commodity_summaries)\n",
    "    \n",
    "    print(\"\\nSummary across all commodities:\")\n",
    "    print(\"\\nCommodity Performance:\")\n",
    "    for _, row in cross_df.iterrows():\n",
    "        print(f\"\\n{row['commodity'].upper()}:\")\n",
    "        print(f\"  MA Baseline:     ${row['ma_baseline_earnings']:,.2f}\")\n",
    "        print(f\"  MA + Predictions: ${row['ma_prediction_earnings']:,.2f}\")\n",
    "        print(f\"  Advantage:        ${row['earnings_advantage']:,.2f} ({row['pct_advantage']:+.1f}%)\")\n",
    "        print(f\"  Recommendation:   {row['recommendation']}\")\n",
    "    \n",
    "    # Save cross-commodity comparison\n",
    "    cross_csv_path = f\"{BASE_PATH}/cross_commodity_three_scenario_summary.csv\"\n",
    "    cross_df.to_csv(cross_csv_path, index=False)\n",
    "    print(f\"\\n✓ Saved: {cross_csv_path}\")\n",
    "    \n",
    "    # Create cross-commodity visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    fig.suptitle('Cross-Commodity Comparison: Prediction Value-Add', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Earnings comparison\n",
    "    ax1 = axes[0]\n",
    "    x = np.arange(len(cross_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1.bar(x - width/2, cross_df['ma_baseline_earnings'], width, \n",
    "            label='MA Baseline', color='#e74c3c', alpha=0.8)\n",
    "    ax1.bar(x + width/2, cross_df['ma_prediction_earnings'], width, \n",
    "            label='MA + Predictions', color='#2ecc71', alpha=0.8)\n",
    "    \n",
    "    ax1.set_xlabel('Commodity', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Net Earnings ($)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Earnings Comparison by Commodity', fontsize=13, fontweight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([c.title() for c in cross_df['commodity']])\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Percentage advantage\n",
    "    ax2 = axes[1]\n",
    "    colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in cross_df['pct_advantage']]\n",
    "    bars = ax2.bar(cross_df['commodity'], cross_df['pct_advantage'], color=colors, alpha=0.8)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, val) in enumerate(zip(bars, cross_df['pct_advantage'])):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, val, f'{val:+.1f}%', \n",
    "                ha='center', va='bottom' if val > 0 else 'top', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    ax2.axhline(0, color='black', linewidth=1)\n",
    "    ax2.set_xlabel('Commodity', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Prediction Advantage (%)', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Percentage Improvement with Predictions', fontsize=13, fontweight='bold')\n",
    "    ax2.set_xticklabels([c.title() for c in cross_df['commodity']])\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    cross_viz_path = f'{BASE_PATH}/cross_commodity_comparison.png'\n",
    "    plt.savefig(cross_viz_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"✓ Saved: {cross_viz_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ALL COMMODITIES ANALYZED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Commodities processed: {', '.join([s['commodity'].title() for s in all_commodity_summaries])}\")\n",
    "else:\n",
    "    print(\"\\n⚠️  WARNING: No commodities were successfully processed!\")\n",
    "    print(\"   Check for errors above.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BLOCK 09 COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAll analysis outputs saved to: {BASE_PATH}/\")\n",
    "print(\"  • Individual commodity dashboards (PNG)\")\n",
    "print(\"  • Comparison tables (CSV)\")\n",
    "print(\"  • Detailed analysis (PKL)\")\n",
    "print(\"  • Text reports (TXT)\")\n",
    "print(\"  • Cross-commodity comparison (PNG, CSV)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8181239232938016,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "trading_prediction_analysis_multi_model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
