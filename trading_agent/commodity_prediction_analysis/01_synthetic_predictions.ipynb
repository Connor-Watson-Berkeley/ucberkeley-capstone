{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f27cb649-580e-4934-bf61-fcad318141d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./00_setup_and_config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d86279a-b61f-47b3-aebb-53a07066d39d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NOTEBOOK 01A: GENERATE SYNTHETIC PREDICTIONS (MEMORY-EFFICIENT) - FIXED\n",
    "# ============================================================================\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Generate Synthetic Predictions - All Commodities (Memory-Efficient)\n",
    "# MAGIC \n",
    "# MAGIC Generates synthetic predictions using chunk-based processing for Unity Catalog tables.\n",
    "# MAGIC Creates multiple versions with different accuracy levels.\n",
    "# MAGIC \n",
    "# MAGIC **Note**: Does NOT save prices_prepared - notebooks read directly from bronze.market\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./00_setup_and_config\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "from builtins import min as builtin_min, max as builtin_max\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Configuration for synthetic predictions\n",
    "SYNTHETIC_START_DATE = '2022-01-01'  # Only generate synthetic predictions from 2022 onward\n",
    "ACCURACY_LEVELS = [0.60, 0.70, 0.80, 0.90]  # Test multiple accuracy levels\n",
    "\n",
    "print(f\"Synthetic prediction configuration:\")\n",
    "print(f\"  Synthetic start date: {SYNTHETIC_START_DATE}\")\n",
    "print(f\"  Accuracy levels: {ACCURACY_LEVELS}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Generate Predictions for All Commodities\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Load price data from bronze market table\n",
    "MARKET_TABLE = \"commodity.bronze.market\"\n",
    "print(f\"\\nLoading price data from {MARKET_TABLE}...\")\n",
    "\n",
    "# Get all market data (full history)\n",
    "market_df = spark.table(MARKET_TABLE).toPandas()\n",
    "market_df['date'] = pd.to_datetime(market_df['date'])\n",
    "\n",
    "print(f\"✓ Loaded market price data (FULL HISTORY)\")\n",
    "commodity_counts = market_df.groupby('commodity').size()\n",
    "print(f\"Available commodities:\")\n",
    "for commodity, count in commodity_counts.items():\n",
    "    print(f\"  - {commodity}: {count} rows\")\n",
    "print(f\"\\nDate range: {market_df['date'].min()} to {market_df['date'].max()}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def generate_predictions_for_accuracy(prices_df, model_version, n_runs=2000, n_horizons=14,\n",
    "                                     base_accuracy=0.65, noise_level=0.10, chunk_size=20):\n",
    "    \"\"\"\n",
    "    Generate synthetic predictions for a single accuracy level.\n",
    "    Returns a DataFrame with a model_version column.\n",
    "    \n",
    "    Parameters:\n",
    "    - chunk_size: Number of dates to process per chunk (smaller = less memory per chunk)\n",
    "    \"\"\"\n",
    "    n_dates = len(prices_df) - n_horizons\n",
    "    \n",
    "    # Collect chunks\n",
    "    all_chunks = []\n",
    "    \n",
    "    # Process in small chunks to manage memory\n",
    "    for chunk_start in range(0, n_dates, chunk_size):\n",
    "        chunk_end = builtin_min(chunk_start + chunk_size, n_dates)\n",
    "        \n",
    "        # Build predictions for this chunk\n",
    "        chunk_records = []\n",
    "        \n",
    "        for i in range(chunk_start, chunk_end):\n",
    "            current_date = prices_df.loc[i, 'date']\n",
    "            current_price = prices_df.loc[i, 'price']\n",
    "            future_prices = prices_df.loc[i+1:i+n_horizons, 'price'].values\n",
    "            \n",
    "            # Generate predictions for this date (vectorized per date)\n",
    "            random_components = current_price * (1 + np.random.normal(0, noise_level, (n_runs, n_horizons)))\n",
    "            run_biases = np.random.normal(0, noise_level * 0.3, (n_runs, 1))\n",
    "            \n",
    "            future_prices_matrix = np.tile(future_prices, (n_runs, 1))\n",
    "            predicted_prices_matrix = (base_accuracy * future_prices_matrix + \n",
    "                                      (1 - base_accuracy) * random_components)\n",
    "            predicted_prices_matrix *= (1 + run_biases)\n",
    "            \n",
    "            # Append to chunk records with model_version\n",
    "            for run_id in range(1, n_runs + 1):\n",
    "                for day_ahead in range(1, n_horizons + 1):\n",
    "                    chunk_records.append({\n",
    "                        'timestamp': current_date,\n",
    "                        'run_id': run_id,\n",
    "                        'day_ahead': day_ahead,\n",
    "                        'predicted_price': predicted_prices_matrix[run_id-1, day_ahead-1],\n",
    "                        'model_version': model_version\n",
    "                    })\n",
    "        \n",
    "        # Convert chunk to DataFrame and store\n",
    "        chunk_df = pd.DataFrame(chunk_records)\n",
    "        all_chunks.append(chunk_df)\n",
    "        \n",
    "        # Clear memory\n",
    "        del chunk_records\n",
    "        gc.collect()\n",
    "        \n",
    "        # Progress update\n",
    "        if chunk_end % 100 == 0 or chunk_end == n_dates:\n",
    "            print(f\"    Progress: {chunk_end}/{n_dates} dates... ({len(all_chunks)} chunks collected)\")\n",
    "    \n",
    "    # Concatenate all chunks\n",
    "    final_df = pd.concat(all_chunks, ignore_index=True)\n",
    "    \n",
    "    # Clear memory\n",
    "    del all_chunks\n",
    "    gc.collect()\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def process_single_commodity(commodity_name, prices_raw_pd, analysis_config, output_schema, accuracy_levels, synthetic_start_date):\n",
    "    \"\"\"\n",
    "    Process a single commodity with multiple accuracy levels.\n",
    "    Only generates synthetic predictions from synthetic_start_date onward.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PROCESSING: {commodity_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Filter and prepare price data\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(f\"\\nPreparing price data...\")\n",
    "    \n",
    "    # Filter to commodity\n",
    "    prices_full = prices_raw_pd[prices_raw_pd['commodity'].str.lower() == commodity_name.lower()].copy()\n",
    "    \n",
    "    # Extract date and close price\n",
    "    prices_full['date'] = pd.to_datetime(prices_full['date'])\n",
    "    prices_full['price'] = prices_full['close']\n",
    "    prices_full = prices_full[['date', 'price']].sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"✓ Full price history: {len(prices_full)} days\")\n",
    "    print(f\"  Date range: {prices_full['date'].min()} to {prices_full['date'].max()}\")\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Filter to synthetic_start_date+ for SYNTHETIC prediction generation\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(f\"\\nFiltering to {synthetic_start_date}+ for synthetic predictions...\")\n",
    "    prices = prices_full[prices_full['date'] >= synthetic_start_date].copy().reset_index(drop=True)\n",
    "    print(f\"✓ Filtered to {len(prices)} days for synthetic generation\")\n",
    "    print(f\"  Synthetic date range: {prices['date'].min()} to {prices['date'].max()}\")\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Generate predictions for all accuracy levels\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(f\"\\nGenerating predictions for {len(accuracy_levels)} accuracy levels...\")\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    for accuracy in accuracy_levels:\n",
    "        model_version = f\"synthetic_acc{int(accuracy*100)}\"\n",
    "        \n",
    "        print(f\"\\n  Generating {model_version}...\")\n",
    "        print(f\"    Accuracy: {accuracy:.0%}\")\n",
    "        print(f\"    Runs per date: {analysis_config['prediction_runs']}\")\n",
    "        print(f\"    Horizon: {analysis_config['forecast_horizon']} days\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        predictions_df = generate_predictions_for_accuracy(\n",
    "            prices,\n",
    "            model_version=model_version,\n",
    "            n_runs=analysis_config['prediction_runs'],\n",
    "            n_horizons=analysis_config['forecast_horizon'],\n",
    "            base_accuracy=accuracy,\n",
    "            noise_level=0.10,\n",
    "            chunk_size=20\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"    ✓ Generated {len(predictions_df):,} prediction rows in {elapsed:.1f}s\")\n",
    "        \n",
    "        all_predictions.append(predictions_df)\n",
    "        \n",
    "        # Clear memory\n",
    "        del predictions_df\n",
    "        gc.collect()\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Combine all accuracy levels\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(f\"\\nCombining all accuracy levels...\")\n",
    "    \n",
    "    combined_predictions = pd.concat(all_predictions, ignore_index=True)\n",
    "    print(f\"✓ Combined: {len(combined_predictions):,} total rows\")\n",
    "    \n",
    "    # Clear memory\n",
    "    del all_predictions\n",
    "    gc.collect()\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Save to Delta table\n",
    "    # --------------------------------------------------------------------------\n",
    "    predictions_table = f\"{output_schema}.predictions_{commodity_name.lower()}\"\n",
    "    \n",
    "    print(f\"\\nSaving to Delta table: {predictions_table}\")\n",
    "    print(f\"  Total rows: {len(combined_predictions):,}\")\n",
    "    print(f\"  Model versions: {combined_predictions['model_version'].nunique()}\")\n",
    "    \n",
    "    # Convert to Spark and save\n",
    "    predictions_spark = spark.createDataFrame(combined_predictions)\n",
    "    predictions_spark.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(predictions_table)\n",
    "    \n",
    "    print(f\"✓ Saved successfully\")\n",
    "    \n",
    "    # Verify\n",
    "    saved_count = spark.table(predictions_table).count()\n",
    "    print(f\"✓ Verified: {saved_count:,} rows in table\")\n",
    "    \n",
    "    # Clear memory\n",
    "    del combined_predictions\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\n✓ {commodity_name.upper()} COMPLETE\")\n",
    "    \n",
    "    return {\n",
    "        'commodity': commodity_name,\n",
    "        'n_dates_full': len(prices_full),\n",
    "        'n_dates_synthetic': len(prices),\n",
    "        'n_accuracy_levels': len(accuracy_levels),\n",
    "        'total_predictions': len(accuracy_levels) * (len(prices) - analysis_config['forecast_horizon']) * analysis_config['prediction_runs'] * analysis_config['forecast_horizon'],\n",
    "        'table': predictions_table\n",
    "    }\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Process all commodities with all accuracy levels\n",
    "all_results = []\n",
    "\n",
    "for commodity_name in COMMODITY_CONFIGS.keys():\n",
    "    try:\n",
    "        result = process_single_commodity(\n",
    "            commodity_name,\n",
    "            market_df,\n",
    "            ANALYSIS_CONFIG,\n",
    "            OUTPUT_SCHEMA,\n",
    "            ACCURACY_LEVELS,\n",
    "            SYNTHETIC_START_DATE\n",
    "        )\n",
    "        all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error processing {commodity_name.upper()}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(f\"   Skipping...\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATION COMPLETE - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    summary_df = pd.DataFrame(all_results)\n",
    "    print(f\"\\nSuccessfully processed {len(all_results)} commodities:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nPrediction tables created:\")\n",
    "    for table in sorted(summary_df['table'].unique()):\n",
    "        print(f\"  - {table}\")\n",
    "        model_versions = spark.table(table).select(\"model_version\").distinct().collect()\n",
    "        for mv in model_versions:\n",
    "            print(f\"      • {mv.model_version}\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No commodities were successfully processed\")\n",
    "\n",
    "print(\"\\n✓ Block 01A complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e96ecf6-7f7b-423f-9687-d649add32789",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NOTEBOOK 01B: DATA PREPARATION (Synthetic Predictions) - FIXED\n",
    "# ============================================================================\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Data Preparation - All Commodities (Synthetic)\n",
    "# MAGIC \n",
    "# MAGIC Prepares data for all configured commodities and synthetic model versions.\n",
    "# MAGIC Uses memory-efficient Spark PIVOT to handle large synthetic prediction datasets.\n",
    "# MAGIC \n",
    "# MAGIC **FIXED**: Now loads prices directly from bronze.market\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %run ./00_setup_and_config\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from pyspark.sql.functions import to_date, col\n",
    "from builtins import min as builtin_min, max as builtin_max\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Process All Commodities and Model Versions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Loop through all commodities\n",
    "for CURRENT_COMMODITY in COMMODITY_CONFIGS.keys():\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"PROCESSING: {CURRENT_COMMODITY.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Get configuration for this commodity\n",
    "    CURRENT_CONFIG = COMMODITY_CONFIGS[CURRENT_COMMODITY]\n",
    "    DATA_PATHS = get_data_paths(CURRENT_COMMODITY)\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  Harvest windows: {CURRENT_CONFIG['harvest_windows']}\")\n",
    "    print(f\"  Annual volume: {CURRENT_CONFIG['harvest_volume']} tons\")\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Load Prices from Bronze Market Table\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(f\"\\nLoading prices from commodity.bronze.market...\")\n",
    "    \n",
    "    prices = spark.table('commodity.bronze.market') \\\n",
    "        .filter(f\"commodity = '{CURRENT_COMMODITY}'\") \\\n",
    "        .select(\"date\", \"close\") \\\n",
    "        .toPandas()\n",
    "    \n",
    "    prices = prices.rename(columns={'close': 'price'})\n",
    "    prices['date'] = pd.to_datetime(prices['date'])\n",
    "    prices = prices.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"✓ Loaded {len(prices)} days of {CURRENT_COMMODITY.upper()} price data\")\n",
    "    print(f\"  Date range: {prices['date'].min()} to {prices['date'].max()}\")\n",
    "    print(f\"  Price range: ${prices['price'].min():.2f} to ${prices['price'].max():.2f}\")\n",
    "    \n",
    "    # Validation\n",
    "    assert prices['date'].is_unique, \"Duplicate dates found\"\n",
    "    assert prices['price'].isnull().sum() == 0, \"Missing prices\"\n",
    "    assert (prices['price'] > 0).all(), \"Non-positive prices\"\n",
    "    print(\"✓ Price data validated\")\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Calculate Harvest Information\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(f\"\\nCalculating harvest schedule...\")\n",
    "    \n",
    "    harvest_schedule = get_harvest_schedule(CURRENT_COMMODITY)\n",
    "    harvest_weeks = harvest_schedule['total_weeks']\n",
    "    weekly_harvest = CURRENT_CONFIG['harvest_volume'] / harvest_weeks\n",
    "    \n",
    "    print(f\"✓ Harvest schedule:\")\n",
    "    print(f\"  Total weeks: {harvest_weeks}\")\n",
    "    print(f\"  Weekly harvest: {weekly_harvest:.2f} tons\")\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Discover synthetic model versions for this commodity\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(f\"\\nDiscovering synthetic model versions...\")\n",
    "    \n",
    "    try:\n",
    "        synthetic_df = spark.table(DATA_PATHS['predictions']).select(\"model_version\").distinct()\n",
    "        model_versions = [row.model_version for row in synthetic_df.collect()]\n",
    "        print(f\"✓ Found {len(model_versions)} synthetic models: {model_versions}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  No synthetic predictions found: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # Process each synthetic model version\n",
    "    # --------------------------------------------------------------------------\n",
    "    for MODEL_VERSION in model_versions:\n",
    "        print(f\"\\n{'-' * 80}\")\n",
    "        print(f\"MODEL VERSION: {MODEL_VERSION}\")\n",
    "        print(f\"{'-' * 80}\")\n",
    "        \n",
    "        MODEL_DATA_PATHS = get_data_paths(CURRENT_COMMODITY, MODEL_VERSION)\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Load Synthetic Predictions for this model version\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nLoading synthetic predictions for {MODEL_VERSION}...\")\n",
    "        \n",
    "        predictions_table = DATA_PATHS['predictions']\n",
    "        \n",
    "        # Load predictions for this model version\n",
    "        predictions_spark = spark.table(predictions_table) \\\n",
    "            .filter(f\"model_version = '{MODEL_VERSION}'\")\n",
    "        \n",
    "        n_predictions = predictions_spark.count()\n",
    "        print(f\"✓ Loaded {n_predictions:,} prediction rows from: {predictions_table}\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Transform to Prediction Matrices using Spark PIVOT (memory-efficient)\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nTransforming to prediction matrices...\")\n",
    "        \n",
    "        # Pivot: timestamp × run_id → day_ahead columns\n",
    "        pivot_df = predictions_spark.groupBy(\"timestamp\", \"run_id\").pivot(\"day_ahead\").agg({\"predicted_price\": \"first\"})\n",
    "        \n",
    "        # Rename columns to day_1, day_2, etc.\n",
    "        day_cols = [str(i) for i in range(1, 15)]\n",
    "        new_cols = [\"timestamp\", \"run_id\"] + [f\"day_{i}\" for i in range(1, 15)]\n",
    "        \n",
    "        for old_col, new_col in zip(pivot_df.columns, new_cols):\n",
    "            pivot_df = pivot_df.withColumnRenamed(old_col, new_col)\n",
    "        \n",
    "        # Convert to Pandas\n",
    "        predictions_pivot_pd = pivot_df.toPandas()\n",
    "        predictions_pivot_pd['timestamp'] = pd.to_datetime(predictions_pivot_pd['timestamp'])\n",
    "        \n",
    "        print(f\"✓ Transformed to pivot format: {len(predictions_pivot_pd):,} rows\")\n",
    "        \n",
    "        # Save prepared predictions to Delta table\n",
    "        pivot_spark = spark.createDataFrame(predictions_pivot_pd)\n",
    "        pivot_spark.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(MODEL_DATA_PATHS['predictions_prepared'])\n",
    "        print(f\"✓ Saved: {MODEL_DATA_PATHS['predictions_prepared']}\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Convert to Prediction Matrices Dictionary\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nBuilding prediction matrices...\")\n",
    "        \n",
    "        prediction_matrices = {}\n",
    "        day_cols = [f'day_{i}' for i in range(1, 15)]\n",
    "        \n",
    "        for timestamp, group in predictions_pivot_pd.groupby('timestamp'):\n",
    "            # Each row is a run, columns are days\n",
    "            matrix = group[day_cols].values\n",
    "            prediction_matrices[pd.Timestamp(timestamp)] = matrix\n",
    "        \n",
    "        print(f\"✓ Created {len(prediction_matrices)} prediction matrices\")\n",
    "        \n",
    "        if len(prediction_matrices) > 0:\n",
    "            sample_matrix = list(prediction_matrices.values())[0]\n",
    "            print(f\"  Matrix shape: {sample_matrix.shape[0]} runs × {sample_matrix.shape[1]} days\")\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Save Prediction Matrices to Pickle\n",
    "        # ----------------------------------------------------------------------\n",
    "        print(f\"\\nSaving prediction matrices...\")\n",
    "        \n",
    "        with open(MODEL_DATA_PATHS['prediction_matrices'], 'wb') as f:\n",
    "            pickle.dump(prediction_matrices, f)\n",
    "        \n",
    "        print(f\"✓ Saved: {MODEL_DATA_PATHS['prediction_matrices']}\")\n",
    "        print(f\"✓ {MODEL_VERSION} complete\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"✓ {CURRENT_COMMODITY.upper()} COMPLETE\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL COMMODITIES PROCESSED\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n✓ Block 01B complete\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_synthetic_predictions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
