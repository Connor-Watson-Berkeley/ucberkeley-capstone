{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnostic 16: Comprehensive Grid Search - ALL 9 Strategies\n",
    "\n",
    "**Purpose:** Optimize parameters for ALL trading strategies with small farmer costs\n",
    "\n",
    "**Strategies:**\n",
    "1. ImmediateSaleStrategy\n",
    "2. EqualBatchStrategy\n",
    "3. PriceThresholdStrategy\n",
    "4. MovingAverageStrategy\n",
    "5. PriceThresholdPredictive\n",
    "6. MovingAveragePredictive\n",
    "7. ExpectedValueStrategy\n",
    "8. ConsensusStrategy\n",
    "9. RiskAdjustedStrategy\n",
    "\n",
    "**Key Configuration:**\n",
    "- Small farmer costs: 0.005% storage/day, 0.01% transaction\n",
    "- Coarse grid for initial optimization\n",
    "- All strategies imported from all_strategies_pct.py\n",
    "\n",
    "**Expected Results:**\n",
    "- Optimal parameters for each strategy\n",
    "- Clear ranking with realistic costs\n",
    "- Matched pairs showing prediction value-add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup_and_config"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Install Optuna for intelligent hyperparameter optimization\n%pip install optuna --quiet",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import importlib.util\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DIAGNOSTIC 16: OPTUNA OPTIMIZATION - 99.6% FASTER\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n1,800 trials vs 520,000 grid combinations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force fresh reload\n",
    "if 'all_strategies_pct' in sys.modules:\n",
    "    del sys.modules['all_strategies_pct']\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"all_strategies_pct\", \"all_strategies_pct.py\")\n",
    "strategies_module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(strategies_module)\n",
    "\n",
    "ImmediateSaleStrategy = strategies_module.ImmediateSaleStrategy\n",
    "EqualBatchStrategy = strategies_module.EqualBatchStrategy\n",
    "PriceThresholdStrategy = strategies_module.PriceThresholdStrategy\n",
    "MovingAverageStrategy = strategies_module.MovingAverageStrategy\n",
    "PriceThresholdPredictive = strategies_module.PriceThresholdPredictive\n",
    "MovingAveragePredictive = strategies_module.MovingAveragePredictive\n",
    "ExpectedValueStrategy = strategies_module.ExpectedValueStrategy\n",
    "ConsensusStrategy = strategies_module.ConsensusStrategy\n",
    "RiskAdjustedStrategy = strategies_module.RiskAdjustedStrategy\n",
    "\n",
    "print(\"✓ Loaded all 9 strategies from all_strategies_pct.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data with Small Farmer Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMODITY = 'coffee'\n",
    "MODEL_VERSION = 'synthetic_acc90'\n",
    "\n",
    "DATA_PATHS = get_data_paths(COMMODITY, MODEL_VERSION)\n",
    "COMMODITY_CONFIG = COMMODITY_CONFIGS[COMMODITY]\n",
    "\n",
    "# OVERRIDE: Small farmer realistic costs\n",
    "COMMODITY_CONFIG['storage_cost_pct_per_day'] = 0.005  # Was 0.025%\n",
    "COMMODITY_CONFIG['transaction_cost_pct'] = 0.01       # Was 0.25%\n",
    "\n",
    "print(\"Loading data...\")\n",
    "prices_table = get_data_paths(COMMODITY)['prices_prepared']\n",
    "prices = spark.table(prices_table).toPandas()\n",
    "prices['date'] = pd.to_datetime(prices['date'])\n",
    "\n",
    "matrices_path = DATA_PATHS['prediction_matrices']\n",
    "with open(matrices_path, 'rb') as f:\n",
    "    prediction_matrices = pickle.load(f)\n",
    "prediction_matrices = {pd.to_datetime(k): v for k, v in prediction_matrices.items()}\n",
    "\n",
    "print(f\"✓ Loaded {len(prices)} price records\")\n",
    "print(f\"✓ Loaded {len(prediction_matrices)} prediction matrices\")\n",
    "print(f\"\\nSmall Farmer Costs:\")\n",
    "print(f\"  Storage: {COMMODITY_CONFIG['storage_cost_pct_per_day']}% per day\")\n",
    "print(f\"  Transaction: {COMMODITY_CONFIG['transaction_cost_pct']}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtest Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagnosticBacktestEngine:\n",
    "    \"\"\"Backtest engine for grid search\"\"\"\n",
    "    \n",
    "    def __init__(self, prices_df, prediction_matrices, commodity_config):\n",
    "        self.prices = prices_df\n",
    "        self.prediction_matrices = prediction_matrices\n",
    "        self.config = commodity_config\n",
    "        \n",
    "    def run_backtest(self, strategy, initial_inventory=50.0):\n",
    "        inventory = initial_inventory\n",
    "        trades = []\n",
    "        total_revenue = 0\n",
    "        total_transaction_costs = 0\n",
    "        total_storage_costs = 0\n",
    "        \n",
    "        strategy.reset()\n",
    "        strategy.set_harvest_start(0)\n",
    "        \n",
    "        for day in range(len(self.prices)):\n",
    "            current_date = self.prices.iloc[day]['date']\n",
    "            current_price = self.prices.iloc[day]['price']\n",
    "            price_history = self.prices.iloc[:day+1].copy()\n",
    "            predictions = self.prediction_matrices.get(current_date, None)\n",
    "            \n",
    "            decision = strategy.decide(\n",
    "                day=day,\n",
    "                inventory=inventory,\n",
    "                current_price=current_price,\n",
    "                price_history=price_history,\n",
    "                predictions=predictions\n",
    "            )\n",
    "            \n",
    "            if decision['action'] == 'SELL' and decision['amount'] > 0:\n",
    "                amount = min(decision['amount'], inventory)\n",
    "                price_per_ton = current_price * 20\n",
    "                revenue = amount * price_per_ton\n",
    "                transaction_cost = revenue * (self.config['transaction_cost_pct'] / 100)\n",
    "                \n",
    "                total_revenue += revenue\n",
    "                total_transaction_costs += transaction_cost\n",
    "                inventory -= amount\n",
    "                \n",
    "                trades.append({\n",
    "                    'day': day,\n",
    "                    'amount': amount,\n",
    "                    'price': current_price,\n",
    "                    'revenue': revenue\n",
    "                })\n",
    "            \n",
    "            if inventory > 0:\n",
    "                avg_price = self.prices.iloc[:day+1]['price'].mean()\n",
    "                price_per_ton = avg_price * 20\n",
    "                storage_cost = inventory * price_per_ton * (self.config['storage_cost_pct_per_day'] / 100)\n",
    "                total_storage_costs += storage_cost\n",
    "        \n",
    "        net_earnings = total_revenue - total_transaction_costs - total_storage_costs\n",
    "        \n",
    "        return {\n",
    "            'net_earnings': net_earnings,\n",
    "            'total_revenue': total_revenue,\n",
    "            'num_trades': len(trades),\n",
    "            'storage_costs': total_storage_costs,\n",
    "            'final_inventory': inventory\n",
    "        }\n",
    "\n",
    "engine = DiagnosticBacktestEngine(prices, prediction_matrices, COMMODITY_CONFIG)\n",
    "print(\"✓ Backtest engine ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna Search Spaces - ALL 9 Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_search_space(trial, strategy_name):\n    '''COMPLETE parameter coverage for ALL strategies'''\n    if strategy_name == 'immediate_sale':\n        return {'min_batch_size': trial.suggest_float('min_batch_size', 3.0, 10.0), 'sale_frequency_days': trial.suggest_int('sale_frequency_days', 5, 14)}\n    \n    elif strategy_name == 'equal_batch':\n        return {'batch_size': trial.suggest_float('batch_size', 0.15, 0.30), 'frequency_days': trial.suggest_int('frequency_days', 20, 35)}\n    \n    elif strategy_name == 'price_threshold':\n        return {'threshold_pct': trial.suggest_float('threshold_pct', 0.02, 0.07), 'batch_baseline': trial.suggest_float('batch_baseline', 0.20, 0.35), 'batch_overbought_strong': trial.suggest_float('batch_overbought_strong', 0.30, 0.40), 'batch_overbought': trial.suggest_float('batch_overbought', 0.25, 0.35), 'batch_strong_trend': trial.suggest_float('batch_strong_trend', 0.15, 0.25), 'rsi_overbought': trial.suggest_int('rsi_overbought', 65, 75), 'rsi_moderate': trial.suggest_int('rsi_moderate', 60, 70), 'adx_strong': trial.suggest_int('adx_strong', 20, 30), 'cooldown_days': trial.suggest_int('cooldown_days', 5, 10), 'max_days_without_sale': trial.suggest_int('max_days_without_sale', 45, 75)}\n    \n    elif strategy_name == 'moving_average':\n        return {'ma_period': trial.suggest_int('ma_period', 20, 35), 'batch_baseline': trial.suggest_float('batch_baseline', 0.20, 0.30), 'batch_strong_momentum': trial.suggest_float('batch_strong_momentum', 0.15, 0.25), 'batch_overbought': trial.suggest_float('batch_overbought', 0.25, 0.35), 'batch_overbought_strong': trial.suggest_float('batch_overbought_strong', 0.30, 0.40), 'rsi_overbought': trial.suggest_int('rsi_overbought', 65, 75), 'rsi_min': trial.suggest_int('rsi_min', 40, 50), 'adx_strong': trial.suggest_int('adx_strong', 20, 30), 'adx_weak': trial.suggest_int('adx_weak', 15, 25), 'cooldown_days': trial.suggest_int('cooldown_days', 5, 10), 'max_days_without_sale': trial.suggest_int('max_days_without_sale', 45, 75)}\n    \n    elif strategy_name == 'price_threshold_predictive':\n        return {'threshold_pct': trial.suggest_float('threshold_pct', 0.02, 0.07), 'batch_baseline': trial.suggest_float('batch_baseline', 0.20, 0.35), 'batch_overbought_strong': trial.suggest_float('batch_overbought_strong', 0.30, 0.40), 'batch_overbought': trial.suggest_float('batch_overbought', 0.25, 0.35), 'batch_strong_trend': trial.suggest_float('batch_strong_trend', 0.15, 0.25), 'rsi_overbought': trial.suggest_int('rsi_overbought', 65, 75), 'rsi_moderate': trial.suggest_int('rsi_moderate', 60, 70), 'adx_strong': trial.suggest_int('adx_strong', 20, 30), 'cooldown_days': trial.suggest_int('cooldown_days', 5, 10), 'max_days_without_sale': trial.suggest_int('max_days_without_sale', 45, 75), 'min_net_benefit_pct': trial.suggest_float('min_net_benefit_pct', 0.3, 1.0), 'high_confidence_cv': trial.suggest_float('high_confidence_cv', 0.03, 0.08), 'scenario_shift_aggressive': trial.suggest_int('scenario_shift_aggressive', 1, 2), 'scenario_shift_conservative': trial.suggest_int('scenario_shift_conservative', 1, 2)}\n    \n    elif strategy_name == 'moving_average_predictive':\n        return {'ma_period': trial.suggest_int('ma_period', 20, 35), 'batch_baseline': trial.suggest_float('batch_baseline', 0.20, 0.30), 'batch_strong_momentum': trial.suggest_float('batch_strong_momentum', 0.15, 0.25), 'batch_overbought': trial.suggest_float('batch_overbought', 0.25, 0.35), 'batch_overbought_strong': trial.suggest_float('batch_overbought_strong', 0.30, 0.40), 'rsi_overbought': trial.suggest_int('rsi_overbought', 65, 75), 'rsi_min': trial.suggest_int('rsi_min', 40, 50), 'adx_strong': trial.suggest_int('adx_strong', 20, 30), 'adx_weak': trial.suggest_int('adx_weak', 15, 25), 'cooldown_days': trial.suggest_int('cooldown_days', 5, 10), 'max_days_without_sale': trial.suggest_int('max_days_without_sale', 45, 75), 'min_net_benefit_pct': trial.suggest_float('min_net_benefit_pct', 0.3, 1.0), 'high_confidence_cv': trial.suggest_float('high_confidence_cv', 0.03, 0.08), 'scenario_shift_aggressive': trial.suggest_int('scenario_shift_aggressive', 1, 2), 'scenario_shift_conservative': trial.suggest_int('scenario_shift_conservative', 1, 2)}\n    \n    elif strategy_name == 'expected_value':\n        return {'min_net_benefit_pct': trial.suggest_float('min_net_benefit_pct', 0.3, 1.0), 'negative_threshold_pct': trial.suggest_float('negative_threshold_pct', -0.5, -0.1), 'high_confidence_cv': trial.suggest_float('high_confidence_cv', 0.03, 0.08), 'medium_confidence_cv': trial.suggest_float('medium_confidence_cv', 0.10, 0.15), 'strong_trend_adx': trial.suggest_int('strong_trend_adx', 20, 25), 'batch_positive_confident': trial.suggest_float('batch_positive_confident', 0.0, 0.05), 'batch_positive_uncertain': trial.suggest_float('batch_positive_uncertain', 0.10, 0.20), 'batch_marginal': trial.suggest_float('batch_marginal', 0.15, 0.20), 'batch_negative_mild': trial.suggest_float('batch_negative_mild', 0.25, 0.30), 'batch_negative_strong': trial.suggest_float('batch_negative_strong', 0.35, 0.40), 'cooldown_days': trial.suggest_int('cooldown_days', 5, 7), 'baseline_batch': trial.suggest_float('baseline_batch', 0.15, 0.20), 'baseline_frequency': trial.suggest_int('baseline_frequency', 25, 30)}\n    \n    elif strategy_name == 'consensus':\n        return {'consensus_threshold': trial.suggest_float('consensus_threshold', 0.60, 0.75), 'very_strong_consensus': trial.suggest_float('very_strong_consensus', 0.80, 0.85), 'moderate_consensus': trial.suggest_float('moderate_consensus', 0.55, 0.60), 'min_return': trial.suggest_float('min_return', 0.02, 0.05), 'min_net_benefit_pct': trial.suggest_float('min_net_benefit_pct', 0.3, 0.7), 'high_confidence_cv': trial.suggest_float('high_confidence_cv', 0.03, 0.08), 'batch_strong_consensus': trial.suggest_float('batch_strong_consensus', 0.0, 0.05), 'batch_moderate': trial.suggest_float('batch_moderate', 0.10, 0.20), 'batch_weak': trial.suggest_float('batch_weak', 0.25, 0.30), 'batch_bearish': trial.suggest_float('batch_bearish', 0.35, 0.40), 'evaluation_day': trial.suggest_int('evaluation_day', 10, 14), 'cooldown_days': trial.suggest_int('cooldown_days', 5, 7)}\n    \n    elif strategy_name == 'risk_adjusted':\n        return {'min_return': trial.suggest_float('min_return', 0.02, 0.05), 'min_net_benefit_pct': trial.suggest_float('min_net_benefit_pct', 0.3, 0.7), 'max_uncertainty_low': trial.suggest_float('max_uncertainty_low', 0.03, 0.08), 'max_uncertainty_medium': trial.suggest_float('max_uncertainty_medium', 0.10, 0.20), 'max_uncertainty_high': trial.suggest_float('max_uncertainty_high', 0.25, 0.35), 'strong_trend_adx': trial.suggest_int('strong_trend_adx', 20, 25), 'batch_low_risk': trial.suggest_float('batch_low_risk', 0.0, 0.05), 'batch_medium_risk': trial.suggest_float('batch_medium_risk', 0.10, 0.15), 'batch_high_risk': trial.suggest_float('batch_high_risk', 0.25, 0.30), 'batch_very_high_risk': trial.suggest_float('batch_very_high_risk', 0.35, 0.40), 'evaluation_day': trial.suggest_int('evaluation_day', 10, 14), 'cooldown_days': trial.suggest_int('cooldown_days', 5, 7)}\n    \n    else:\n        raise ValueError(f'Unknown: {strategy_name}')\n\nprint('✓ COMPLETE search spaces - all parameters for all 9 strategies')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "def optimize_strategy(strategy_class, strategy_name, engine, n_trials=200, n_jobs=1, needs_costs=False):\n    \"\"\"\n    Optimize strategy using Optuna.\n    \n    Note: Running sequentially (n_jobs=1) to avoid any SQLite/parallelization issues.\n    Set n_jobs higher if your environment supports it.\n    \"\"\"\n    print(f'\\n{\"=\"*80}\\nOptimizing {strategy_name}: {n_trials} trials\\n{\"=\"*80}')\n    \n    # Create study with NO storage backend (pure in-memory)\n    study = optuna.create_study(\n        direction='maximize', \n        sampler=TPESampler(seed=42)\n    )\n    \n    def objective(trial):\n        params = get_search_space(trial, strategy_name)\n        if needs_costs:\n            params['storage_cost_pct_per_day'] = COMMODITY_CONFIG['storage_cost_pct_per_day']\n            params['transaction_cost_pct'] = COMMODITY_CONFIG['transaction_cost_pct']\n        try:\n            strategy = strategy_class(**params)\n            result = engine.run_backtest(strategy)\n            return result['net_earnings']\n        except Exception as e:\n            print(f\"  Trial {trial.number} failed: {e}\")\n            return -1e9\n    \n    # Run optimization (n_jobs=1 for sequential, increase if environment supports it)\n    study.optimize(objective, n_trials=n_trials, n_jobs=n_jobs, show_progress_bar=True)\n    \n    print(f'\\n✓ Best: ${study.best_value:,.2f}')\n    print(f'  Completed {len(study.trials)} trials')\n    \n    return study.best_params, study\n\nprint('✓ Optimization ready (in-memory, no SQLite)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_strategy(strategy_class, strategy_name, engine, n_trials=200, n_workers=8, needs_costs=False):\n    print(f'\\n{\"=\"*80}\\nOptimizing {strategy_name}: {n_trials} trials, {n_workers} workers\\n{\"=\"*80}')\n    storage_dir = f'/dbfs/tmp/optuna_{strategy_name}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n    os.makedirs(storage_dir, exist_ok=True)\n    storage_url = f'sqlite:///{storage_dir}/study.db'\n    study = optuna.create_study(study_name=strategy_name, storage=storage_url, direction='maximize', sampler=TPESampler(seed=42), load_if_exists=True)\n    \n    def objective(trial):\n        params = get_search_space(trial, strategy_name)\n        if needs_costs:\n            params['storage_cost_pct_per_day'] = COMMODITY_CONFIG['storage_cost_pct_per_day']\n            params['transaction_cost_pct'] = COMMODITY_CONFIG['transaction_cost_pct']\n        try:\n            strategy = strategy_class(**params)\n            result = engine.run_backtest(strategy)\n            return result['net_earnings']\n        except Exception as e:\n            return -1e9\n    \n    def run_worker(worker_id):\n        worker_study = optuna.load_study(study_name=strategy_name, storage=storage_url, sampler=TPESampler(seed=42 + worker_id))\n        worker_study.optimize(objective, n_trials=n_trials//n_workers, show_progress_bar=False)\n        return worker_id\n    \n    rdd = spark.sparkContext.parallelize(range(n_workers), n_workers)\n    rdd.map(run_worker).collect()\n    final_study = optuna.load_study(study_name=strategy_name, storage=storage_url)\n    \n    try:\n        import shutil\n        shutil.rmtree(storage_dir)\n    except:\n        pass\n    \n    print(f'Best: ${final_study.best_value:,.2f}')\n    return final_study.best_params, final_study\n\nprint('✓ Optimization ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Optimizations - ALL 9 Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "params, study = optimize_strategy(ImmediateSaleStrategy, 'immediate_sale', engine, n_trials=200, n_jobs=8, needs_costs=False)\nall_results['immediate_sale'] = (params, study.best_value)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, study = optimize_strategy(ImmediateSaleStrategy, 'immediate_sale', engine, n_trials=200, n_workers=8, needs_costs=False)\nall_results['immediate_sale'] = (params, study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "params, study = optimize_strategy(EqualBatchStrategy, 'equal_batch', engine, n_trials=200, n_jobs=8, needs_costs=False)\nall_results['equal_batch'] = (params, study.best_value)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, study = optimize_strategy(EqualBatchStrategy, 'equal_batch', engine, n_trials=200, n_workers=8, needs_costs=False)\nall_results['equal_batch'] = (params, study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "params, study = optimize_strategy(PriceThresholdStrategy, 'price_threshold', engine, n_trials=200, n_jobs=8, needs_costs=False)\nall_results['price_threshold'] = (params, study.best_value)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, study = optimize_strategy(PriceThresholdStrategy, 'price_threshold', engine, n_trials=200, n_workers=8, needs_costs=False)\nall_results['price_threshold'] = (params, study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "params, study = optimize_strategy(MovingAverageStrategy, 'moving_average', engine, n_trials=200, n_jobs=8, needs_costs=False)\nall_results['moving_average'] = (params, study.best_value)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, study = optimize_strategy(MovingAverageStrategy, 'moving_average', engine, n_trials=200, n_workers=8, needs_costs=False)\nall_results['moving_average'] = (params, study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "params, study = optimize_strategy(PriceThresholdPredictive, 'price_threshold_predictive', engine, n_trials=200, n_jobs=8, needs_costs=True)\nall_results['price_threshold_predictive'] = (params, study.best_value)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, study = optimize_strategy(PriceThresholdPredictive, 'price_threshold_predictive', engine, n_trials=200, n_workers=8, needs_costs=True)\nall_results['price_threshold_predictive'] = (params, study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "params, study = optimize_strategy(MovingAveragePredictive, 'moving_average_predictive', engine, n_trials=200, n_jobs=8, needs_costs=True)\nall_results['moving_average_predictive'] = (params, study.best_value)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, study = optimize_strategy(MovingAveragePredictive, 'moving_average_predictive', engine, n_trials=200, n_workers=8, needs_costs=True)\nall_results['moving_average_predictive'] = (params, study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "params, study = optimize_strategy(ExpectedValueStrategy, 'expected_value', engine, n_trials=200, n_jobs=8, needs_costs=True)\nall_results['expected_value'] = (params, study.best_value)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, study = optimize_strategy(ExpectedValueStrategy, 'expected_value', engine, n_trials=200, n_workers=8, needs_costs=True)\nall_results['expected_value'] = (params, study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "params, study = optimize_strategy(ConsensusStrategy, 'consensus', engine, n_trials=200, n_jobs=8, needs_costs=True)\nall_results['consensus'] = (params, study.best_value)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, study = optimize_strategy(ConsensusStrategy, 'consensus', engine, n_trials=200, n_workers=8, needs_costs=True)\nall_results['consensus'] = (params, study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "params, study = optimize_strategy(RiskAdjustedStrategy, 'risk_adjusted', engine, n_trials=200, n_jobs=8, needs_costs=True)\nall_results['risk_adjusted'] = (params, study.best_value)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, study = optimize_strategy(RiskAdjustedStrategy, 'risk_adjusted', engine, n_trials=200, n_workers=8, needs_costs=True)\nall_results['risk_adjusted'] = (params, study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"OPTUNA OPTIMIZATION COMPLETE - ALL 9 STRATEGIES\")\nprint(\"=\"*80)\n\n# Create summary\nsummary = []\nfor name, (params, best_value) in all_results.items():\n    summary.append({\n        'strategy': name,\n        'net_earnings': best_value,\n        'params': params\n    })\n\n# Sort by earnings\nsummary.sort(key=lambda x: x['net_earnings'], reverse=True)\n\n# Display ranking\nprint(\"\\nRanking by Net Earnings:\")\nprint(f\"{'Rank':<6} {'Strategy':<35} {'Net Earnings':>15}\")\nprint(\"-\" * 80)\n\nfor i, s in enumerate(summary, 1):\n    print(f\"{i:<6} {s['strategy']:<35} ${s['net_earnings']:>14,.2f}\")\n\n# Best baseline\nbaselines = ['immediate_sale', 'equal_batch', 'price_threshold', 'moving_average']\nbest_baseline = max([s for s in summary if s['strategy'] in baselines],\n                   key=lambda x: x['net_earnings'])\n\nprint(f\"\\nBest Baseline: {best_baseline['strategy']} (${best_baseline['net_earnings']:,.2f})\")\n\n# Prediction strategies vs baseline\nprint(\"\\n\" + \"=\"*80)\nprint(\"PREDICTION STRATEGIES vs BEST BASELINE\")\nprint(\"=\"*80)\n\npredictions = [s for s in summary if s['strategy'] not in baselines]\nfor s in predictions:\n    improvement = s['net_earnings'] - best_baseline['net_earnings']\n    pct = 100 * improvement / best_baseline['net_earnings']\n    marker = \"✓\" if improvement > 0 else \"✗\"\n    print(f\"{marker} {s['strategy']:<35} ${s['net_earnings']:>12,.2f} ({pct:+6.2f}%)\")\n\n# Matched pairs\nprint(\"\\n\" + \"=\"*80)\nprint(\"MATCHED PAIR ANALYSIS\")\nprint(\"=\"*80)\n\npairs = [\n    ('price_threshold', 'price_threshold_predictive'),\n    ('moving_average', 'moving_average_predictive')\n]\n\nfor base_name, pred_name in pairs:\n    base_earnings = all_results[base_name][1]\n    pred_earnings = all_results[pred_name][1]\n    improvement = pred_earnings - base_earnings\n    pct = 100 * improvement / base_earnings\n\n    print(f\"\\n{base_name.upper()}:\")\n    print(f\"  Baseline:   ${base_earnings:,.2f}\")\n    print(f\"  Predictive: ${pred_earnings:,.2f}\")\n    print(f\"  Improvement: ${improvement:,.2f} ({pct:+.2f}%)\")\n    if improvement > 0:\n        print(f\"  ✓ Predictions add value!\")\n    else:\n        print(f\"  ✗ Predictions hurt performance\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"EFFICIENCY GAIN: 1,800 trials vs 520,000 grid combinations\")\nprint(\"99.6% reduction in evaluations (289x fewer)\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\n\n# Prepare summary for JSON\njson_summary = {\n    'timestamp': datetime.now().isoformat(),\n    'commodity': COMMODITY,\n    'model_version': MODEL_VERSION,\n    'optimization': {\n        'algorithm': 'Optuna TPE',\n        'trials_per_strategy': 200,\n        'workers': 8,\n        'total_trials': len(all_results) * 200\n    },\n    'costs': {\n        'storage_pct_per_day': COMMODITY_CONFIG['storage_cost_pct_per_day'],\n        'transaction_pct': COMMODITY_CONFIG['transaction_cost_pct']\n    },\n    'strategies': {}\n}\n\nfor name, (params, best_value) in all_results.items():\n    # Remove cost params from display\n    display_params = {k: v for k, v in params.items()\n                     if k not in ['storage_cost_pct_per_day', 'transaction_cost_pct']}\n    \n    json_summary['strategies'][name] = {\n        'net_earnings': float(best_value),\n        'best_params': display_params\n    }\n\n# Save JSON\njson_path = '/Volumes/commodity/trading_agent/files/diagnostic_16_optuna_summary.json'\nwith open(json_path, 'w') as f:\n    json.dump(json_summary, f, indent=2)\n\nprint(f\"✓ Summary saved: {json_path}\")\n\n# Save full results as pickle\npkl_path = '/Volumes/commodity/trading_agent/files/diagnostic_16_optuna_results.pkl'\nwith open(pkl_path, 'wb') as f:\n    pickle.dump(all_results, f)\n\nprint(f\"✓ Full results saved: {pkl_path}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"DIAGNOSTIC 16 OPTUNA OPTIMIZATION COMPLETE\")\nprint(\"=\"*80)\nprint(f\"\\nOptimized {len(all_results)} strategies\")\nprint(f\"Total trials: {len(all_results) * 200:,}\")\nprint(f\"vs Grid search: 520,000 combinations\")\nprint(f\"Reduction: 99.6% fewer evaluations\")\nprint(f\"\\nResults saved to Volume for analysis\")"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}