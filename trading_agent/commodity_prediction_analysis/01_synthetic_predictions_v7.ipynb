{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./00_setup_and_config\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Generate Calibrated Synthetic Predictions - All Commodities\n\n**v7: Saves to volume for download**\n- **FIX**: Now saves validation pickle to `/Volumes/commodity/trading_agent/files/` for easy download\n- v6 fix: Day alignment corrected - 100% accurate shows 0% MAPE\n- Point accuracy: Median prediction has target MAPE (aligned with forecast_agent)\n- All validation details saved to persistent storage for detailed review\n\n**Accuracy levels:**\n- 100% accurate: MAPE = 0%, MAE = 0 (all predictions exactly match actuals)\n- 90% accurate: MAPE = 10%\n- 80% accurate: MAPE = 20%\n- 70% accurate: MAPE = 30%\n- 60% accurate: MAPE = 40%"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from builtins import min as builtin_min, max as builtin_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nSYNTHETIC_START_DATE = '2022-01-01'\nACCURACY_LEVELS = [1.00, 0.90, 0.80, 0.70, 0.60]\nVOLUME_PATH = \"/Volumes/commodity/trading_agent/files\"\nVALIDATION_OUTPUT_FILE = f'{VOLUME_PATH}/validation_results_full.pkl'\n\nprint(f\"Synthetic prediction configuration:\")\nprint(f\"  Synthetic start date: {SYNTHETIC_START_DATE}\")\nprint(f\"  Accuracy levels: {[f'{a:.0%}' for a in ACCURACY_LEVELS]}\")\nprint(f\"  Validation output: {VALIDATION_OUTPUT_FILE}\")\nprint(f\"\\n✓ v7 FIX: Saves to volume for download\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKET_TABLE = \"commodity.bronze.market\"\n",
    "print(f\"\\nLoading price data from {MARKET_TABLE}...\")\n",
    "\n",
    "market_df = spark.table(MARKET_TABLE).toPandas()\n",
    "market_df['date'] = pd.to_datetime(market_df['date'])\n",
    "\n",
    "print(f\"✓ Loaded market price data\")\n",
    "print(f\"Date range: {market_df['date'].min()} to {market_df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrated Prediction Generation (with future_date fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_calibrated_predictions(prices_df, model_version, target_accuracy=0.90, \n",
    "                                    n_runs=2000, n_horizons=14, chunk_size=20):\n",
    "    \"\"\"\n",
    "    Generate calibrated synthetic predictions.\n",
    "    FIXED: Now stores actual future_date for each prediction to align with validation.\n",
    "    \"\"\"\n",
    "    n_dates = len(prices_df) - n_horizons\n",
    "    target_mape = 1.0 - target_accuracy\n",
    "    \n",
    "    print(f\"    Target MAPE: {target_mape:.1%}\")\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    for chunk_start in range(0, n_dates, chunk_size):\n",
    "        chunk_end = builtin_min(chunk_start + chunk_size, n_dates)\n",
    "        chunk_records = []\n",
    "        \n",
    "        for i in range(chunk_start, chunk_end):\n",
    "            current_date = prices_df.loc[i, 'date']\n",
    "            \n",
    "            # Get actual future dates AND prices (row-based, not calendar-based)\n",
    "            future_rows = prices_df.loc[i+1:i+n_horizons]\n",
    "            future_dates = future_rows['date'].values\n",
    "            future_prices = future_rows['price'].values\n",
    "            \n",
    "            # Ensure we have exactly n_horizons entries\n",
    "            if len(future_prices) < n_horizons:\n",
    "                continue  # Skip if not enough future data\n",
    "            \n",
    "            if target_accuracy == 1.0:\n",
    "                predicted_prices_matrix = np.tile(future_prices, (n_runs, 1))\n",
    "            else:\n",
    "                sigma_lognormal = target_mape * np.sqrt(np.pi / 2)\n",
    "                log_errors = np.random.normal(0, sigma_lognormal, (n_runs, n_horizons))\n",
    "                multiplicative_errors = np.exp(log_errors)\n",
    "                future_prices_matrix = np.tile(future_prices, (n_runs, 1))\n",
    "                predicted_prices_matrix = future_prices_matrix * multiplicative_errors\n",
    "                run_biases = np.random.normal(1.0, 0.02, (n_runs, 1))\n",
    "                predicted_prices_matrix *= run_biases\n",
    "            \n",
    "            # Store predictions with actual future_date\n",
    "            for run_id in range(1, n_runs + 1):\n",
    "                for day_ahead in range(1, n_horizons + 1):\n",
    "                    chunk_records.append({\n",
    "                        'timestamp': current_date,\n",
    "                        'future_date': future_dates[day_ahead-1],  # FIXED: Store actual future date\n",
    "                        'run_id': run_id,\n",
    "                        'day_ahead': day_ahead,\n",
    "                        'predicted_price': predicted_prices_matrix[run_id-1, day_ahead-1],\n",
    "                        'model_version': model_version\n",
    "                    })\n",
    "        \n",
    "        chunk_df = pd.DataFrame(chunk_records)\n",
    "        all_chunks.append(chunk_df)\n",
    "        del chunk_records\n",
    "        gc.collect()\n",
    "        \n",
    "        if chunk_end % 100 == 0 or chunk_end == n_dates:\n",
    "            print(f\"    Progress: {chunk_end}/{n_dates} dates...\")\n",
    "    \n",
    "    final_df = pd.concat(all_chunks, ignore_index=True)\n",
    "    del all_chunks\n",
    "    gc.collect()\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_crps(actuals: np.ndarray, forecast_paths: np.ndarray) -> list:\n",
    "    \"\"\"Calculate CRPS\"\"\"\n",
    "    n_paths, horizon = forecast_paths.shape\n",
    "    crps_values = []\n",
    "    \n",
    "    for t in range(horizon):\n",
    "        if np.isnan(actuals[t]):\n",
    "            continue\n",
    "        actual = actuals[t]\n",
    "        sorted_samples = np.sort(forecast_paths[:, t])\n",
    "        term1 = np.mean(np.abs(sorted_samples - actual))\n",
    "        n = len(sorted_samples)\n",
    "        indices = np.arange(1, n + 1)\n",
    "        term2 = np.sum((2 * indices - 1) * sorted_samples) / (n ** 2) - np.mean(sorted_samples)\n",
    "        crps_values.append(term1 - 0.5 * term2)\n",
    "    \n",
    "    return crps_values\n",
    "\n",
    "\n",
    "def calculate_directional_accuracy(actuals: pd.Series, forecasts: pd.Series) -> dict:\n",
    "    \"\"\"Calculate directional accuracy\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    if len(actuals) > 1:\n",
    "        actual_direction = np.sign(actuals.diff().dropna())\n",
    "        forecast_direction = np.sign(forecasts.diff().dropna())\n",
    "        correct_direction = (actual_direction == forecast_direction).sum()\n",
    "        metrics['directional_accuracy'] = float(correct_direction / len(actual_direction) * 100)\n",
    "    \n",
    "    if len(actuals) > 1:\n",
    "        day_0_actual = actuals.iloc[0]\n",
    "        day_0_forecast = forecasts.iloc[0]\n",
    "        correct_from_day0 = sum(1 for i in range(1, len(actuals)) \n",
    "                               if (actuals.iloc[i] > day_0_actual) == (forecasts.iloc[i] > day_0_forecast))\n",
    "        metrics['directional_accuracy_from_day0'] = float(correct_from_day0 / (len(actuals) - 1) * 100)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_predictions(predictions_df, prices_df, commodity, model_version, target_accuracy, n_horizons=14):\n",
    "    \"\"\"\n",
    "    Validation using stored future_date (FIXED).\n",
    "    \"\"\"\n",
    "    print(f\"\\n  Validating predictions...\")\n",
    "    \n",
    "    # Group by timestamp, day_ahead, future_date and compute median\n",
    "    medians = predictions_df.groupby(['timestamp', 'day_ahead', 'future_date'])['predicted_price'].median().reset_index()\n",
    "    medians.columns = ['timestamp', 'day_ahead', 'future_date', 'median_pred']\n",
    "    \n",
    "    prices_df = prices_df.copy()\n",
    "    prices_df['date'] = pd.to_datetime(prices_df['date'])\n",
    "    \n",
    "    # Merge with actuals using stored future_date (FIXED)\n",
    "    results = []\n",
    "    for _, row in medians.iterrows():\n",
    "        timestamp = row['timestamp']\n",
    "        day_ahead = int(row['day_ahead'])\n",
    "        future_date = pd.to_datetime(row['future_date'])\n",
    "        median_pred = row['median_pred']\n",
    "        \n",
    "        # Use stored future_date instead of calendar calculation\n",
    "        actual_row = prices_df[prices_df['date'] == future_date]\n",
    "        \n",
    "        if len(actual_row) > 0:\n",
    "            actual_price = actual_row['price'].values[0]\n",
    "            ape = abs(median_pred - actual_price) / actual_price\n",
    "            ae = abs(median_pred - actual_price)\n",
    "            results.append({\n",
    "                'timestamp': timestamp,\n",
    "                'day_ahead': day_ahead,\n",
    "                'future_date': future_date,\n",
    "                'median_pred': median_pred,\n",
    "                'actual': actual_price,\n",
    "                'ape': ape,\n",
    "                'ae': ae\n",
    "            })\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        print(f\"    ⚠️  No matching actuals\")\n",
    "        return None\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    target_mape = 1.0 - target_accuracy\n",
    "    \n",
    "    # Overall metrics\n",
    "    overall_mae = results_df['ae'].mean()\n",
    "    overall_mape = results_df['ape'].mean()\n",
    "    \n",
    "    print(f\"\\n    Overall: MAE=${overall_mae:.2f}, MAPE={overall_mape:.1%} (target: {target_mape:.1%})\")\n",
    "    \n",
    "    # Per-horizon\n",
    "    per_horizon = results_df.groupby('day_ahead').agg({\n",
    "        'ae': ['mean', 'std'], 'ape': ['mean', 'std'], 'timestamp': 'count'\n",
    "    })\n",
    "    per_horizon.columns = ['mae_mean', 'mae_std', 'mape_mean', 'mape_std', 'n_samples']\n",
    "    \n",
    "    print(f\"\\n    Per-Horizon:\")\n",
    "    for h in sorted(per_horizon.index)[:5]:  # Show first 5\n",
    "        mape = per_horizon.loc[h, 'mape_mean']\n",
    "        status = '✓' if mape <= target_mape * 1.15 else '⚠️'\n",
    "        print(f\"      Day {h:2d}: MAPE={mape:5.1%} {status}\")\n",
    "    \n",
    "    # Directional accuracy\n",
    "    timestamps = results_df['timestamp'].unique()\n",
    "    dir_data = []\n",
    "    for ts in timestamps:\n",
    "        ts_data = results_df[results_df['timestamp'] == ts].sort_values('day_ahead')\n",
    "        if len(ts_data) >= 2:\n",
    "            dir_m = calculate_directional_accuracy(\n",
    "                pd.Series(ts_data['actual'].values),\n",
    "                pd.Series(ts_data['median_pred'].values)\n",
    "            )\n",
    "            dir_m['timestamp'] = ts\n",
    "            dir_data.append(dir_m)\n",
    "    \n",
    "    dir_df = pd.DataFrame(dir_data)\n",
    "    if len(dir_df) > 0:\n",
    "        print(f\"    Directional: {dir_df['directional_accuracy'].mean():.1f}% (day-to-day), \"\n",
    "              f\"{dir_df['directional_accuracy_from_day0'].mean():.1f}% (from day 0)\")\n",
    "    \n",
    "    # CRPS (sample)\n",
    "    sample_ts = np.random.choice(timestamps, size=min(50, len(timestamps)), replace=False)\n",
    "    crps_data = []\n",
    "    for ts in sample_ts:\n",
    "        ts_pred = predictions_df[predictions_df['timestamp'] == ts]\n",
    "        matrix = ts_pred.pivot_table(index='run_id', columns='day_ahead', values='predicted_price').values\n",
    "        actuals = results_df[results_df['timestamp'] == ts].sort_values('day_ahead')['actual'].values\n",
    "        if len(actuals) == matrix.shape[1]:\n",
    "            crps_vals = calculate_crps(actuals, matrix)\n",
    "            if crps_vals:\n",
    "                crps_data.append({'timestamp': ts, 'crps_mean': np.mean(crps_vals)})\n",
    "    \n",
    "    crps_df = pd.DataFrame(crps_data)\n",
    "    if len(crps_df) > 0:\n",
    "        print(f\"    CRPS: ${crps_df['crps_mean'].mean():.2f}\")\n",
    "    \n",
    "    # Coverage\n",
    "    intervals = predictions_df.groupby(['timestamp', 'day_ahead'])['predicted_price'].agg(\n",
    "        p10=lambda x: x.quantile(0.1), p90=lambda x: x.quantile(0.9)\n",
    "    ).reset_index()\n",
    "    val = results_df.merge(intervals, on=['timestamp', 'day_ahead'])\n",
    "    cov80 = ((val['actual'] >= val['p10']) & (val['actual'] <= val['p90'])).mean()\n",
    "    print(f\"    Coverage 80%: {cov80:.1%}\")\n",
    "    print(f\"  ✓ Validation complete\")\n",
    "    \n",
    "    return {\n",
    "        'commodity': commodity,\n",
    "        'model_version': model_version,\n",
    "        'target_accuracy': target_accuracy,\n",
    "        'target_mape': target_mape,\n",
    "        'overall_mae': float(overall_mae),\n",
    "        'overall_mape': float(overall_mape),\n",
    "        'results_df': results_df,\n",
    "        'per_horizon_metrics': per_horizon,\n",
    "        'directional_df': dir_df,\n",
    "        'crps_df': crps_df,\n",
    "        'coverage_80': float(cov80)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Commodities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_commodity(commodity_name, prices_raw_pd, analysis_config, output_schema, \n",
    "                            accuracy_levels, synthetic_start_date):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PROCESSING: {commodity_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    prices_full = prices_raw_pd[prices_raw_pd['commodity'].str.lower() == commodity_name.lower()].copy()\n",
    "    prices_full['date'] = pd.to_datetime(prices_full['date'])\n",
    "    prices_full['price'] = prices_full['close']\n",
    "    prices_full = prices_full[['date', 'price']].sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    prices = prices_full[prices_full['date'] >= synthetic_start_date].copy().reset_index(drop=True)\n",
    "    print(f\"✓ {len(prices)} days of data\")\n",
    "    \n",
    "    all_predictions = []\n",
    "    validation_data = []\n",
    "    \n",
    "    for accuracy in accuracy_levels:\n",
    "        model_version = f\"synthetic_acc{int(accuracy*100)}\"\n",
    "        print(f\"\\n  {model_version}: {accuracy:.0%} accurate\")\n",
    "        \n",
    "        predictions_df = generate_calibrated_predictions(\n",
    "            prices, model_version, accuracy,\n",
    "            analysis_config['prediction_runs'],\n",
    "            analysis_config['forecast_horizon'], 20\n",
    "        )\n",
    "        print(f\"    ✓ Generated {len(predictions_df):,} rows\")\n",
    "        \n",
    "        val_data = validate_predictions(\n",
    "            predictions_df, prices, commodity_name, model_version, \n",
    "            accuracy, analysis_config['forecast_horizon']\n",
    "        )\n",
    "        \n",
    "        if val_data:\n",
    "            validation_data.append(val_data)\n",
    "        \n",
    "        all_predictions.append(predictions_df)\n",
    "        del predictions_df\n",
    "        gc.collect()\n",
    "    \n",
    "    combined = pd.concat(all_predictions, ignore_index=True)\n",
    "    del all_predictions\n",
    "    gc.collect()\n",
    "    \n",
    "    predictions_table = f\"{output_schema}.predictions_{commodity_name.lower()}\"\n",
    "    spark.createDataFrame(combined).write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(predictions_table)\n",
    "    print(f\"\\n✓ Saved to {predictions_table}\")\n",
    "    \n",
    "    del combined\n",
    "    gc.collect()\n",
    "    \n",
    "    return {'commodity': commodity_name, 'table': predictions_table, 'validation_data': validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all\n",
    "all_results = []\n",
    "all_validation_data = {}\n",
    "\n",
    "for commodity_name in COMMODITY_CONFIGS.keys():\n",
    "    try:\n",
    "        result = process_single_commodity(\n",
    "            commodity_name, market_df, ANALYSIS_CONFIG, OUTPUT_SCHEMA,\n",
    "            ACCURACY_LEVELS, SYNTHETIC_START_DATE\n",
    "        )\n",
    "        all_results.append({'commodity': result['commodity'], 'table': result['table']})\n",
    "        all_validation_data[commodity_name] = result['validation_data']\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_output = {\n",
    "    'generation_timestamp': datetime.now(),\n",
    "    'config': {'synthetic_start_date': SYNTHETIC_START_DATE, 'accuracy_levels': ACCURACY_LEVELS},\n",
    "    'commodities': all_validation_data,\n",
    "    'summary': all_results\n",
    "}\n",
    "\n",
    "with open(VALIDATION_OUTPUT_FILE, 'wb') as f:\n",
    "    pickle.dump(validation_output, f)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Saved validation data to: {VALIDATION_OUTPUT_FILE}\")\n",
    "print(f\"  Size: {os.path.getsize(VALIDATION_OUTPUT_FILE) / (1024*1024):.1f} MB\")\n",
    "print(f\"\\n✓ COMPLETE - 100% accurate should show 0% MAPE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}