{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./00_setup_and_config\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Calibrated Synthetic Predictions - All Commodities\n",
    "\n",
    "**Enhanced with validation metrics saved to Delta table:**\n",
    "- Point accuracy: Median prediction has target MAPE (aligned with forecast_agent)\n",
    "- Distribution calibration: Prediction intervals properly calibrated\n",
    "- Includes 100% accurate scenario (perfect foresight for testing)\n",
    "- **NEW**: Validation metrics saved to `commodity_analysis.synthetic_validation_metrics`\n",
    "\n",
    "**Accuracy levels:**\n",
    "- 100% accurate: MAPE = 0%, MAE = 0 (all predictions exactly match actuals)\n",
    "- 90% accurate: MAPE = 10%\n",
    "- 80% accurate: MAPE = 20%\n",
    "- 70% accurate: MAPE = 30%\n",
    "- 60% accurate: MAPE = 40%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from builtins import min as builtin_min, max as builtin_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SYNTHETIC_START_DATE = '2022-01-01'\n",
    "ACCURACY_LEVELS = [1.00, 0.90, 0.80, 0.70, 0.60]  # 100%, 90%, 80%, 70%, 60%\n",
    "VALIDATION_METRICS_TABLE = \"commodity_analysis.synthetic_validation_metrics\"\n",
    "\n",
    "print(f\"Synthetic prediction configuration:\")\n",
    "print(f\"  Synthetic start date: {SYNTHETIC_START_DATE}\")\n",
    "print(f\"  Accuracy levels: {[f'{a:.0%}' for a in ACCURACY_LEVELS]}\")\n",
    "print(f\"  Validation metrics table: {VALIDATION_METRICS_TABLE}\")\n",
    "print(f\"\\nAccuracy definition (aligned with forecast_agent):\")\n",
    "print(f\"  - Point forecast: Median has target MAPE\")\n",
    "print(f\"  - Distribution: Calibrated prediction intervals\")\n",
    "print(f\"  - Validation: MAE, MAPE, Directional Accuracy, CRPS (saved to table)\")\n",
    "print(f\"  - 100% accurate: Perfect foresight (MAPE = 0%, MAE = 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKET_TABLE = \"commodity.bronze.market\"\n",
    "print(f\"\\nLoading price data from {MARKET_TABLE}...\")\n",
    "\n",
    "market_df = spark.table(MARKET_TABLE).toPandas()\n",
    "market_df['date'] = pd.to_datetime(market_df['date'])\n",
    "\n",
    "print(f\"✓ Loaded market price data (FULL HISTORY)\")\n",
    "commodity_counts = market_df.groupby('commodity').size()\n",
    "print(f\"Available commodities:\")\n",
    "for commodity, count in commodity_counts.items():\n",
    "    print(f\"  - {commodity}: {count} rows\")\n",
    "print(f\"\\nDate range: {market_df['date'].min()} to {market_df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrated Prediction Generation\n",
    "\n",
    "Key improvements:\n",
    "1. **Target MAPE**: Median prediction has specified MAPE\n",
    "2. **Calibrated uncertainty**: Prediction spread reflects realistic uncertainty\n",
    "3. **100% accuracy**: Perfect scenario for algorithm testing\n",
    "4. **Aligned validation**: Uses same metrics as forecast_agent (MAE, MAPE, Directional, CRPS)\n",
    "5. **Saved metrics**: Validation results saved to Delta table for later review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_calibrated_predictions(prices_df, model_version, target_accuracy=0.90, \n",
    "                                    n_runs=2000, n_horizons=14, chunk_size=20):\n",
    "    \"\"\"\n",
    "    Generate calibrated synthetic predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    - target_accuracy: 0.90 means median has 10% MAPE\n",
    "    - n_runs: Number of ensemble runs (2000)\n",
    "    - n_horizons: Forecast horizon (14 days)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with predictions having target MAPE and calibrated intervals\n",
    "    \"\"\"\n",
    "    n_dates = len(prices_df) - n_horizons\n",
    "    target_mape = 1.0 - target_accuracy  # 90% accurate = 10% MAPE\n",
    "    \n",
    "    print(f\"    Target MAPE: {target_mape:.1%}\")\n",
    "    print(f\"    Calibration: 80% interval should contain actual ~80% of time\")\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    for chunk_start in range(0, n_dates, chunk_size):\n",
    "        chunk_end = builtin_min(chunk_start + chunk_size, n_dates)\n",
    "        chunk_records = []\n",
    "        \n",
    "        for i in range(chunk_start, chunk_end):\n",
    "            current_date = prices_df.loc[i, 'date']\n",
    "            future_prices = prices_df.loc[i+1:i+n_horizons, 'price'].values\n",
    "            \n",
    "            if target_accuracy == 1.0:\n",
    "                # 100% accurate: All runs exactly match actual\n",
    "                predicted_prices_matrix = np.tile(future_prices, (n_runs, 1))\n",
    "            \n",
    "            else:\n",
    "                # Generate predictions with target MAPE\n",
    "                sigma_lognormal = target_mape * np.sqrt(np.pi / 2)\n",
    "                \n",
    "                # Generate 2000 runs with calibrated uncertainty\n",
    "                log_errors = np.random.normal(0, sigma_lognormal, (n_runs, n_horizons))\n",
    "                multiplicative_errors = np.exp(log_errors)\n",
    "                \n",
    "                future_prices_matrix = np.tile(future_prices, (n_runs, 1))\n",
    "                predicted_prices_matrix = future_prices_matrix * multiplicative_errors\n",
    "                \n",
    "                # Add small run-specific bias for additional realism (±2%)\n",
    "                run_biases = np.random.normal(1.0, 0.02, (n_runs, 1))\n",
    "                predicted_prices_matrix *= run_biases\n",
    "            \n",
    "            # Store predictions\n",
    "            for run_id in range(1, n_runs + 1):\n",
    "                for day_ahead in range(1, n_horizons + 1):\n",
    "                    chunk_records.append({\n",
    "                        'timestamp': current_date,\n",
    "                        'run_id': run_id,\n",
    "                        'day_ahead': day_ahead,\n",
    "                        'predicted_price': predicted_prices_matrix[run_id-1, day_ahead-1],\n",
    "                        'model_version': model_version\n",
    "                    })\n",
    "        \n",
    "        chunk_df = pd.DataFrame(chunk_records)\n",
    "        all_chunks.append(chunk_df)\n",
    "        \n",
    "        del chunk_records\n",
    "        gc.collect()\n",
    "        \n",
    "        if chunk_end % 100 == 0 or chunk_end == n_dates:\n",
    "            print(f\"    Progress: {chunk_end}/{n_dates} dates...\")\n",
    "    \n",
    "    final_df = pd.concat(all_chunks, ignore_index=True)\n",
    "    del all_chunks\n",
    "    gc.collect()\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Validation Functions (Aligned with forecast_agent)\n",
    "\n",
    "These functions use the exact same formulas as forecast_agent for consistency:\n",
    "- **MAE**: Mean Absolute Error (from `ground_truth/core/evaluator.py`)\n",
    "- **MAPE**: Mean Absolute Percentage Error (from `ground_truth/core/evaluator.py`)\n",
    "- **Directional Accuracy**: Day-to-day and from Day 0 (from `ground_truth/core/evaluator.py`)\n",
    "- **CRPS**: Continuous Ranked Probability Score (from `evaluate_historical_forecasts.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_crps(actuals: np.ndarray, forecast_paths: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Continuous Ranked Probability Score (CRPS).\n",
    "    (Aligned with forecast_agent/evaluate_historical_forecasts.py lines 47-92)\n",
    "    \"\"\"\n",
    "    n_paths, horizon = forecast_paths.shape\n",
    "    crps_values = []\n",
    "    \n",
    "    for t in range(horizon):\n",
    "        if np.isnan(actuals[t]):\n",
    "            continue\n",
    "        \n",
    "        actual = actuals[t]\n",
    "        forecast_samples = forecast_paths[:, t]\n",
    "        sorted_samples = np.sort(forecast_samples)\n",
    "        \n",
    "        # CRPS = E[|X - Y|] - 0.5 * E[|X - X'|]\n",
    "        term1 = np.mean(np.abs(sorted_samples - actual))\n",
    "        \n",
    "        n = len(sorted_samples)\n",
    "        indices = np.arange(1, n + 1)\n",
    "        term2 = np.sum((2 * indices - 1) * sorted_samples) / (n ** 2) - np.mean(sorted_samples)\n",
    "        \n",
    "        crps = term1 - 0.5 * term2\n",
    "        crps_values.append(crps)\n",
    "    \n",
    "    return float(np.mean(crps_values)) if crps_values else None\n",
    "\n",
    "\n",
    "def calculate_directional_accuracy(actuals: pd.Series, forecasts: pd.Series) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate directional accuracy metrics.\n",
    "    (Aligned with forecast_agent/ground_truth/core/evaluator.py lines 39-67)\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Day-to-day directional accuracy\n",
    "    if len(actuals) > 1:\n",
    "        actual_direction = np.sign(actuals.diff().dropna())\n",
    "        forecast_direction = np.sign(forecasts.diff().dropna())\n",
    "        correct_direction = (actual_direction == forecast_direction).sum()\n",
    "        metrics['directional_accuracy'] = float(correct_direction / len(actual_direction) * 100)\n",
    "    \n",
    "    # Directional accuracy from Day 0 (primary trading metric)\n",
    "    if len(actuals) > 1:\n",
    "        day_0_actual = actuals.iloc[0]\n",
    "        day_0_forecast = forecasts.iloc[0]\n",
    "        \n",
    "        correct_from_day0 = 0\n",
    "        total_from_day0 = 0\n",
    "        \n",
    "        for i in range(1, len(actuals)):\n",
    "            actual_higher = actuals.iloc[i] > day_0_actual\n",
    "            forecast_higher = forecasts.iloc[i] > day_0_forecast\n",
    "            \n",
    "            if actual_higher == forecast_higher:\n",
    "                correct_from_day0 += 1\n",
    "            total_from_day0 += 1\n",
    "        \n",
    "        if total_from_day0 > 0:\n",
    "            metrics['directional_accuracy_from_day0'] = float(correct_from_day0 / total_from_day0 * 100)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_predictions(predictions_df, prices_df, commodity, model_version, target_accuracy, n_horizons=14):\n",
    "    \"\"\"\n",
    "    Validate that generated predictions have target accuracy.\n",
    "    Returns validation metrics dictionary for saving to Delta table.\n",
    "    \"\"\"\n",
    "    print(f\"\\n  Validating predictions (forecast_agent-aligned metrics)...\")\n",
    "    \n",
    "    # Group by timestamp and day_ahead, compute median\n",
    "    medians = predictions_df.groupby(['timestamp', 'day_ahead'])['predicted_price'].median().reset_index()\n",
    "    medians.columns = ['timestamp', 'day_ahead', 'median_pred']\n",
    "    \n",
    "    # Get actual future prices\n",
    "    prices_df = prices_df.copy()\n",
    "    prices_df['date'] = pd.to_datetime(prices_df['date'])\n",
    "    \n",
    "    # Merge predictions with actuals\n",
    "    results = []\n",
    "    for _, row in medians.iterrows():\n",
    "        timestamp = row['timestamp']\n",
    "        day_ahead = int(row['day_ahead'])\n",
    "        median_pred = row['median_pred']\n",
    "        \n",
    "        future_date = timestamp + pd.Timedelta(days=day_ahead)\n",
    "        actual_row = prices_df[prices_df['date'] == future_date]\n",
    "        \n",
    "        if len(actual_row) > 0:\n",
    "            actual_price = actual_row['price'].values[0]\n",
    "            ape = abs(median_pred - actual_price) / actual_price\n",
    "            ae = abs(median_pred - actual_price)\n",
    "            results.append({\n",
    "                'timestamp': timestamp,\n",
    "                'day_ahead': day_ahead,\n",
    "                'median_pred': median_pred,\n",
    "                'actual': actual_price,\n",
    "                'ape': ape,\n",
    "                'ae': ae\n",
    "            })\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        print(f\"    ⚠️  Could not validate - no matching actuals found\")\n",
    "        return None\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    target_mape = 1.0 - target_accuracy\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_mae = results_df['ae'].mean()\n",
    "    overall_mape = results_df['ape'].mean()\n",
    "    median_ape = results_df['ape'].median()\n",
    "    pct90_ape = results_df['ape'].quantile(0.9)\n",
    "    \n",
    "    print(f\"\\n    Overall Metrics:\")\n",
    "    print(f\"      MAE:  ${overall_mae:.2f}\")\n",
    "    print(f\"      MAPE: {overall_mape:.1%} (target: {target_mape:.1%})\")\n",
    "    print(f\"      Median APE: {median_ape:.1%}\")\n",
    "    print(f\"      90th pct APE: {pct90_ape:.1%}\")\n",
    "    \n",
    "    # Per-horizon metrics\n",
    "    print(f\"\\n    Per-Horizon Metrics (each should meet target {target_mape:.1%}):\")\n",
    "    per_horizon = results_df.groupby('day_ahead').agg({\n",
    "        'ae': 'mean',\n",
    "        'ape': 'mean',\n",
    "        'timestamp': 'count'\n",
    "    }).rename(columns={'timestamp': 'count'})\n",
    "    \n",
    "    per_horizon_list = []\n",
    "    for horizon in sorted(per_horizon.index):\n",
    "        horizon_mae = per_horizon.loc[horizon, 'ae']\n",
    "        horizon_mape = per_horizon.loc[horizon, 'ape']\n",
    "        horizon_count = int(per_horizon.loc[horizon, 'count'])\n",
    "        status = '✓' if horizon_mape <= target_mape * 1.15 else '⚠️'\n",
    "        print(f\"      Day {horizon:2d}: MAE=${horizon_mae:5.2f}, MAPE={horizon_mape:5.1%} ({horizon_count:4d} samples) {status}\")\n",
    "        \n",
    "        per_horizon_list.append({\n",
    "            'day_ahead': int(horizon),\n",
    "            'mae': float(horizon_mae),\n",
    "            'mape': float(horizon_mape),\n",
    "            'n_samples': int(horizon_count)\n",
    "        })\n",
    "    \n",
    "    # Calculate directional accuracy\n",
    "    timestamps = results_df['timestamp'].unique()\n",
    "    all_dir_acc = []\n",
    "    all_dir_acc_day0 = []\n",
    "    \n",
    "    for ts in timestamps:\n",
    "        ts_data = results_df[results_df['timestamp'] == ts].sort_values('day_ahead')\n",
    "        if len(ts_data) >= 2:\n",
    "            actuals_series = pd.Series(ts_data['actual'].values)\n",
    "            forecasts_series = pd.Series(ts_data['median_pred'].values)\n",
    "            \n",
    "            dir_metrics = calculate_directional_accuracy(actuals_series, forecasts_series)\n",
    "            \n",
    "            if 'directional_accuracy' in dir_metrics:\n",
    "                all_dir_acc.append(dir_metrics['directional_accuracy'])\n",
    "            if 'directional_accuracy_from_day0' in dir_metrics:\n",
    "                all_dir_acc_day0.append(dir_metrics['directional_accuracy_from_day0'])\n",
    "    \n",
    "    dir_acc = np.mean(all_dir_acc) if all_dir_acc else None\n",
    "    dir_acc_day0 = np.mean(all_dir_acc_day0) if all_dir_acc_day0 else None\n",
    "    \n",
    "    if all_dir_acc:\n",
    "        print(f\"\\n    Directional Accuracy:\")\n",
    "        print(f\"      Day-to-day: {dir_acc:.1f}%\")\n",
    "        if all_dir_acc_day0:\n",
    "            print(f\"      From Day 0: {dir_acc_day0:.1f}% (primary trading metric)\")\n",
    "    \n",
    "    # Calculate CRPS and coverage (for non-100% accurate)\n",
    "    crps_mean = None\n",
    "    coverage_80 = None\n",
    "    \n",
    "    if target_accuracy < 1.0:\n",
    "        print(f\"\\n    Probabilistic Metrics:\")\n",
    "        \n",
    "        # Sample timestamps for CRPS (computationally expensive)\n",
    "        sample_timestamps = np.random.choice(timestamps, size=min(50, len(timestamps)), replace=False)\n",
    "        crps_values = []\n",
    "        \n",
    "        for ts in sample_timestamps:\n",
    "            ts_predictions = predictions_df[predictions_df['timestamp'] == ts]\n",
    "            forecast_matrix = ts_predictions.pivot_table(\n",
    "                index='run_id', \n",
    "                columns='day_ahead', \n",
    "                values='predicted_price'\n",
    "            ).values\n",
    "            \n",
    "            ts_actuals = results_df[results_df['timestamp'] == ts].sort_values('day_ahead')['actual'].values\n",
    "            \n",
    "            if len(ts_actuals) == forecast_matrix.shape[1]:\n",
    "                crps = calculate_crps(ts_actuals, forecast_matrix)\n",
    "                if crps is not None:\n",
    "                    crps_values.append(crps)\n",
    "        \n",
    "        if crps_values:\n",
    "            crps_mean = float(np.mean(crps_values))\n",
    "            print(f\"      CRPS: ${crps_mean:.2f} (lower is better)\")\n",
    "        \n",
    "        # Coverage\n",
    "        intervals = predictions_df.groupby(['timestamp', 'day_ahead'])['predicted_price'].agg(\n",
    "            p10=lambda x: x.quantile(0.1),\n",
    "            p90=lambda x: x.quantile(0.9)\n",
    "        ).reset_index()\n",
    "        \n",
    "        validation = results_df.merge(intervals, on=['timestamp', 'day_ahead'])\n",
    "        coverage_80 = float(((validation['actual'] >= validation['p10']) & \n",
    "                      (validation['actual'] <= validation['p90'])).mean())\n",
    "        \n",
    "        print(f\"      80% interval coverage: {coverage_80:.1%} (target: ~80%)\")\n",
    "    \n",
    "    print(f\"  ✓ Validation complete\")\n",
    "    \n",
    "    # Return metrics dictionary\n",
    "    return {\n",
    "        'commodity': commodity,\n",
    "        'model_version': model_version,\n",
    "        'generation_timestamp': datetime.now(),\n",
    "        'target_accuracy': float(target_accuracy),\n",
    "        'target_mape': float(target_mape),\n",
    "        'achieved_mae': float(overall_mae),\n",
    "        'achieved_mape': float(overall_mape),\n",
    "        'median_ape': float(median_ape),\n",
    "        'pct90_ape': float(pct90_ape),\n",
    "        'directional_accuracy': dir_acc,\n",
    "        'directional_accuracy_day0': dir_acc_day0,\n",
    "        'crps': crps_mean,\n",
    "        'coverage_80': coverage_80,\n",
    "        'n_samples': len(results_df),\n",
    "        'per_horizon_metrics': json.dumps(per_horizon_list)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_validation_metrics(metrics_list, table_name):\n",
    "    \"\"\"\n",
    "    Save validation metrics to Delta table.\n",
    "    \"\"\"\n",
    "    if not metrics_list:\n",
    "        print(\"\\n⚠️  No validation metrics to save\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nSaving validation metrics to {table_name}...\")\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    metrics_spark = spark.createDataFrame(metrics_df)\n",
    "    \n",
    "    # Append to existing table (or create if doesn't exist)\n",
    "    metrics_spark.write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(table_name)\n",
    "    \n",
    "    print(f\"✓ Saved {len(metrics_list)} validation metric records\")\n",
    "    print(f\"\\n  Query with: SELECT * FROM {table_name} ORDER BY generation_timestamp DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Commodities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_commodity(commodity_name, prices_raw_pd, analysis_config, output_schema, \n",
    "                            accuracy_levels, synthetic_start_date):\n",
    "    \"\"\"\n",
    "    Process a single commodity with multiple calibrated accuracy levels.\n",
    "    Returns validation metrics for all accuracy levels.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PROCESSING: {commodity_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Filter and prepare prices\n",
    "    print(f\"\\nPreparing price data...\")\n",
    "    prices_full = prices_raw_pd[prices_raw_pd['commodity'].str.lower() == commodity_name.lower()].copy()\n",
    "    prices_full['date'] = pd.to_datetime(prices_full['date'])\n",
    "    prices_full['price'] = prices_full['close']\n",
    "    prices_full = prices_full[['date', 'price']].sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"✓ Full price history: {len(prices_full)} days\")\n",
    "    print(f\"  Date range: {prices_full['date'].min()} to {prices_full['date'].max()}\")\n",
    "    \n",
    "    # Filter to synthetic date range\n",
    "    print(f\"\\nFiltering to {synthetic_start_date}+ for synthetic predictions...\")\n",
    "    prices = prices_full[prices_full['date'] >= synthetic_start_date].copy().reset_index(drop=True)\n",
    "    print(f\"✓ Filtered to {len(prices)} days\")\n",
    "    \n",
    "    # Generate predictions for all accuracy levels\n",
    "    print(f\"\\nGenerating calibrated predictions for {len(accuracy_levels)} accuracy levels...\")\n",
    "    \n",
    "    all_predictions = []\n",
    "    validation_metrics = []\n",
    "    \n",
    "    for accuracy in accuracy_levels:\n",
    "        model_version = f\"synthetic_acc{int(accuracy*100)}\"\n",
    "        \n",
    "        print(f\"\\n  {model_version}: {accuracy:.0%} accurate (MAPE = {(1-accuracy):.0%})\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        predictions_df = generate_calibrated_predictions(\n",
    "            prices,\n",
    "            model_version=model_version,\n",
    "            target_accuracy=accuracy,\n",
    "            n_runs=analysis_config['prediction_runs'],\n",
    "            n_horizons=analysis_config['forecast_horizon'],\n",
    "            chunk_size=20\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"    ✓ Generated {len(predictions_df):,} rows in {elapsed:.1f}s\")\n",
    "        \n",
    "        # Validate accuracy and collect metrics\n",
    "        metrics = validate_predictions(\n",
    "            predictions_df, \n",
    "            prices, \n",
    "            commodity_name,\n",
    "            model_version,\n",
    "            accuracy, \n",
    "            analysis_config['forecast_horizon']\n",
    "        )\n",
    "        \n",
    "        if metrics:\n",
    "            validation_metrics.append(metrics)\n",
    "        \n",
    "        all_predictions.append(predictions_df)\n",
    "        \n",
    "        del predictions_df\n",
    "        gc.collect()\n",
    "    \n",
    "    # Combine all accuracy levels\n",
    "    print(f\"\\nCombining all accuracy levels...\")\n",
    "    combined_predictions = pd.concat(all_predictions, ignore_index=True)\n",
    "    print(f\"✓ Combined: {len(combined_predictions):,} total rows\")\n",
    "    \n",
    "    del all_predictions\n",
    "    gc.collect()\n",
    "    \n",
    "    # Save to Delta table\n",
    "    predictions_table = f\"{output_schema}.predictions_{commodity_name.lower()}\"\n",
    "    \n",
    "    print(f\"\\nSaving to Delta table: {predictions_table}\")\n",
    "    predictions_spark = spark.createDataFrame(combined_predictions)\n",
    "    predictions_spark.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(predictions_table)\n",
    "    \n",
    "    saved_count = spark.table(predictions_table).count()\n",
    "    print(f\"✓ Saved and verified: {saved_count:,} rows\")\n",
    "    \n",
    "    del combined_predictions\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\n✓ {commodity_name.upper()} COMPLETE\")\n",
    "    \n",
    "    return {\n",
    "        'commodity': commodity_name,\n",
    "        'n_dates': len(prices),\n",
    "        'n_accuracy_levels': len(accuracy_levels),\n",
    "        'table': predictions_table,\n",
    "        'validation_metrics': validation_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all commodities\n",
    "all_results = []\n",
    "all_validation_metrics = []\n",
    "\n",
    "for commodity_name in COMMODITY_CONFIGS.keys():\n",
    "    try:\n",
    "        result = process_single_commodity(\n",
    "            commodity_name,\n",
    "            market_df,\n",
    "            ANALYSIS_CONFIG,\n",
    "            OUTPUT_SCHEMA,\n",
    "            ACCURACY_LEVELS,\n",
    "            SYNTHETIC_START_DATE\n",
    "        )\n",
    "        \n",
    "        all_results.append({\n",
    "            'commodity': result['commodity'],\n",
    "            'n_dates': result['n_dates'],\n",
    "            'n_accuracy_levels': result['n_accuracy_levels'],\n",
    "            'table': result['table']\n",
    "        })\n",
    "        \n",
    "        # Collect validation metrics\n",
    "        if result.get('validation_metrics'):\n",
    "            all_validation_metrics.extend(result['validation_metrics'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error processing {commodity_name.upper()}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(f\"   Skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all validation metrics to Delta table\n",
    "save_validation_metrics(all_validation_metrics, VALIDATION_METRICS_TABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CALIBRATED PREDICTION GENERATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    summary_df = pd.DataFrame(all_results)\n",
    "    print(f\"\\nSuccessfully processed {len(all_results)} commodities:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nPrediction tables created:\")\n",
    "    for table in sorted(summary_df['table'].unique()):\n",
    "        print(f\"  - {table}\")\n",
    "        model_versions = spark.table(table).select(\"model_version\").distinct().collect()\n",
    "        for mv in model_versions:\n",
    "            acc = int(mv.model_version.replace('synthetic_acc', ''))\n",
    "            mape = 100 - acc\n",
    "            print(f\"      • {mv.model_version}: {acc}% accurate (MAPE = {mape}%)\")\n",
    "    \n",
    "    print(f\"\\n✓ Validation metrics saved to: {VALIDATION_METRICS_TABLE}\")\n",
    "    print(f\"  Total validation records: {len(all_validation_metrics)}\")\n",
    "    \n",
    "    print(f\"\\n✓ Key features:\")\n",
    "    print(f\"  1. Median predictions have target MAPE (e.g., 90% accurate = 10% MAPE)\")\n",
    "    print(f\"  2. Prediction intervals properly calibrated (80% interval ≈ 80% coverage)\")\n",
    "    print(f\"  3. Includes 100% accurate scenario for algorithm testing\")\n",
    "    print(f\"  4. Uses log-normal errors for realistic multiplicative noise\")\n",
    "    print(f\"  5. Validation metrics aligned with forecast_agent:\")\n",
    "    print(f\"     - MAE (Mean Absolute Error)\")\n",
    "    print(f\"     - MAPE (Mean Absolute Percentage Error)\")\n",
    "    print(f\"     - Directional Accuracy (day-to-day and from Day 0)\")\n",
    "    print(f\"     - CRPS (Continuous Ranked Probability Score)\")\n",
    "    print(f\"     - Calibration (80% interval coverage)\")\n",
    "    print(f\"  6. Validation metrics saved to Delta table for later review\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No commodities were successfully processed\")\n",
    "\n",
    "print(\"\\n✓ Calibrated prediction generation complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
