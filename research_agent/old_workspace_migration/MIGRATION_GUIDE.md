# Databricks Workspace Migration Guide

## Overview

This folder contains the ETL pipeline code from the old Databricks workspace (stuholland@berkeley.edu). We are migrating to a new workspace due to access control issues with the previous owner.

**Migration Status**: In Progress
**New AWS Account ID**: 534150427458
**Region**: us-west-2

---

## New S3 Bucket Structure

All data has been migrated to the following S3 buckets in the new AWS account:

```
├── s3://groundtruth-bronze/
│   ├── market_data/market_data.csv
│   ├── vix_data/vix_data.csv
│   ├── macro_data/macro_data.csv
│   ├── weather_data/weather_data.csv
│   └── gdelt/bronze_gkg.csv
│
├── s3://groundtruth-silver/
│   └── unified_data/unified_data.csv
│
├── s3://groundtruth-raw/
│   (reserved for future raw data ingestion)
│
├── s3://groundtruth-forecasting-agent/
│   (reserved for forecast outputs)
│
├── s3://groundtruth-trading-agent/
│   (reserved for trading agent outputs)
│
└── s3://groundtruth-models/
    (reserved for model artifacts)
```

---

## Unity Catalog Table Structure

**CRITICAL**: Table names must match the old workspace setup to maintain compatibility with existing code.

### Catalog Name
```
commodity
```

### Bronze Tables (commodity.bronze.*)

| Table Name | Source S3 Location | Data Type |
|------------|-------------------|-----------|
| `v_market_data_all` | `s3://groundtruth-bronze/market_data/` | Coffee & Sugar market prices |
| `v_vix_data_all` | `s3://groundtruth-bronze/vix_data/` | VIX volatility index |
| `v_macro_data_all` | `s3://groundtruth-bronze/macro_data/` | Exchange rates (COP/USD) |
| `v_weather_data_all` | `s3://groundtruth-bronze/weather_data/` | Weather features |
| `bronze_gkg` | `s3://groundtruth-bronze/gdelt/` | GDELT sentiment data |

### Silver Tables (commodity.silver.*)

| Table Name | Source S3 Location | Data Type |
|------------|-------------------|-----------|
| `unified_data` | `s3://groundtruth-silver/unified_data/` | Joined dataset for forecasting |
| `point_forecasts` | (Generated by forecast_agent) | 14-day point forecasts |
| `distributions` | (Generated by forecast_agent) | Monte Carlo paths for risk analysis |
| `forecast_actuals` | (Generated by forecast_agent) | Realized prices for backtesting |

---

## Migration Checklist

### Step 1: Unity Catalog Setup
- [ ] Create catalog: `commodity`
- [ ] Create schema: `commodity.bronze`
- [ ] Create schema: `commodity.silver`

### Step 2: External Location Configuration
- [ ] Configure S3 access credentials
- [ ] Create external location for `s3://groundtruth-bronze/`
- [ ] Create external location for `s3://groundtruth-silver/`
- [ ] Verify IAM roles (see `../../infra/databricks_config.yaml`)

### Step 3: Bronze Layer Tables
Create external tables pointing to S3:

```sql
-- Example: Market Data
CREATE TABLE IF NOT EXISTS commodity.bronze.v_market_data_all
USING CSV
OPTIONS (
  path 's3://groundtruth-bronze/market_data/market_data.csv',
  header 'true',
  inferSchema 'true'
);

-- Repeat for other bronze tables (see etl/02. Move Data to Bronze Layer.sql)
```

### Step 4: Silver Layer Setup
Run the unified data creation pipeline:

```python
# From repository root
cd research_agent/
python create_gdelt_unified_data.py
```

This creates `commodity.silver.unified_data` from the bronze tables.

### Step 5: Forecast Agent Setup
Deploy forecast agent to Databricks:

```bash
# See forecast_agent/databricks_quickstart.py
# This will create:
# - commodity.silver.point_forecasts
# - commodity.silver.distributions
# - commodity.silver.forecast_actuals
```

### Step 6: Validation
- [ ] Verify all bronze tables have data
- [ ] Verify unified_data table matches expected schema
- [ ] Run test forecast to validate end-to-end pipeline
- [ ] Verify trading agent can read forecast tables

---

## Folder Organization

### `etl/`
Contains the original ETL pipeline scripts from the old workspace:
- `01.Set-up Project Environment.sql` - Initial workspace setup
- `02. Move Data to Bronze Layer.sql` - Bronze table creation
- `03. Incremental Auto Loader.sql` - Streaming ingestion setup
- `DLT Coffee Feature Pipeline.sql` - Delta Live Tables for Coffee
- `DLT_Silver_Sugar_Feature_Pipeline.py` - Sugar feature engineering
- `DLT_Gold_GDelt_Pipeline.py` - GDELT sentiment processing
- `Market Data EDA.sql` - Exploratory analysis

### `eda/`
Exploratory data analysis notebooks:
- `Coffee and Sugar Feature Engineering.sql` - Feature engineering queries

### `feature_pipelines/`
Sugar-specific feature engineering code (likely deprecated):
- `transformations/` - Feature transformation logic
- `explorations/` - Data exploration notebooks
- `utilities/` - Utility functions

### `automl_experiments/`
Databricks AutoML trials (historical reference):
- Prophet and ARIMA model experiments
- AutoML generated notebooks

---

## Key Differences from Old Workspace

1. **S3 Bucket Naming**: Old workspace used generic bucket names, new workspace uses `groundtruth-*` naming convention
2. **IAM Roles**: New cross-account roles and storage roles (see `infra/databricks_config.yaml`)
3. **Region**: Confirmed us-west-2 (Oregon)
4. **Catalog Structure**: Same catalog/schema names maintained for compatibility
5. **Access Control**: New workspace with proper admin access

---

## IAM Roles

See `../../infra/databricks_config.yaml` for complete configuration:

- **S3 Bucket**: `databricks-otxszqwuhb1lbkqvfac5kd-cloud-storage-bucket`
- **Storage Role**: `databricks-otxszqwuhb1lbkqvfac5kd-cloud-storage-role`
- **Cross-Account Role**: `databricks-otxszqwuhb1lbkqvfac5kd-cross-account-role`

---

## Migration Support

For questions about the migration:
1. Review old ETL scripts in `etl/` folder
2. Check data contracts in `../../project_overview/DATA_CONTRACTS.md`
3. Verify table schemas match expected structure
4. Test with small dataset before full migration

---

## Next Steps

1. Complete Unity Catalog setup in new workspace
2. Migrate bronze layer data from S3
3. Run unified data pipeline to create silver tables
4. Deploy forecast agent
5. Validate end-to-end pipeline
6. Archive or delete old workspace

**Last Updated**: October 29, 2025
