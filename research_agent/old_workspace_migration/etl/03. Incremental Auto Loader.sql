-- Databricks notebook source
-- MAGIC %python
-- MAGIC from pyspark.sql.functions import current_timestamp, expr, to_date, col, year, month
-- MAGIC
-- MAGIC inc_path = "s3://berkeley-datasci210-capstone/landing/vix_data/"
-- MAGIC bronze_tbl = "commodity.bronze.vix_data_raw"
-- MAGIC checkpoint_inc = "s3://berkeley-datasci210-capstone/_checkpoints/vix_data_inc"
-- MAGIC schema_loc = "s3://berkeley-datasci210-capstone/_schemas/vix_data"
-- MAGIC
-- MAGIC (
-- MAGIC   spark.readStream
-- MAGIC     .format("cloudFiles")
-- MAGIC     .option("cloudFiles.format", "csv")
-- MAGIC     .option("cloudFiles.schemaLocation", schema_loc)
-- MAGIC     .option("cloudFiles.inferColumnTypes", "true")
-- MAGIC     .option("header", "true")
-- MAGIC     .option("pathGlobFilter", "*.csv")
-- MAGIC     .load(inc_path)
-- MAGIC     .withColumn("source_file", expr("_metadata.file_path"))
-- MAGIC     .withColumn("ingest_ts", current_timestamp())
-- MAGIC     # Derive partition columns from business date if available; otherwise use ingest_ts
-- MAGIC     # Assume the CSV has a column named "date" (yyyy-MM-dd or similar). If not, swap to ingest_ts.
-- MAGIC     .withColumn("event_date", to_date(col("date")))           # change "date" if your column name differs
-- MAGIC     .withColumn("p_year",  year(col("event_date")))
-- MAGIC     .withColumn("p_month", month(col("event_date")))
-- MAGIC     .writeStream
-- MAGIC     .format("delta")
-- MAGIC     .option("checkpointLocation", checkpoint_inc)
-- MAGIC     .option("mergeSchema", "true")
-- MAGIC     .option("partitionColumns", "p_year,p_month")
-- MAGIC     .trigger(availableNow=True)
-- MAGIC     .toTable(bronze_tbl)
-- MAGIC )

-- COMMAND ----------

-- MAGIC %python
-- MAGIC
-- MAGIC inc_path = "s3://berkeley-datasci210-capstone/landing/market_data/"
-- MAGIC bronze_tbl = "commodity.bronze.market_data_raw"
-- MAGIC checkpoint_inc = "s3://berkeley-datasci210-capstone/_checkpoints/market_data_inc"
-- MAGIC schema_loc = "s3://berkeley-datasci210-capstone/_schemas/market_data"
-- MAGIC
-- MAGIC (
-- MAGIC   spark.readStream
-- MAGIC     .format("cloudFiles")
-- MAGIC     .option("cloudFiles.format", "csv")
-- MAGIC     .option("cloudFiles.schemaLocation", schema_loc)
-- MAGIC     .option("cloudFiles.inferColumnTypes", "true")
-- MAGIC     .option("header", "true")
-- MAGIC     .option("pathGlobFilter", "*.csv")
-- MAGIC     .load(inc_path)
-- MAGIC     .withColumn("source_file", expr("_metadata.file_path"))
-- MAGIC     .withColumn("ingest_ts", current_timestamp())
-- MAGIC     .withColumn("event_date", to_date(col("date")))           
-- MAGIC     .withColumn("p_year",  year(col("event_date")))
-- MAGIC     .withColumn("p_month", month(col("event_date")))
-- MAGIC     .writeStream
-- MAGIC     .format("delta")
-- MAGIC     .option("checkpointLocation", checkpoint_inc)
-- MAGIC     .option("mergeSchema", "true")
-- MAGIC     .option("partitionColumns", "p_year,p_month")
-- MAGIC     .trigger(availableNow=True)
-- MAGIC     .toTable(bronze_tbl)
-- MAGIC )

-- COMMAND ----------

-- MAGIC %python
-- MAGIC from pyspark.sql.functions import current_timestamp, expr
-- MAGIC
-- MAGIC inc_path = "s3://berkeley-datasci210-capstone/landing/macro_data/"
-- MAGIC bronze_tbl = "commodity.bronze.macro_data_raw"
-- MAGIC checkpoint_inc = "s3://berkeley-datasci210-capstone/_checkpoints/macro_data_inc"
-- MAGIC schema_loc = "s3://berkeley-datasci210-capstone/_schemas/macro_data"
-- MAGIC
-- MAGIC (
-- MAGIC   spark.readStream
-- MAGIC     .format("cloudFiles")
-- MAGIC     .option("cloudFiles.format", "csv")
-- MAGIC     .option("cloudFiles.schemaLocation", schema_loc)
-- MAGIC     .option("cloudFiles.inferColumnTypes", "true")
-- MAGIC     .option("header", "true")
-- MAGIC     .option("pathGlobFilter", "*.csv")
-- MAGIC     .load(inc_path)
-- MAGIC     .withColumn("source_file", expr("_metadata.file_path"))
-- MAGIC     .withColumn("ingest_ts", current_timestamp())
-- MAGIC     .withColumn("event_date", to_date(col("date")))          
-- MAGIC     .withColumn("p_year",  year(col("event_date")))
-- MAGIC     .withColumn("p_month", month(col("event_date")))
-- MAGIC     .writeStream
-- MAGIC     .format("delta")
-- MAGIC     .option("checkpointLocation", checkpoint_inc)
-- MAGIC     .option("mergeSchema", "true")
-- MAGIC     .option("partitionColumns", "p_year,p_month")
-- MAGIC     .trigger(availableNow=True)
-- MAGIC     .toTable(bronze_tbl)
-- MAGIC )

-- COMMAND ----------

-- MAGIC %python
-- MAGIC from pyspark.sql.functions import current_timestamp, expr
-- MAGIC import re
-- MAGIC
-- MAGIC def sanitize(name: str) -> str:
-- MAGIC     # replace spaces and forbidden chars [ ,;{}()\n\t=] with _
-- MAGIC     n = re.sub(r"[ ,;{}\(\)\n\t=]+", "_", name.strip())
-- MAGIC     # collapse repeats and lowercase
-- MAGIC     n = re.sub(r"_+", "_", n).strip("_").lower()
-- MAGIC     return n
-- MAGIC
-- MAGIC def sanitize_cols(df):
-- MAGIC     safe = []
-- MAGIC     seen = set()
-- MAGIC     for c in df.columns:
-- MAGIC         s = sanitize(c)
-- MAGIC         # avoid accidental duplicates after sanitizing
-- MAGIC         i, base = 1, s
-- MAGIC         while s in seen:
-- MAGIC             i += 1
-- MAGIC             s = f"{base}_{i}"
-- MAGIC         seen.add(s)
-- MAGIC         safe.append(s)
-- MAGIC     return df.toDF(*safe)
-- MAGIC   
-- MAGIC inc_path = "s3://berkeley-datasci210-capstone/landing/cftc_data/"
-- MAGIC bronze_tbl = "commodity.bronze.cftc_data_raw"
-- MAGIC checkpoint_inc = "s3://berkeley-datasci210-capstone/_checkpoints/cftc_data_inc"
-- MAGIC schema_loc = "s3://berkeley-datasci210-capstone/_schemas/cftc_data"
-- MAGIC
-- MAGIC (
-- MAGIC   spark.readStream
-- MAGIC     .format("cloudFiles")
-- MAGIC     .option("cloudFiles.format", "csv")
-- MAGIC     .option("cloudFiles.schemaLocation", schema_loc)
-- MAGIC     .option("cloudFiles.inferColumnTypes", "true")
-- MAGIC     .option("header", "true")
-- MAGIC     .option("pathGlobFilter", "*.csv")
-- MAGIC     .load(inc_path)
-- MAGIC     .transform(sanitize_cols)
-- MAGIC     .withColumn("source_file", expr("_metadata.file_path"))
-- MAGIC     .withColumn("ingest_ts", current_timestamp())
-- MAGIC     .withColumn("event_date", to_date(col("`as_of_date_in_form_yyyy-mm-dd`")))           
-- MAGIC     .withColumn("p_year",  year(col("event_date")))
-- MAGIC     .withColumn("p_month", month(col("event_date")))
-- MAGIC     .writeStream
-- MAGIC     .format("delta")
-- MAGIC     .option("checkpointLocation", checkpoint_inc)
-- MAGIC     .option("mergeSchema", "true")
-- MAGIC     .option("partitionColumns", "p_year,p_month")
-- MAGIC     .trigger(availableNow=True)
-- MAGIC     .toTable(bronze_tbl)
-- MAGIC )

-- COMMAND ----------

-- MAGIC %python
-- MAGIC from pyspark.sql.functions import col, to_date, to_timestamp, input_file_name, current_timestamp
-- MAGIC
-- MAGIC # TARGET bronze table (single source of truth)
-- MAGIC BRONZE_TBL   = "commodity.bronze.weather_data_raw"
-- MAGIC
-- MAGIC # Shared schema & checkpoint locations
-- MAGIC SCHEMA_LOC   = "s3://berkeley-datasci210-capstone/_schemas/weather"
-- MAGIC CKPT_INC     = "s3://berkeley-datasci210-capstone/_checkpoints/weather_inc"
-- MAGIC
-- MAGIC # Source prefixes (adjust if yours differ)
-- MAGIC INC_PATH     = "s3://berkeley-datasci210-capstone/landing/weather_data/"
-- MAGIC
-- MAGIC (
-- MAGIC   spark.readStream
-- MAGIC     .format("cloudFiles")
-- MAGIC     .option("cloudFiles.format", "csv")
-- MAGIC     .option("cloudFiles.schemaLocation", SCHEMA_LOC)   # same store as hist
-- MAGIC     .option("cloudFiles.inferColumnTypes", "true")
-- MAGIC     .option("header", "true")
-- MAGIC     .option("pathGlobFilter", "*.csv")
-- MAGIC     .load(INC_PATH)
-- MAGIC     .withColumn("source_file", expr("_metadata.file_path"))
-- MAGIC     .withColumn("ingest_ts",   current_timestamp())
-- MAGIC     .withColumn("event_date", to_date(col("date")))           
-- MAGIC     .withColumn("p_year",  year(col("event_date")))
-- MAGIC     .withColumn("p_month", month(col("event_date")))
-- MAGIC     .writeStream
-- MAGIC     .format("delta")
-- MAGIC     .option("checkpointLocation", CKPT_INC)             # distinct from HIST
-- MAGIC     .option("mergeSchema", "true")
-- MAGIC     .option("partitionColumns", "p_year,p_month")
-- MAGIC     .trigger(availableNow=True)                         # picks up new files & exits
-- MAGIC     .toTable(BRONZE_TBL)
-- MAGIC )

-- COMMAND ----------

-- Count rows & inspect schema
SELECT COUNT(*) FROM commodity.bronze.weather_data_raw;
DESCRIBE DETAIL commodity.bronze.weather_data_raw;
DESCRIBE HISTORY commodity.bronze.weather_data_raw;

-- Sample recent records
SELECT dt, region, commodity, temp_c, humidity_pct, source, source_file
FROM commodity.bronze.weather_data_raw
ORDER BY ingest_ts DESC
LIMIT 50;
