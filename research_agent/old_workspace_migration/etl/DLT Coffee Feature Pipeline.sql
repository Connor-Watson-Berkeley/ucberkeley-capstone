-- Databricks notebook source
-- MAGIC %python
-- MAGIC import dlt
-- MAGIC import pyspark.sql.functions as F
-- MAGIC import pyspark.sql.window as W
-- MAGIC from pyspark.sql.types import DateType
-- MAGIC
-- MAGIC # --- 1. CONFIGURATION ---
-- MAGIC TARGET_COMMODITY = "COFFEE"
-- MAGIC # Target regions adjusted for major coffee production zones
-- MAGIC TARGET_REGIONS = ["Brazil", "Vietnam", "Colombia", "Ethiopia"] 
-- MAGIC
-- MAGIC # --- 2. DLT BASE LAYER (Daily Merged Input) ---
-- MAGIC # This table merges all daily inputs (Price, Macro, Weather) for the commodity.
-- MAGIC
-- MAGIC @dlt.table(
-- MAGIC     name="silver_coffee_daily_merged",
-- MAGIC     comment="Daily merged raw features for Coffee commodity."
-- MAGIC )
-- MAGIC def coffee_daily_merged():
-- MAGIC     # Read the Bronze tables 
-- MAGIC     market_df = dlt.read("commodity.bronze.market_data_raw").filter(F.upper(F.col("commodity")) == TARGET_COMMODITY)
-- MAGIC     vix_df = dlt.read("commodity.bronze.vix_data_raw")
-- MAGIC     macro_df = dlt.read("commodity.bronze.macro_data_raw")
-- MAGIC     # For weather, we select the required regions
-- MAGIC     weather_df = dlt.read("commodity.bronze.v_weather_data_all").filter(
-- MAGIC         (F.upper(F.col("commodity")) == TARGET_COMMODITY) & (F.col("region").isin(TARGET_REGIONS))
-- MAGIC     )
-- MAGIC
-- MAGIC     # Standardize columns for joining
-- MAGIC     market_df = market_df.select(F.col("Date").alias("Date"), F.col("Close_Price").alias("Coffee_Price"))
-- MAGIC     vix_df = vix_df.select(F.col("date").alias("Date"), F.col("vix").alias("VIX"))
-- MAGIC     # FIX: Use 'cop_usd' as placeholder for USD/BRL
-- MAGIC     macro_df = macro_df.select(F.col("date").alias("Date"), F.col("cop_usd").alias("USD_BRL"))
-- MAGIC     
-- MAGIC     # ------------------
-- MAGIC     # Daily Joins
-- MAGIC     # ------------------
-- MAGIC     daily_df = market_df.join(vix_df, on="Date", how="full_outer")
-- MAGIC     daily_df = daily_df.join(macro_df, on="Date", how="full_outer")
-- MAGIC     
-- MAGIC     # Select relevant weather features before joining (note: weather is joined later)
-- MAGIC     weather_df = weather_df.select(
-- MAGIC         F.col("dt").alias("Date"),
-- MAGIC         F.col("region").alias("Region"),
-- MAGIC         F.col("temp_c").alias("Min_Temp_C"),
-- MAGIC         F.col("precipitation_mm").alias("Rainfall_MM")
-- MAGIC     )
-- MAGIC
-- MAGIC     # Return the merged daily data
-- MAGIC     return daily_df.filter(F.col("Date").isNotNull())
-- MAGIC
-- MAGIC
-- MAGIC # --- 3. DLT FEATURE ENGINEERING LAYER (Weekly Aggregation) ---
-- MAGIC
-- MAGIC @dlt.table(
-- MAGIC     name="silver_coffee_weekly",
-- MAGIC     comment="Weekly price, macro, and weather features for Coffee."
-- MAGIC )
-- MAGIC def coffee_weekly_features():
-- MAGIC     df = dlt.read("silver_coffee_daily_merged")
-- MAGIC     weather_df = dlt.read("commodity.bronze.v_weather_data_all") 
-- MAGIC
-- MAGIC     # 1. Price Feature Engineering (Spark Window Functions)
-- MAGIC     df = df.withColumn("Log_Return", F.log(F.col("Coffee_Price")) - F.log(F.lag(F.col("Coffee_Price"), 1).over(W.Window.orderBy("Date"))))
-- MAGIC
-- MAGIC     # Calculate 20-Day Historical Volatility
-- MAGIC     window_20d = W.Window.orderBy("Date").rowsBetween(-19, W.Window.currentRow)
-- MAGIC     df = df.withColumn("HV_20D_Raw", F.stddev("Log_Return").over(window_20d))
-- MAGIC     df = df.withColumn("HV_20D_Annualized", F.col("HV_20D_Raw") * F.sqrt(F.lit(252)))
-- MAGIC
-- MAGIC     # 2. Weather Feature Engineering (Z-Score and Frost Count)
-- MAGIC     
-- MAGIC     # Window partitioned by Region for independent calculations
-- MAGIC     window_90d_region = W.Window.partitionBy("Region").orderBy("Date").rowsBetween(-89, W.Window.currentRow)
-- MAGIC
-- MAGIC     weather_features = weather_df.select(
-- MAGIC         F.col("dt").alias("Date"),
-- MAGIC         F.col("region").alias("Region"),
-- MAGIC         F.col("temp_c").alias("Min_Temp_C"),
-- MAGIC         F.col("precipitation_mm").alias("Rainfall_MM")
-- MAGIC     ).filter(F.col("Region").isin(TARGET_REGIONS))
-- MAGIC     
-- MAGIC     # Calculate 90-day rolling sum for rainfall and temp features
-- MAGIC     weather_features = weather_features.withColumn(
-- MAGIC         "Rainfall_90D_Sum",
-- MAGIC         F.sum("Rainfall_MM").over(window_90d_region)
-- MAGIC     ).withColumn(
-- MAGIC         "Frost_Day_Indicator",
-- MAGIC         F.when(F.col("Min_Temp_C") <= 0, 1).otherwise(0)
-- MAGIC     ).withColumn(
-- MAGIC         "Frost_Count_60D",
-- MAGIC         F.sum("Frost_Day_Indicator").over(W.Window.partitionBy("Region").orderBy("Date").rowsBetween(-59, W.Window.currentRow))
-- MAGIC     )
-- MAGIC     
-- MAGIC     # Use global stats (or static stats) for Z-Score mean/stddev calculation (simplified here for DLT)
-- MAGIC     # NOTE: For true Z-score, these should be historical averages. Here we use an approximation:
-- MAGIC     weather_features = weather_features.withColumn(
-- MAGIC         "Rainfall_Z_Score",
-- MAGIC         (F.col("Rainfall_90D_Sum") - F.avg("Rainfall_90D_Sum").over(W.Window.partitionBy("Region").rowsBetween(-365*5, W.Window.currentRow)))
-- MAGIC         / F.stddev("Rainfall_90D_Sum").over(W.Window.partitionBy("Region").rowsBetween(-365*5, W.Window.currentRow))
-- MAGIC     )
-- MAGIC
-- MAGIC     # 3. Aggregate Weather Stress (Worst Case)
-- MAGIC     # Group by Date and find the worst stress across all monitored regions
-- MAGIC     weather_stress_daily = weather_features.groupBy("Date").agg(
-- MAGIC         F.min("Rainfall_Z_Score").alias("Worst_Rainfall_Z_Score"),
-- MAGIC         F.max("Frost_Count_60D").alias("Max_Frost_Count")
-- MAGIC     )
-- MAGIC
-- MAGIC     # 4. Final Weekly Aggregation and Merge
-- MAGIC     
-- MAGIC     # Join features back to the main daily data
-- MAGIC     df = df.join(weather_stress_daily, on="Date", how="left")
-- MAGIC
-- MAGIC     # Determine the week start date for grouping
-- MAGIC     df = df.withColumn("Week_Start_Date", F.date_trunc('week', F.col("Date")))
-- MAGIC
-- MAGIC     weekly_features = df.groupBy("Week_Start_Date").agg(
-- MAGIC         # Price Features
-- MAGIC         F.last("Coffee_Price").alias("Coffee_Price_EOW"),
-- MAGIC         F.mean("HV_20D_Annualized").alias("HV_20D_Annualized_Wk_Mean"),
-- MAGIC         
-- MAGIC         # Macro Features
-- MAGIC         F.mean("VIX").alias("VIX_Wk_Mean"),
-- MAGIC         F.mean("USD_BRL").alias("USD_BRL_Wk_Mean"),
-- MAGIC         
-- MAGIC         # Weather Stress (Take the minimum/maximum of the worst daily stress over the week)
-- MAGIC         F.min("Worst_Rainfall_Z_Score").alias("Worst_Rainfall_Z_Score_Wk_Min"),
-- MAGIC         F.max("Max_Frost_Count").alias("Max_Frost_Count_Wk_Max")
-- MAGIC     ).filter(F.col("Coffee_Price_EOW").isNotNull()) # Only keep weeks where we have an end-of-week price
-- MAGIC
-- MAGIC     return weekly_features
-- MAGIC
-- MAGIC # --- 4. DLT FEATURE ENGINEERING LAYER (Monthly Aggregation) ---
-- MAGIC
-- MAGIC @dlt.table(
-- MAGIC     name="silver_coffee_monthly",
-- MAGIC     comment="Monthly price, macro, and weather features for Coffee."
-- MAGIC )
-- MAGIC def coffee_monthly_features():
-- MAGIC     df = dlt.read("silver_coffee_daily_merged")
-- MAGIC     
-- MAGIC     # Determine the month start date for grouping
-- MAGIC     df = df.withColumn("Month_Start_Date", F.date_trunc('month', F.col("Date")))
-- MAGIC
-- MAGIC     # Use the same logic as the weekly feature calculation for monthly means
-- MAGIC     df = df.withColumn("Log_Return", F.log(F.col("Coffee_Price")) - F.log(F.lag(F.col("Coffee_Price"), 1).over(W.Window.orderBy("Date"))))
-- MAGIC     
-- MAGIC     monthly_features = df.groupBy("Month_Start_Date").agg(
-- MAGIC         # Price Features
-- MAGIC         F.mean("Coffee_Price").alias("Coffee_Price_Mo_Mean"),
-- MAGIC         F.stddev("Log_Return").alias("Coffee_Price_Mo_Vol"),
-- MAGIC         
-- MAGIC         # Macro Features
-- MAGIC         F.mean("VIX").alias("VIX_Mo_Mean"),
-- MAGIC         F.mean("USD_BRL").alias("USD_BRL_Mo_Mean"),
-- MAGIC     )
-- MAGIC     
-- MAGIC     return monthly_features
-- MAGIC
-- MAGIC # --- 5. DLT GOLD LAYER (Final Merge: Features + CFTC) ---
-- MAGIC # This is the Gold table, ready for model training.
-- MAGIC
-- MAGIC @dlt.table(
-- MAGIC     name="gold_coffee_weekly_merged",
-- MAGIC     comment="Final model-ready table combining engineered features and CFTC sentiment."
-- MAGIC )
-- MAGIC def coffee_weekly_merged():
-- MAGIC     
-- MAGIC     # 1. Read the upstream Silver features table
-- MAGIC     features_df = dlt.read("silver_coffee_weekly")
-- MAGIC     
-- MAGIC     # 2. Read the raw CFTC data (Bronze Layer)
-- MAGIC     cftc_df = dlt.read("commodity.bronze.cftc_data_raw")
-- MAGIC     
-- MAGIC     # Prepare CFTC Data
-- MAGIC     cftc_df_coffee = cftc_df.filter(F.upper(F.col("market_and_exchange_names")) == TARGET_COMMODITY) 
-- MAGIC     
-- MAGIC     # Select and standardize CFTC columns
-- MAGIC     cftc_df_coffee = cftc_df_coffee.select(
-- MAGIC         F.col("as_of_date_in_form_yyyy-mm-dd").cast(DateType()).alias("Week_Start_Date"), 
-- MAGIC         F.col("noncommercial_positions-long_all").alias("NC_Long"),
-- MAGIC         F.col("noncommercial_positions-short_all").alias("NC_Short"),
-- MAGIC         F.col("open_interest_all").alias("Open_Interest")
-- MAGIC     ).filter(F.col("Week_Start_Date").isNotNull())
-- MAGIC
-- MAGIC     # Calculate the Net Non-Commercial Position
-- MAGIC     cftc_df_coffee = cftc_df_coffee.withColumn(
-- MAGIC         "Net_Non_Commercial_Position",
-- MAGIC         F.col("NC_Long") - F.col("NC_Short")
-- MAGIC     )
-- MAGIC     
-- MAGIC     # 3. Perform Left Join to Merge
-- MAGIC     final_merged_df = cftc_df_coffee.alias("cftc").join(
-- MAGIC         features_df.alias("features"),
-- MAGIC         on="Week_Start_Date",
-- MAGIC         how="left" # Keep all CFTC weeks
-- MAGIC     )
-- MAGIC
-- MAGIC     # 4. Final Cleanup and Selection
-- MAGIC     final_merged_df = final_merged_df.select(
-- MAGIC         F.col("Week_Start_Date"),
-- MAGIC         F.col("Net_Non_Commercial_Position"),  # TARGET VARIABLE
-- MAGIC         F.col("Open_Interest"),
-- MAGIC         # Select all features from the coffee_weekly table
-- MAGIC         *[col for col in features_df.columns if col not in ("Week_Start_Date")] 
-- MAGIC     ).na.drop(subset=["Net_Non_Commercial_Position"]) # Drop rows if we don't have the target
-- MAGIC
-- MAGIC     return final_merged_df
-- MAGIC